{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing a corpus for vectorization\n",
    "One of the best ways to speed your research and development time is to checkpoint your work so that you can reuse portions of your pipeline that don't change. Fortunately, as long as you don't use lambda statement in you scikit learn pipelines, you should be able to pickle and reload your pipelines, and furthermore, the resulting matrix output can be pickled, and reloaded. However, before we get to demonstrate impressive reuse, we typically have to deal with dirty data.\n",
    "## Dealing with dirty data\n",
    "A good rule of thumb is: if you don't think your data is dirty, you're probably not looking at it.\n",
    "Let's preprocess the Latin Library corpus and show some transformations that can be done to auto correct some data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from scipy import sparse\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from cltk.corpus.readers import get_corpus_reader, assemble_corpus\n",
    "from cltk.prosody.latin.string_utils import punctuation_for_spaces_dict\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from cltk.prosody.latin.scansion_constants import ScansionConstants\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from cltk.corpus.latin.latin_library_corpus_types import corpus_directories_by_type, corpus_texts_by_type\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from cltk.utils.matrix_corpus_fun import (\n",
    "    separate_camel_cases,\n",
    "    splice_hyphens,\n",
    "    drop_empty_lists,\n",
    "    drop_non_lower,\n",
    "    drop_probable_entities,\n",
    "    drop_editorial,\n",
    "    drop_arabic_numeric,\n",
    "    drop_all_caps,\n",
    "    jv_transform,\n",
    "    accept_editorial,    \n",
    "    drop_enclitics ,\n",
    "    drop_fringe_punctuation, \n",
    "    divide_separate_words,\n",
    "    drop_all_punctuation,\n",
    "    drop_short_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add our common library to the path and load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import inspect\n",
    "from pathlib import Path \n",
    "currentdir = Path.cwd()\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir) \n",
    "from mlyoucanuse.featurize_text_fun import featurize, vectorize_features\n",
    "from mlyoucanuse.smart_lower_transformer import SmartLowerTransformer\n",
    "from mlyoucanuse.trie_transformer import TrieTransformer\n",
    "from mlyoucanuse.featurize_text_fun import word_to_features\n",
    "from mlyoucanuse.matrix_fun import run_length_encoding, extract_words, patch_cluster_holes, merge_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = logging.getLogger('preprocess_corpus')\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = currentdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Smart Lower transformer and show how it's used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['leuissima', 'virum', 'cano'], ['perlucent', 'Arenas', 'stuff']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_lower_transformer = SmartLowerTransformer(lower_only_file=os.path.join(work_dir, 'latin.words.always.lower.txt' ))\n",
    "smart_lower_transformer.transform([['Leuissima', 'virum', 'cano'],\n",
    "                                        ['perlucent', 'Arenas', 'stuff']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Trie Transformer and show how it's used to autocorrect corrupt text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['maturitatem', 'perueniunt'],\n",
       " ['radicibus', 'subministres'],\n",
       " ['peregrinationes', 'habere'],\n",
       " ['uersibus', 'disertissimis'],\n",
       " ['crudelitatis', 'consuetudinem'],\n",
       " ['adiciebat', 'contrahendam'],\n",
       " ['translationes', 'inprobas']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filename = 'latin_word_trie.pkl'\n",
    "trie_transformer = TrieTransformer(trie_file=filename)\n",
    "trie_transformer.transform([\n",
    "    # actually found in latin library\n",
    "    ['maturitatemperueniunt'],\n",
    "    ['radicibussubministres'],\n",
    "    ['peregrinationeshabere'],\n",
    "    ['uersibusdisertissimis'],\n",
    "    ['crudelitatisconsuetudinem'],\n",
    "    ['adiciebatcontrahendam'],\n",
    "    ['translationesinprobas']     \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our classifier from another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_cls = None\n",
    "with open(os.path.join('../detecting_loanwords', 'is_transliterated_greek.mdl.0.20.2.joblib'), 'rb') as reader:\n",
    "    greek_cls = joblib.load(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our text processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_latin_text_pipeline = None\n",
    "with open(os.path.join('../detecting_loanwords', 'process_latin_text_pipeline.0.20.2.joblib'), 'rb') as reader:\n",
    "    process_latin_text_pipeline = joblib.load(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom function for a transformer that uses our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drop_greek(string_matrix, max_len=25):\n",
    "    results = []\n",
    "    for sentence in string_matrix:\n",
    "        unseen_X = process_latin_text_pipeline.fit_transform([(sentence)])\n",
    "        if unseen_X and len(unseen_X[0]) > 1:\n",
    "            arr = greek_cls.predict(\n",
    "                sparse.csr_matrix(np.array([word_to_features(word, max_len) \n",
    "                                            for sent in unseen_X \n",
    "                                            for word in sent])))\n",
    "            arr = patch_cluster_holes(arr)\n",
    "            purified_words = [word for idx, word in enumerate(unseen_X[0]) if arr[idx] == 0 ]\n",
    "            found_greek = merge_words(extract_words(unseen_X[0], *run_length_encoding(arr)))  # works with sent\n",
    "            if found_greek:                \n",
    "                LOG.debug(found_greek)\n",
    "                LOG.debug('purified words %s', purified_words)\n",
    "            results.append(purified_words) \n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a helper transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_sentence_matrix(string_matrix):\n",
    "    results = []\n",
    "    for sentence in string_matrix:\n",
    "        sent =[]\n",
    "        for word in sentence:            \n",
    "            if word and not isinstance(word, str):\n",
    "                LOG.warning('fail, expected word as string: %s' , word)\n",
    "        results.append(sentence)\n",
    "    LOG.info('X size: %s', len(results))\n",
    "    return results \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('verify', FunctionTransformer(verify_sentence_matrix, validate=False)),\n",
    "    ('jv_transform', FunctionTransformer(jv_transform, validate=False)),\n",
    "    ('drop_editorial', FunctionTransformer(drop_editorial, validate=False)),    \n",
    "    ('drop_enclitics', FunctionTransformer(drop_enclitics, validate=False)),\n",
    "    ('drop_fringe_punctuation', FunctionTransformer(drop_fringe_punctuation, validate=False)),\n",
    "    ('smart_lower', smart_lower_transformer),\n",
    "    ('trier', trie_transformer),\n",
    "    ('drop_non_lower', FunctionTransformer(drop_non_lower, validate=False)),\n",
    "    ('drop_arabic_numeric', FunctionTransformer(drop_arabic_numeric, validate=False)),\n",
    "    ('drop_greek', FunctionTransformer(drop_greek, validate=False)),\n",
    "    ('verify2', FunctionTransformer(verify_sentence_matrix, validate=False)),\n",
    "    ('drop_all_caps', FunctionTransformer(drop_all_caps, validate=False)),\n",
    "    ('verify3', FunctionTransformer(verify_sentence_matrix, validate=False)),\n",
    "    ('drop_probable_entities', FunctionTransformer(drop_probable_entities, validate=False)),\n",
    "    ('drop_empty_lists', FunctionTransformer(drop_empty_lists, validate=False)),\n",
    "    ('drop_short_sentences', FunctionTransformer(drop_short_sentences, validate=False)) \n",
    "])\n",
    "\n",
    "# TODO incorporate above into the more refined pipeline below\n",
    "\n",
    "process_text_model = Pipeline([\n",
    "#     ('fix_text', FunctionTransformer(fix_text, validate=False)),\n",
    "    ('correct_camel_cases', FunctionTransformer(separate_camel_cases, validate=False)),\n",
    "    ('splice_hyphens', FunctionTransformer(splice_hyphens, validate=False)),\n",
    "    ('jv_transform', FunctionTransformer(jv_transform, validate=False)),  \n",
    "    ('accept_editorial', FunctionTransformer(accept_editorial, validate=False)),    \n",
    "    ('drop_enclitics', FunctionTransformer(drop_enclitics, validate=False)),\n",
    "    ('drop_fringe_punctuation', FunctionTransformer(drop_fringe_punctuation, validate=False)),\n",
    "    ('drop_all_punctuation', FunctionTransformer(drop_all_punctuation, validate=False)),    \n",
    "    ('drop_non_lower', FunctionTransformer(drop_non_lower, validate=False)),\n",
    "    ('drop_arabic_numeric', FunctionTransformer(drop_arabic_numeric, validate=False)),\n",
    "    ('drop_all_caps', FunctionTransformer(drop_all_caps, validate=False)),\n",
    "    ('divide_separate_words', FunctionTransformer(divide_separate_words, validate=False)),    \n",
    "    # Normally, for word vector building we would want to do the next step\n",
    "#     ('drop_probable_entities', FunctionTransformer(drop_probable_entities, validate=False)),\n",
    "    ('drop_empty_lists', FunctionTransformer(drop_empty_lists, validate=False)),\n",
    "    ('drop_short_sentences', FunctionTransformer(drop_short_sentences, validate=False)) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_reader = get_corpus_reader(language='latin', corpus_name='latin_text_latin_library')\n",
    "# corpus_reader._fileids = ['pliny.ep1.txt']   # ['catullus.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1038668/1038668 [00:02<00:00, 484467.19it/s]\n",
      "INFO : X size: 1038668\n",
      "WARNING : Excessive word length 30 for perseuerantiamtantasustinentem, truncating to 25\n",
      "WARNING : Excessive word length 26 for repromissionesobturauerunt, truncating to 25\n",
      "WARNING : Excessive word length 26 for impudentioremintemperantia, truncating to 25\n",
      "WARNING : Excessive word length 26 for iniuriarumquetollerabilium, truncating to 25\n"
     ]
    }
   ],
   "source": [
    "#X = model.fit_transform(tqdm(list(corrected_reader.sents())))\n",
    "X = model.fit_transform(tqdm(list(corpus_reader.sents())))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Output\n",
    "The pipeline log statements show a few items that were not handled properly, such a improperly joined words that the Trie Transformer couldn't correct, however, one \"perseuerantiamtantasustinentem\" is due to several words being improperly joined (\"perseuerant\", \"iam\", \"tanta\", \"sustinentem\")--and trying to automatically recover from that level of textual corruption would strain credulity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[block:block+10 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block +=100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(X):\n",
    "    distinct=set()\n",
    "    for sentence in X:\n",
    "        for word in sentence:\n",
    "            distinct.add(word)\n",
    "    return distinct\n",
    "\n",
    "corpus_words = get_unique_words(X)\n",
    "print(len(corpus_words))\n",
    "# catullus before dropping greek: 5235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Save the preprocessed corpus so it can be assessed, and reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X, open('latin.corpus.X.processed.pkl', \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
