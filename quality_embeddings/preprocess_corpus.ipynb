{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing a corpus for vectorization\n",
    "One of the best ways to speed your research and development time is to checkpoint your work so that you can reuse portions of your pipeline that don't change. Fortunately, as long as you don't use lambda statement in you scikit learn pipelines, you should be able to pickle and reload your pipelines, and furthermore, the resulting matrix output can be pickled, and reloaded. However, before we get to demonstrate impressive reuse, we typically have to deal with dirty data.\n",
    "## Dealing with dirty data\n",
    "A good rule of thumb is: if you don't think your data is dirty, you're probably not looking at it.\n",
    "Let's preprocess the Latin Library corpus and show some transformations that can be done to auto correct some data quality issues.\n",
    "\n",
    "### In this notebook we will:\n",
    "1. Create a reusable text processing pipeline\n",
    "1. Assess the pipeline processing\n",
    "1. Divide and conquer - multiprocess a corpus in sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from scipy import sparse\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from cltk.corpus.readers import get_corpus_reader, assemble_corpus\n",
    "from cltk.corpus.latin.latin_library_corpus_types import corpus_directories_by_type, corpus_texts_by_type\n",
    "from cltk.prosody.latin.string_utils import punctuation_for_spaces_dict\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from cltk.prosody.latin.scansion_constants import ScansionConstants\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from cltk.corpus.latin.latin_library_corpus_types import corpus_directories_by_type, corpus_texts_by_type\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from cltk.utils.matrix_corpus_fun import (\n",
    "    separate_camel_cases,\n",
    "    splice_hyphens,\n",
    "    drop_empty_lists,\n",
    "    drop_non_lower,\n",
    "    drop_probable_entities,\n",
    "    drop_editorial,\n",
    "    drop_arabic_numeric,\n",
    "    drop_all_caps,\n",
    "    jv_transform,\n",
    "    accept_editorial,    \n",
    "    drop_enclitics ,\n",
    "    drop_fringe_punctuation, \n",
    "    divide_separate_words,\n",
    "    drop_all_punctuation,\n",
    "    drop_short_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add our common library to the path and load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import inspect\n",
    "from pathlib import Path \n",
    "currentdir = Path.cwd()\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir) \n",
    "from mlyoucanuse.featurize_text_fun import featurize, vectorize_features\n",
    "from mlyoucanuse.smart_lower_transformer import SmartLowerTransformer\n",
    "from mlyoucanuse.trie_transformer import TrieTransformer\n",
    "from mlyoucanuse.featurize_text_fun import word_to_features\n",
    "from mlyoucanuse.matrix_fun import run_length_encoding, extract_words, patch_cluster_holes, merge_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = logging.getLogger('preprocess_corpus')\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Smart Lower transformer and show how it's used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['leuissima', 'virum', 'cano'], ['perlucent', 'Arenas', 'stuff']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_lower_transformer = SmartLowerTransformer(lower_only_file=os.path.join(currentdir, 'latin.words.always.lower.txt' ))\n",
    "smart_lower_transformer.transform([['Leuissima', 'virum', 'cano'],\n",
    "                                        ['perlucent', 'Arenas', 'stuff']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Trie Transformer and show how it's used to autocorrect corrupt text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['maturitatem', 'perueniunt'],\n",
       " ['radicibus', 'subministres'],\n",
       " ['peregrinationes', 'habere'],\n",
       " ['uersibus', 'disertissimis'],\n",
       " ['crudelitatis', 'consuetudinem'],\n",
       " ['adiciebat', 'contrahendam'],\n",
       " ['translationes', 'inprobas']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'latin_word_trie.pkl'\n",
    "trie_transformer = TrieTransformer(trie_file=filename)\n",
    "trie_transformer.transform([\n",
    "    # actually found in latin library\n",
    "    ['maturitatemperueniunt'],\n",
    "    ['radicibussubministres'],\n",
    "    ['peregrinationeshabere'],\n",
    "    ['uersibusdisertissimis'],\n",
    "    ['crudelitatisconsuetudinem'],\n",
    "    ['adiciebatcontrahendam'],\n",
    "    ['translationesinprobas']     \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our classifier from another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_cls = None\n",
    "with open(os.path.join('../detecting_loanwords', 'is_transliterated_greek.mdl.0.20.2.joblib'), 'rb') as reader:\n",
    "    greek_cls = joblib.load(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our text processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_latin_text_pipeline = None\n",
    "with open(os.path.join('../detecting_loanwords', 'process_latin_text_pipeline.0.20.2.joblib'), 'rb') as reader:\n",
    "    process_latin_text_pipeline = joblib.load(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom function for a transformer that uses our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_greek(string_matrix, max_len=25):\n",
    "    results = []\n",
    "    for sentence in string_matrix:\n",
    "        unseen_X = process_latin_text_pipeline.fit_transform([(sentence)])\n",
    "        if unseen_X and len(unseen_X[0]) > 1:\n",
    "            arr = greek_cls.predict(\n",
    "                sparse.csr_matrix(np.array([word_to_features(word, max_len) \n",
    "                                            for sent in unseen_X \n",
    "                                            for word in sent])))\n",
    "            arr = patch_cluster_holes(arr)\n",
    "            purified_words = [word for idx, word in enumerate(unseen_X[0]) if arr[idx] == 0 ]\n",
    "            found_greek = merge_words(extract_words(unseen_X[0], *run_length_encoding(arr)))  # works with sent\n",
    "            if found_greek:                \n",
    "                LOG.debug(found_greek)\n",
    "                LOG.debug('purified words %s', purified_words)\n",
    "            results.append(purified_words) \n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a helper transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_sentence_matrix(string_matrix):\n",
    "    results = []\n",
    "    for sentence in string_matrix:\n",
    "        sent =[]\n",
    "        for word in sentence:            \n",
    "            if word and not isinstance(word, str):\n",
    "                LOG.warning('fail, expected word as string: %s' , word)\n",
    "        results.append(sentence)\n",
    "    LOG.info('X size: %s', len(results))\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_process_text = Pipeline([\n",
    "    ('verify', FunctionTransformer(verify_sentence_matrix, validate=False)),\n",
    "    ('correct_camel_cases', FunctionTransformer(separate_camel_cases, validate=False)), \n",
    "    ('splice_hyphens', FunctionTransformer(splice_hyphens, validate=False)), \n",
    "    ('jv_transform', FunctionTransformer(jv_transform, validate=False)),\n",
    "    ('drop_editorial', FunctionTransformer(drop_editorial, validate=False)),    \n",
    "    ('drop_enclitics', FunctionTransformer(drop_enclitics, validate=False)),\n",
    "    ('drop_fringe_punctuation', FunctionTransformer(drop_fringe_punctuation, validate=False)),\n",
    "    ('smart_lower', smart_lower_transformer),\n",
    "    ('trier', trie_transformer),\n",
    "    ('drop_non_lower', FunctionTransformer(drop_non_lower, validate=False)),\n",
    "    ('drop_arabic_numeric', FunctionTransformer(drop_arabic_numeric, validate=False)),\n",
    "    ('drop_all_punctuation', FunctionTransformer(drop_all_punctuation, validate=False)),   \n",
    "    ('divide_separate_words', FunctionTransformer(divide_separate_words, validate=False)),   \n",
    "    ('drop_greek', FunctionTransformer(drop_greek, validate=False)),\n",
    "    ('verify2', FunctionTransformer(verify_sentence_matrix, validate=False)),\n",
    "    ('drop_all_caps', FunctionTransformer(drop_all_caps, validate=False)),\n",
    "    ('drop_probable_entities', FunctionTransformer(drop_probable_entities, validate=False)),\n",
    "    ('drop_empty_lists', FunctionTransformer(drop_empty_lists, validate=False)),\n",
    "    ('drop_short_sentences', FunctionTransformer(drop_short_sentences, validate=False)) ,\n",
    "    ('verify3', FunctionTransformer(verify_sentence_matrix, validate=False))\n",
    "])\n",
    "# Other function transformations worth considering:\n",
    "#  ('accept_editorial', FunctionTransformer(accept_editorial, validate=False)),    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_reader = get_corpus_reader(language='latin', corpus_name='latin_text_latin_library')\n",
    "corpus_reader._fileids = ['pliny.ep1.txt']   # ['catullus.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:00<00:00, 149200.96it/s]\n",
      "INFO : X size: 501\n",
      "INFO : X size: 422\n",
      "INFO : X size: 422\n",
      "INFO : X size: 404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = model.fit_transform(tqdm(list(corrected_reader.sents())))\n",
    "X = process_text_model.fit_transform(tqdm(list(corpus_reader.sents())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Output\n",
    "The pipeline log statements show a few items that were not handled properly, such a improperly joined words that the Trie Transformer couldn't correct, however, one \"perseuerantiamtantasustinentem\" is due to several words being improperly joined (\"perseuerant\", \"iam\", \"tanta\", \"sustinentem\")--and trying to automatically recover from that level of textual corruption would strain credulity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To inspect the corpus processing output, uncomment the lines below\n",
    "# X[:10]\n",
    "# block=10 \n",
    "# X[block:block + 10]\n",
    "# block +=100 \n",
    "# X[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2692\n"
     ]
    }
   ],
   "source": [
    "def get_unique_words(X):\n",
    "    distinct = set()\n",
    "    for sentence in X:\n",
    "        for word in sentence:\n",
    "            distinct.add(word)\n",
    "    return distinct\n",
    "\n",
    "corpus_words = get_unique_words(X)\n",
    "print(len(corpus_words))\n",
    "# catullus before dropping greek: 5235\n",
    "# pliny epistles 1: 2686 words, 400 sentences\n",
    "# pliny epistles 1: 2692, 404 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide and conquer: finding discrete sections of a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Era: republican files: 208 \n",
      "Era: augustan files: 302 \n",
      "Era: early_silver files: 247 \n",
      "Era: late_silver files: 118 \n",
      "Era: old files: 29 \n",
      "Era: christian files: 347 \n",
      "Era: medieval files: 12 \n",
      "Era: renaissance files: 3 \n",
      "Era: neo_latin files: 75 \n",
      "Era: misc files: 405 \n",
      "Era: early files: 1 \n"
     ]
    }
   ],
   "source": [
    "for key in corpus_directories_by_type.keys():\n",
    "    reader = get_corpus_reader(corpus_name='latin_text_latin_library', language='latin')\n",
    "    reader = assemble_corpus(reader, [key],\n",
    "                          corpus_directories_by_type, \n",
    "                          corpus_texts_by_type )\n",
    "    print(f'Era: {key} files: {len(reader._fileids)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_filename_latin = 'pipeline_process_text_latin.pkl'\n",
    "\n",
    "with open(pipeline_filename_latin, 'wb') as dumper:\n",
    "    pickle.dump(pipeline_process_text, dumper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a reusable function for processing the corpus sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus_section(section):  \n",
    "    from time import time\n",
    "    start = time()\n",
    "    reader = get_corpus_reader(corpus_name='latin_text_latin_library', language='latin')\n",
    "    reader = assemble_corpus(reader, [section],\n",
    "                          corpus_directories_by_type, \n",
    "                          corpus_texts_by_type)\n",
    "    pipeline_file  = 'pipeline_process_text_latin.pkl'\n",
    "    with open (pipeline_file, 'rb') as loader:\n",
    "        pipeline = pickle.load(loader)\n",
    "    X = pipeline.fit_transform(list(reader.sents()))\n",
    "    pickle.dump(X, open('latin_library.corpus.{}.processed.pkl'.format(section), \"wb\"))\n",
    "    del X\n",
    "    del reader\n",
    "    del pipeline\n",
    "    return (section, time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : X size: 191\n",
      "INFO : X size: 88\n",
      "INFO : X size: 74\n"
     ]
    }
   ],
   "source": [
    "process_corpus_section('early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : X size: 219\n",
      "INFO : X size: 153\n",
      "INFO : X size: 149\n"
     ]
    }
   ],
   "source": [
    "process_corpus_section('renaissance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : X size: 4164\n",
      "WARNING : Excessive word length 27 for souramagnificentissimamente, truncating to 25\n",
      "INFO : X size: 3060\n",
      "INFO : X size: 2952\n"
     ]
    }
   ],
   "source": [
    "process_corpus_section('medieval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras_to_do =['early', 'renaissance', 'medieval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "# pool = Pool(processes=len(eras_to_do)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pool.apply_async(process_corpus_section, eras_to_do)\n",
    "# results\n",
    "# pool.close()\n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : X size: 47994\n",
      "INFO : X size: 25302\n",
      "INFO : X size: 29540\n",
      "INFO : X size: 29540\n",
      "INFO : X size: 60027\n",
      "INFO : X size: 82644\n",
      "INFO : X size: 121570\n",
      "INFO : X size: 138657\n",
      "WARNING : Excessive word length 30 for perseuerantiamtantasustinentem, truncating to 25\n",
      "INFO : X size: 324958\n",
      "WARNING : Excessive word length 26 for repromissionesobturauerunt, truncating to 25\n",
      "WARNING : Excessive word length 26 for impudentioremintemperantia, truncating to 25\n",
      "WARNING : Excessive word length 28 for Thensaurochrysonicochrysides, truncating to 25\n",
      "WARNING : Excessive word length 26 for iniuriarumquetollerabilium, truncating to 25\n",
      "WARNING : Excessive word length 48 for Ythalonimualoniuthsicorathiisthymhimihymacomsyth, truncating to 25\n",
      "WARNING : Excessive word length 28 for combaepumamitalmetlotiambeat, truncating to 25\n",
      "WARNING : Excessive word length 28 for iulecantheconaalonimbalumbar, truncating to 25\n",
      "WARNING : Excessive word length 26 for hunesobinesubicsillimbalim, truncating to 25\n",
      "WARNING : Excessive word length 32 for esseantidamossonalemuedubertefet, truncating to 25\n",
      "WARNING : Excessive word length 33 for donobunhuneccilthumucommucroluful, truncating to 25\n",
      "WARNING : Excessive word length 36 for altanimauosduberithemhuarcharistolem, truncating to 25\n",
      "WARNING : Excessive word length 28 for sittesedanecnasotersahelicot, truncating to 25\n",
      "WARNING : Excessive word length 29 for alemusdubertimurmucopsuistiti, truncating to 25\n",
      "WARNING : Excessive word length 39 for aoccaaneclictorbodesiussilimlimmimcolus, truncating to 25\n",
      "INFO : X size: 22196\n",
      "INFO : X size: 21482\n",
      "INFO : X size: 25265\n",
      "INFO : X size: 24548\n",
      "INFO : X size: 25265\n",
      "INFO : X size: 24548\n",
      "INFO : X size: 29145\n",
      "INFO : X size: 24765\n",
      "INFO : X size: 56983\n",
      "INFO : X size: 55485\n",
      "WARNING : Excessive word length 28 for recollegissetanimaduertisset, truncating to 25\n",
      "WARNING : Excessive word length 26 for conprehendimustemperantiam, truncating to 25\n",
      "INFO : X size: 74269\n",
      "INFO : X size: 70308\n",
      "INFO : X size: 107449\n",
      "INFO : X size: 102628\n",
      "INFO : X size: 123475\n",
      "INFO : X size: 118063\n",
      "WARNING : Excessive word length 26 for Anebanensisepiscopidebitae, truncating to 25\n",
      "INFO : X size: 220449\n",
      "INFO : X size: 173140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('misc', 236195.42858600616),\n",
       " ('republican', 100088.08493995667),\n",
       " ('augustan', 82857.92074799538),\n",
       " ('early_silver', 125404.99969887733),\n",
       " ('late_silver', 44229.48902797699),\n",
       " ('christian', 135963.50766682625),\n",
       " ('neo_latin', 50776.54932188988),\n",
       " ('old', 70243.92168569565),\n",
       " ('neo_latin', 50763.268376111984)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eras_to_do = ['misc', 'republican', 'augustan', 'early_silver',\n",
    "#             'late_silver',\n",
    "              'christian', \n",
    "#               'neo_latin',\n",
    "              'old']\n",
    "pool = Pool(processes=len(eras_to_do)+1)\n",
    "pool.map(process_corpus_section, eras_to_do)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
