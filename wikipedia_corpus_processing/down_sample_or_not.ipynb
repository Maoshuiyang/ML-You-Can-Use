{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Down Sample or Not\n",
    "Sample is an easy hyper parameter to set, but should you use it at all?\n",
    "\n",
    "   `sample (float, optional) â€“ The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).`\n",
    "   \n",
    "The parameter attempts to approximate the effect of removing stopwords, which are words of the highest frequency that don't contribute much in the way of value to a context. Here's the effect of the sample parameter in practice:\n",
    "\n",
    "INFO : collected 6089246 word types from a corpus of 1566076356 raw words and 83830773 sentences\n",
    "\n",
    "INFO : sample=1e-05 downsamples 4072 most-common words\n",
    "\n",
    "INFO : collected 79260 word types from a corpus of 10277876 raw words and 593596 sentences\n",
    "\n",
    "...\n",
    "\n",
    "0.00001  # sample=1e-05 downsamples 4158 most-common words\n",
    "\n",
    "...\n",
    "\n",
    "sample=0.001 downsamples 32 most-common words\n",
    "    \n",
    "    \n",
    "## Let's look at how using it could change your results:\n",
    "We'll scan our wikipedia English corpus and count the occurences of distinct words. Then we'll pull the top 200 words and see how many would be down sampled and erased from vector processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 84231634 wikimedia.en.processed.cor\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l wikimedia.en.processed.cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1566161633 wikimedia.en.processed.cor\r\n"
     ]
    }
   ],
   "source": [
    "! wc -w wikimedia.en.processed.cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "\n",
    "with open('wikimedia.en.processed.cor', 'rt') as reader:\n",
    "    for line in reader:\n",
    "        words =line.strip().split()\n",
    "        for word in words:\n",
    "            counter.update({word:1})            \n",
    "\n",
    "counter.most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Although many linking words of dubious value are in the top 200, there are many that likely provide valuable information links, such as:\n",
    "* ('New', 1599801),\n",
    "* ('United', 1312294),\n",
    "* ('University', 1228325),\n",
    "* ('American', 1160728),\n",
    "* ('town', 662736),\n",
    "* ('song', 660788),\n",
    "* ('public', 651966),\n",
    "* ('building', 650227)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better is to filter stopwords out during preprocessing the corpus, and not to set a down sample rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
