{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing English Wikipedia corpus\n",
    "\n",
    "There are a number of corrections that will improve the quality of our word vector representations. We'll iterate on the preprocessing step to create a better word vector.\n",
    "\n",
    "Common preprocessing steps are:\n",
    "* dropping numbers\n",
    "* dropping punctuation\n",
    "* coercing to lower case\n",
    "* stemming\n",
    "* lemmatization\n",
    "\n",
    "However, some of the operations can be undesirably lossy, e.g.:\n",
    " * dropping the hypen can remove certain word combinations\n",
    "     * the word \"email\" was once written \"e-mail\"\n",
    " * Coercing to lower case squashes the dimensionality of entities (capitalized nouns and Proper names)\n",
    "     * \"Edward Said\" becomes \"edward said\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from cltk.prosody.latin.string_utils import punctuation_for_spaces_dict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words as unix_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyphen has same ascii and unicode value: 45\n"
     ]
    }
   ],
   "source": [
    "replacer = punctuation_for_spaces_dict()\n",
    "print(f\"Hyphen has same ascii and unicode value: {ord('-')}\")\n",
    "del replacer[45] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madam  I m a Adam \n",
      "The word email  used to be e-mail \n",
      "But hyphen assimilation isn t always possible based on sense  beauty-obsessed\n"
     ]
    }
   ],
   "source": [
    "print(\"Madam, I'm a Adam!\".translate(replacer))\n",
    "print(\"The word email, used to be e-mail.\".translate(replacer))\n",
    "print(\"But hyphen assimilation isn't always possible based on sense: beauty-obsessed\".translate(replacer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_numeric = re.compile('.*[0-9]+.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_words = set([tmp for tmp in unix_dictionary.words() if tmp.lower() == tmp]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikimedia.en.processed.cor', 'wt') as writer:\n",
    "    with open('wikimedia.en.cor', 'rt') as reader:\n",
    "        for line in reader:            \n",
    "            line = line.strip()\n",
    "            words = line.translate(replacer).split()\n",
    "            # Check the first word of each sentence, if it typically occurs in lower case, coerce to lower\n",
    "            if words and words[0][0].upper() == words[0][0]:\n",
    "                if words[0] in lower_words:\n",
    "                    words[0] = words[0].lower()\n",
    "            cleaned_words = [tmp for tmp in words\n",
    "                                 if not has_numeric.match(tmp)                                   \n",
    "                                 and len(tmp) > 1 \n",
    "                                 and tmp not in stop_words ]\n",
    "            writer.write(' '.join(cleaned_words))\n",
    "            writer.write('\\n')                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we're ready to create a quality word vector from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: notebook testing of the cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why has_numeric\n",
    "'salt-making'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## alpha and non-alpha match are bit problematic\n",
    "# if truly needed we should scan the corpus and pull out a set of all the characters we want.\n",
    "non_alpha = re.compile(r'.*[^a-zA-Z\\-]+')\n",
    "non_alpha.match('salt-Making')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alpha.match('abc120')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alpha.match('120abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_numeric.match('abc120')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_numeric.match('120abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_numeric.match('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_numeric.match('13')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
