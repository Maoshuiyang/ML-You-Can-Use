{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a corpus file from the LA wikipedia dump\n",
    "In this notebook we'll:\n",
    "1. Process a wikipedia dump that has been transformed into a series of JSONL files\n",
    "1. Select text section that have contiguous group of sentences, so as to yield a higher quality embedding later on\n",
    "1. Tokenize the senteces and words \n",
    "1. Format the output into the Gensim `cor` file format for better stream processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: \n",
    "To run this notebook on a fresh install, you should first run the scripts:\n",
    "* `install_latin_wikipedia_data.sh`\n",
    "* `preprocess_latin_wikipedia_files`\n",
    "\n",
    "as described in this folder's README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumerate the JSONL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 candidate JSONL files\n"
     ]
    }
   ],
   "source": [
    "current_dir = pathlib.Path.cwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "json_files = glob.glob(os.path.join (parent_dir, 'data', 'latin_wikipedia', 'jsonl' ,'**') , recursive=True )\n",
    "print(f\"{len(json_files)} candidate JSONL files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a sentence tokenizer from CLTK, and demonstrate its use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arma virumque cano.', 'odi et amo.', 'Et tu, Brute?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(os.path.expanduser('~/cltk_data/latin/model/latin_models_cltk/tokenizers/sentence'),'latin_punkt.pickle'), 'rb') as loader:\n",
    "    sentence_tokenizer = pickle.load(loader)\n",
    "\n",
    "sentence_tokenizer.tokenize(\"arma virumque cano. odi et amo. Et tu, Brute?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Word Tokenizer and demonstrate its use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Et', 'tu', ',', 'Brute', '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cltk.tokenize.word import WordTokenizer\n",
    "word_tokenizer = WordTokenizer(language='latin')\n",
    "word_tokenizer.tokenize('Et tu, Brute?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Regex pattern to swallow tag text and parenthetical explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtlt_stripper = re.compile(r'<.*>')\n",
    "paren_stripper = re.compile(r'\\(.*\\)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing/verifcation lines commented out \n",
    "#text_data =[]\n",
    "with open ('wikimedia.la.cor', 'wt')as writer:\n",
    "    for filename in json_files:\n",
    "        if os.path.isdir(filename): # skip directories\n",
    "            continue\n",
    "        with open (filename, 'rt') as loader:\n",
    "            for line in loader:\n",
    "                obj = json.loads(line)\n",
    "                if 'text' in obj:\n",
    "                    text = obj['text']                    \n",
    "                    # skip headings, one line sentence pages\n",
    "                    if len(sentence_tokenizer.tokenize(text)) > 1: \n",
    "                        # drop parenthetical and angle bracket info\n",
    "                        text = paren_stripper.sub('', gtlt_stripper.sub('', text)).strip()\n",
    "                        parts = text.split('\\n')\n",
    "                        for part in parts:\n",
    "                            # skip heading sections, and one line sentence sections\n",
    "                            if len(sentence_tokenizer.tokenize(part)) > 1: \n",
    "                                for sent in sentence_tokenizer.tokenize(part):                                \n",
    "                                    if sent:\n",
    "                                        words = word_tokenizer.tokenize(sent)\n",
    "                                        # Testing/verifcation lines commented out\n",
    "                                        #text_data.append(sent)\n",
    "                                        writer.write(' '.join(words))\n",
    "                                        writer.write('\\n')\n",
    "                                        \n",
    "# Testing/verifcation lines commented out                                     \n",
    "# for line in text_data[:5]:                \n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5874245 wikimedia.la.cor\r\n"
     ]
    }
   ],
   "source": [
    "! wc -w wikimedia.la.cor\n",
    "# text_data[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the Latin wikipedia corpus is reading for pipeline processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
