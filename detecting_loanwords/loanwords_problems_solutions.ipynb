{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem of Loanwords: Detection and Remedies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Loanword?\n",
    "They are real: https://en.wikipedia.org/wiki/Loanword but they often overlooked in everyday use. Take this example conversation:\n",
    "* \"It was nice seeing you again. Adios amigo.\"\n",
    "* \"Vaya con Dios!\"\n",
    "\n",
    "#### The last two sentences are loanwords, borrowings from Spanish. Most participants can navigate and interpret conversations even if they don't know the exact translations of the loanwords.\n",
    "\n",
    "However, when doing Natural Language Processing loanwords may become a pesky problem, for example:\n",
    "* If you are trying to generate a high quality embedding\n",
    "    * you many need to detect and drop out the loanwords since they are out of scope for typical monolingual embeddings.\n",
    "* If your application is translating a sequence of sentences\n",
    "    * you may need to switch models to create a better translation of a loanword sequence.\n",
    "* Some loanwords may occur in both languages \n",
    "    * you will need to use other heuristic measures to increase your precision and recall in detection.\n",
    "#### etc.\n",
    "### So let's look at how we can detect and work around these problem areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook you will:\n",
    "1. Perform ETL (Extract-Transform-Load)\n",
    "    1. Extract data using a Corpus Reader object\n",
    "    1. Transform the data using a reusable, composable Scikit-Learn Pipeline object\n",
    "    1. Load the data into a matrix and train a classifier\n",
    "1. Train several classifiers and select the best algorithm for the data\n",
    "1. Use GridSearch to tune hyperparameters to achieve the best algorithm performance\n",
    "1. Use the classifier to predict the classification of each word\n",
    "1. Extract whole phrases by clustering occurences of predicted words \n",
    "1. Save the pipelines and classifier for reuse at runtime\n",
    "1. Record the trained classifier model's provenance\n",
    "1. Examine some unseen, untrained data to discover how well the classifier generalizes to unseen data\n",
    "\n",
    "## Our problem: Detect Transliterated Greek in Classical Latin Authors\n",
    "### Why? \n",
    "### To make a high quality word embedding, using a relatively limited corpus, it's important to filter out foreign words. Often Latin authors will quote Greek authors and transliterate the Greek into Latin equivalents; however the resulting words often haven't been generally adopted in the language. True, some transliterated Greek words are valid Latin words, but we aren't concerned about a word here or there, but rather clusters of foreign words in the source language. \n",
    "### The data sets are from CLTK (Classical Language Toolkit http://cltk.org/ ):\n",
    "* The Latin Library\n",
    "    * Julius Caesar\n",
    "    * Prudentius\n",
    "    * Eutropius\n",
    "* The Perseus Library Greek Texts\n",
    "    * The Works of Plato\n",
    "    * Homer's Odyssey\n",
    "* We will augment our Latin data set using probability distributions in tandem with CLTK's lemmatization dictionary.\n",
    "\n",
    "#### The probability distributions come from our other notebooks:\n",
    "* `building_language_model/make_frequency_distribution.ipynb` \n",
    "* `detecting_loanwords/make_frequency_distribution_greek_transliterated.ipynb`\n",
    "\n",
    "#### The Greek will be transliterated into Latin. \n",
    "#### We will use our classifier to examine the corpus of Pliny the Younger to detect the use of transliterated Greek words, and we'll assess the classifier's effectiveness on the entire Latin Library corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import site\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from scipy import sparse\n",
    " \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from joblib import dump, load\n",
    "import sklearn\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.prosody.latin.scansion_constants import ScansionConstants\n",
    "from cltk.prosody.latin.string_utils import remove_punctuation_dict\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "from cltk.utils.featurization import word_to_features\n",
    "from cltk.utils.file_operations import md5\n",
    "from cltk.utils.matrix_corpus_fun import (\n",
    "    distinct_words,\n",
    "    separate_camel_cases,\n",
    "    drop_empty_lists,\n",
    "    drop_non_lower,\n",
    "    drop_arabic_numeric,\n",
    "    drop_all_caps,\n",
    "    drop_empty_strings,\n",
    "    jv_transform,\n",
    "    splice_hyphens,\n",
    "    accept_editorial,\n",
    "    profile_chars,\n",
    "    demacronize,\n",
    "    drop_enclitics,\n",
    "    drop_fringe_punctuation,\n",
    "    divide_separate_words,\n",
    "    drop_all_punctuation)\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add parent directory to path so we can access our common code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlyoucanuse.romanizer import Romanizer, romanizer_transform  \n",
    "from mlyoucanuse.aeoe_replacer import aeoe_transform\n",
    "from mlyoucanuse.matrix_fun import (run_length_encoding,\n",
    "                                    extract_words,                                     \n",
    "                                    patch_cluster_holes)\n",
    "from mlyoucanuse.featurize_text_fun import word_to_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn on logging, primarily so that library methods may report warnings, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = logging.getLogger('make_model')\n",
    "LOG.addHandler(logging.NullHandler())\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a CorpusReader and select only the text files of Prudentius, Caesar and Eutropius. \n",
    "#### As shown in the appendix of this notebook, the authors of this seed set have a low incidence of using transliterated Greek words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:make_model:available good files 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['caesar/alex.txt',\n",
       " 'caesar/bc1.txt',\n",
       " 'caesar/bc2.txt',\n",
       " 'caesar/bc3.txt',\n",
       " 'caesar/bellafr.txt',\n",
       " 'caesar/gall1.txt',\n",
       " 'caesar/gall2.txt',\n",
       " 'caesar/gall3.txt',\n",
       " 'caesar/gall4.txt',\n",
       " 'caesar/gall5.txt',\n",
       " 'caesar/gall6.txt',\n",
       " 'caesar/gall7.txt',\n",
       " 'caesar/gall8.txt',\n",
       " 'caesar/hisp.txt',\n",
       " 'eutropius/eutropius1.txt',\n",
       " 'eutropius/eutropius10.txt',\n",
       " 'eutropius/eutropius2.txt',\n",
       " 'eutropius/eutropius3.txt',\n",
       " 'eutropius/eutropius4.txt',\n",
       " 'eutropius/eutropius5.txt',\n",
       " 'eutropius/eutropius6.txt',\n",
       " 'eutropius/eutropius7.txt',\n",
       " 'eutropius/eutropius8.txt',\n",
       " 'eutropius/eutropius9.txt',\n",
       " 'prudentius/prud.psycho.txt',\n",
       " 'prudentius/prud1.txt',\n",
       " 'prudentius/prud10.txt',\n",
       " 'prudentius/prud11.txt',\n",
       " 'prudentius/prud12.txt',\n",
       " 'prudentius/prud13.txt',\n",
       " 'prudentius/prud14.txt',\n",
       " 'prudentius/prud2.txt',\n",
       " 'prudentius/prud3.txt',\n",
       " 'prudentius/prud4.txt',\n",
       " 'prudentius/prud5.txt',\n",
       " 'prudentius/prud6.txt',\n",
       " 'prudentius/prud7.txt',\n",
       " 'prudentius/prud8.txt',\n",
       " 'prudentius/prud9.txt',\n",
       " 'suetonius/suet.caesar.txt',\n",
       " 'xylander/caesar.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latin_reader = get_corpus_reader('latin_text_latin_library', language='latin')\n",
    "ALL_FILE_IDS = list(latin_reader.fileids())\n",
    "good_files = [file for file in ALL_FILE_IDS\n",
    "              if 'prudentius' in file or\n",
    "              'caesar' in file or\n",
    "              'eutropius' in file]\n",
    "LOG.info('available good files %s', len(good_files))\n",
    "latin_reader._fileids = good_files\n",
    "good_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some unfamiliar entries\n",
    "questionable = ['caesar/alex.txt',\n",
    "                'caesar/hisp.txt',\n",
    "                'prudentius/prud.psycho.txt',\n",
    "                'suetonius/suet.caesar.txt',\n",
    "                'xylander/caesar.txt']\n",
    "for file in questionable:\n",
    "    good_files.remove(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom Scikit-learn Pipeline, and call the CorpusReader `words()` method to process the texts\n",
    "#### The functions used in the pipelines are doctest documented in the `corpus_cleaning` module\n",
    "#### The functions used and their order was developed iteratively by running the pipelines on actual data and carefully inspecting the results prior to runnning it through featurization. Always know your data!\n",
    "\n",
    "#### Lastly, we use the joblib library to save/pickle the pipeline so that it can be reloaded and reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['process_latin_text_pipeline.0.20.2.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_latin_text_pipeline = Pipeline([\n",
    "    ('separate_camel_cases', FunctionTransformer(separate_camel_cases, validate=False)),\n",
    "    ('splice_hyphens', FunctionTransformer(splice_hyphens, validate=False)),\n",
    "    ('jv_transform', FunctionTransformer(jv_transform, validate=False)),\n",
    "    ('aeoe_transform', FunctionTransformer(aeoe_transform, validate=False)),\n",
    "    ('accept_editorial', FunctionTransformer(accept_editorial, validate=False)),\n",
    "    ('drop_enclitics', FunctionTransformer(drop_enclitics, validate=False)),\n",
    "    ('drop_fringe_punctuation', FunctionTransformer(drop_fringe_punctuation, validate=False)),\n",
    "    ('drop_all_punctuation', FunctionTransformer(drop_all_punctuation, validate=False)),\n",
    "    ('drop_non_lower', FunctionTransformer(drop_non_lower, validate=False)),\n",
    "    ('drop_arabic_numeric', FunctionTransformer(drop_arabic_numeric, validate=False)),\n",
    "    ('drop_all_caps', FunctionTransformer(drop_all_caps, validate=False)),\n",
    "    ('divide_separate_words', FunctionTransformer(divide_separate_words, validate=False)),\n",
    "    ('drop_empty_lists', FunctionTransformer(drop_empty_lists, validate=False)),\n",
    "    ('drop_empty_strings', FunctionTransformer(drop_empty_strings, validate=False))])\n",
    "\n",
    "process_latin_text_pipeline_file = 'process_latin_text_pipeline.{}.joblib'.format(\n",
    "    sklearn.__version__)\n",
    "dump(process_latin_text_pipeline, process_latin_text_pipeline_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "X = process_latin_text_pipeline.fit_transform(tqdm([list(latin_reader.words())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyze the resulting matrix, by profiling the character occurences\n",
    "* Go back and adjust the pipeline as necessary\n",
    "* Turn the output into a distinct set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character distribution profile, total chars: 810414\n",
      "Counter({'e': 93827, 'i': 90246, 'u': 75553, 't': 67540, 'a': 65821, 's': 62825, 'r': 54357, 'n': 49472, 'o': 44779, 'm': 41221, 'c': 31563, 'p': 23317, 'l': 23269, 'd': 20970, 'b': 13064, 'q': 12561, 'g': 8357, 'f': 7372, 'x': 4467, 'h': 4172, 'C': 2521, 'A': 1473, 'I': 1185, 'P': 1119, 'H': 1079, 'S': 1078, 'R': 965, 'G': 770, 'Q': 607, 'M': 601, 'T': 557, 'U': 546, 'N': 529, 'L': 475, 'E': 469, 'D': 459, 'y': 399, 'B': 351, 'F': 211, 'O': 203, 'z': 23, 'Z': 16, 'K': 11, 'k': 10, 'X': 3, 'Å': 1})\n",
      "Number of distinct words in Eutropius/Prudentius/Caesar sample: 25,840\n"
     ]
    }
   ],
   "source": [
    "char_count = profile_chars(X)\n",
    "print('Character distribution profile, total chars:', sum(char_count.values()))\n",
    "print(char_count)\n",
    "distinct_good_latin = distinct_words(X)\n",
    "print(f'Number of distinct words in Eutropius/Prudentius/Caesar sample: {len(distinct_good_latin):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After running this notebook several times, we've decided to load in more training data, which is provide by the notebook:\n",
    "* `boosting_training_data.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['praeferrent', 'quiescebat', 'possidentium', 'dimittemus', 'quiescunt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_latin_words = []\n",
    "with open('latin.lemma.forms.txt', 'rt') as reader:\n",
    "    additional_latin_words = reader.read().split('\\n')\n",
    "random.sample(additional_latin_words, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional_latin_words: 175,970\n",
      "distinct_good_latin now: 180,068\n"
     ]
    }
   ],
   "source": [
    "print(f'additional_latin_words: {len(additional_latin_words):,}')\n",
    "distinct_good_latin= list(set(distinct_good_latin) | set(additional_latin_words))\n",
    "print(f'distinct_good_latin now: {len(distinct_good_latin):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the Greek texts of Homer and Plato (two of the most commonly quoted Greek authors)\n",
    "* Preprocess the text\n",
    "* Transliterate into Classical Latin\n",
    "\n",
    "#### We save this pipeline for reuse too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['process_greek_text_pipeline.0.20.2.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perseus_greek = get_corpus_reader(language='greek', corpus_name='greek_text_perseus')\n",
    "plato = [tmp for tmp in perseus_greek.fileids() if 'plato' in tmp]\n",
    "homer = [tmp for tmp in perseus_greek.fileids() if 'homer' in tmp]\n",
    "greek_texts = plato + homer\n",
    "\n",
    "process_greek_pipeline = Pipeline([\n",
    "    ('accept_editorial', FunctionTransformer(accept_editorial, validate=False)),  # problematic\n",
    "    ('romanizer', FunctionTransformer(romanizer_transform, validate=False)),\n",
    "    ('drop_fringe_punctuation', FunctionTransformer(drop_fringe_punctuation, validate=False)),\n",
    "    ('drop_all_punctuation', FunctionTransformer(drop_all_punctuation, validate=False)),\n",
    "    ('drop_arabic_numeric', FunctionTransformer(drop_arabic_numeric, validate=False)),  #ok\n",
    "    ('drop_empty_lists', FunctionTransformer(drop_empty_lists, validate=False)),  # problem?\n",
    "    ('drop_empty_strings', FunctionTransformer(drop_empty_strings, validate=False))  # problem?\n",
    "])\n",
    "\n",
    "process_greek_text_pipeline_file = 'process_greek_text_pipeline.{}.joblib'.format(\n",
    "    sklearn.__version__)\n",
    "dump(process_greek_pipeline, process_greek_text_pipeline_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:38<00:00, 38.49s/it]\n"
     ]
    }
   ],
   "source": [
    "X_greek_transliterated = process_greek_pipeline.fit_transform(tqdm([list(perseus_greek.words(greek_texts))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyze the transliterated Greek examples\n",
    "* Check character profiles for tuning\n",
    "* Create a set distinct words, with and without macrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character distribution profile of transliterated Greek:  Counter({'a': 6947729, 'e': 6728409, 'o': 6340811, 'i': 6283152, 't': 5699742, 'n': 5653401, 's': 4160857, 'h': 3578959, 'u': 2971810, 'p': 2577039, 'k': 2513678, 'ē': 2467247, 'r': 2167104, 'm': 2055580, 'ō': 1964400, 'l': 1932188, 'd': 1624732, 'g': 1175425, 'b': 240982, 'S': 191454, 'x': 175041, 'Ō': 162113, 'E': 148151, 'A': 136839, 'z': 110228, 'T': 78620, 'P': 70134, 'I': 51507, 'R': 47009, 'L': 46766, 'K': 40902, 'X': 40362, 'O': 29058, 'M': 27506, 'D': 27135, 'N': 25385, 'H': 17705, 'Z': 7541, 'Y': 6705, 'Ē': 5036, 'G': 1808, 'B': 1293, 'y': 719, 'F': 434, 'f': 227, 'c': 192, ' ': 120, 'V': 95, 'C': 85, 'v': 81, 'U': 37})\n",
      "48,478 distinct_transliterated_greek_examples\n",
      "47,023 distinct_demacronized_greek\n"
     ]
    }
   ],
   "source": [
    "print('Character distribution profile of transliterated Greek: ', profile_chars(X_greek_transliterated))\n",
    "distinct_transliterated_greek_examples = distinct_words(X_greek_transliterated)\n",
    "print(f'{len(distinct_transliterated_greek_examples):,} distinct_transliterated_greek_examples')\n",
    "distinct_demacronized_greek = distinct_words(demacronize(X_greek_transliterated))\n",
    "print(f'{len(distinct_demacronized_greek):,} distinct_demacronized_greek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how many words from the transliterated Greek words which have also appear in the Latin corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared_words: 490 : {'Sparte', 'Parmenide', 'Hippotades', 'times', 'exe', 'Admete', 'Cypria', 'Thebe', 'manenti', 'poma', 'Amphitrite', 'esse', 'saturo', 'antas', 'ant', 'peribolo', 'polos', 'lege', 'parergo', 'hos', 'Apollo', 'Letoi', 'rhetores', 'Troes', 'phantasmata', 'tauros', 'ergo', 'Nestor', 'doto', 'pedalio', 'ito', 'axioma', 'te', 'epistate', 'Helene', 'ago', 'aget', 'Aristophane', 'Aristotele', 'Lede', 'aletes', 'Erote', 'agre', 'Leontinos', 'ereto', 'deos', 'pedalia', 'Pausania', 'dogmati', 'mere', 'Rhodope', 'genus', 'mese', 'erosa', 'pino', 'Zenoni', 'toros', 'Theodore', 'Orpheus', 'Euripo', 'penes', 'pauet', 'metis', 'iambe', 'halo', 'elephanti', 'tende', 'tria', 'sus', 'throno', 'mones', 'omen', 'philo', 'meni', 'Naxos', 'pompas', 'age', 'dele', 'paries', 'Maia', 'Eo', 'Arete', 'Pheres', 'Eous', 'Apolloni', 'pono', 'pero', 'Protagora', 'o', 'peren', 'pege', 'heroes', 'stephano', 'metreta', 'r', 'mero', 'Asia', 'Homero', 'dein', 'ae', 'oleto', 'ero', 'metere', 'pro', 'aetas', 'noto', 'laro', 'thalamo', 'Polites', 'notio', 'eita', 'sto', 'metalla', 'pe', 'spei', 'ale', 'sin', 'homo', 'agoni', 'theatro', 'osi', 'Persephone', 'aloe', 'do', 'metabole', 'statera', 'daphne', 'geode', 'osse', 'at', 'emata', 'plana', 'ante', 'torno', 'id', 'sei', 'pare', 'pote', 'pales', 'prophetis', 'tot', 'hora', 'is', 'Triptolemo', 'pleto', 'non', 'aule', 'Diomedeos', 'melie', 'aloes', 'philosopho', 'leget', 'agones', 'lotos', 'hesperos', 'aut', 'ei', 'Mida', 'Laertiade', 'phthisis', 'se', 'Orion', 'emo', 'polite', 'logo', 'aspis', 'aer', 'Delo', 'anathema', 'dos', 'aure', 'lae', 'ex', 'poesi', 'protero', 'Thebas', 'Thebes', 'es', 'pontones', 'patri', 'aristas', 'die', 'ego', 'heroi', 'Euripide', 'pares', 'Lampetie', 'limo', 'athletas', 'luto', 'dolos', 'siderea', 'Eros', 'bibas', 'dogmata', 'prophetas', 'petas', 'doma', 'philosophia', 'heroa', 'periodo', 'Delio', 'potho', 'dromo', 'seu', 'det', 'polles', 'gemet', 'de', 'rhetori', 'nassa', 'admete', 'Troas', 'Gai', 'dis', 'nae', 'Lapithas', 'in', 'elateri', 'Minos', 'Paros', 'lis', 'pedo', 'Europes', 'auge', 'esto', 'labe', 'depote', 'ara', 'historia', 'limen', 'meis', 'edos', 'Pergama', 'bio', 'Eos', 'eant', 'olet', 'Dione', 'probata', 'agrio', 'demo', 'eo', 'Are', 'Herme', 'me', 'hamos', 'et', 'sidereos', 'Theseus', 'domo', 'antro', 'neo', 'horai', 'Anaxagoras', 'patris', 'orse', 'diametro', 'aleis', 'dote', 'topo', 'Athenas', 'morio', 'topazo', 'enen', 'Melete', 'rhetor', 'pedali', 'emes', 'nautas', 'eras', 'suas', 'sues', 'polo', 'Melantho', 'heroas', 'deest', 'no', 'messe', 'opta', 'ipsa', 'Euripides', 'oles', 'diae', 'Theodoro', 'asto', 'Minoa', 'xanthe', 'Mimas', 'geme', 'muri', 'lino', 'hebes', 'Perses', 'Epigenes', 'mori', 'eas', 'mel', 'hei', 'leges', 'Lemnos', 'idea', 'agonia', 'nota', 'melle', 'est', 'potero', 'a', 'pater', 'emat', 'alueis', 'hoste', 'dei', 'anterotas', 'an', 'Atrei', 'hebe', 'apate', 'Euenos', 'strato', 'med', 'per', 'Tantalo', 'Aretes', 'hostis', 'glossas', 'melo', 'proi', 'geometria', 'agnos', 'dogma', 'geometre', 'thalle', 'iota', 'lego', 'Gorgia', 'Protagoras', 'horas', 'heros', 'mi', 'ea', 'Hermes', 'alto', 'iste', 'lexeos', 'mela', 'domos', 'elate', 'ne', 'stes', 'organo', 'ede', 'mede', 'eri', 'suos', 'astra', 'dolo', 'aude', 'ere', 'Gorgias', 'nomisma', 'stantes', 'has', 'deo', 'pio', 'Orestes', 'Ares', 'eos', 'thalamos', 'Aristodemo', 'erga', 'beta', 'Neritos', 'deis', 'erat', 'dies', 'Eroti', 'dido', 'phono', 'en', 'lita', 'tris', 'Laertes', 'mala', 'sito', 'ose', 'e', 'Elpenor', 'Agamemnonides', 'auges', 'elephantos', 'phantasia', 'telamon', 'Hesiodo', 'assa', 'erato', 'daphnes', 'lethe', 'Parthenio', 'aera', 'phantasma', 'Delon', 'patria', 'ese', 'psoras', 'isto', 'zeta', 'par', 'ponto', 'ales', 'orto', 'daemones', 'oro', 'ted', 'domat', 'psephismata', 'Hermogene', 'Plato', 'duo', 'lebes', 'problema', 'lupe', 'Dardanide', 'diastemata', 'Atreus', 'duas', 'lu', 'os', 'ites', 'adamas', 'etis', 'aedes', 'metri', 'labes', 'eris', 'ano', 'teste', 'metro', 'io', 'hippo', 'Xanthippe', 'Miletos', 'mone', 'Stesichorus', 'Selene', 'mete', 'eges', 'bie', 'Europe', 'Hos', 'Naxo', 'ambrosie', 'bolos', 'Leto', 'pale', 'ore', 'Troia', 'meri', 'Melite', 'sape', 'problemata', 'Lesbo', 'anathemata', 'Delos', 'exerat', 'Eoi', 'pharetras', 'Phalereus', 'temo', 'liga', 'tropo', 'polle', 'Argos', 'trite', 'Diones', 'aeto', 'ambrosio', 'Menelao', 'Same', 'porro', 'eis', 'aniso', 'pie', 'opi', 'philosophos', 'nome', 'parentes', 'di', 'heroos', 'molis'}\n"
     ]
    }
   ],
   "source": [
    "shared_words = distinct_demacronized_greek & set(distinct_good_latin)\n",
    "print(f'Shared_words: {len(shared_words)} : {shared_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These shared words appear in both language corpora; however, intuitively, we know each word will have a different probability of occurrence in each language. So, rather than arbitrarily excluding some or all of the words from one language or the other, we should split them into the most common probable groups. We can do this by loading the probability distribution pickle objects we have created in the notebooks:\n",
    "* `building_language_model/make_frequency_distribution.ipynb` \n",
    "* `detecting_loanwords/make_frequency_distribution_greek_transliterated.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Frequency Distributions for Latin and transliterated Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_transliterated_word_probs = {}\n",
    "with open('freq_dist.greek.transliterated.pkl', 'rb') as reader:\n",
    "    greek_transliterated_word_probs = pickle.load(reader)\n",
    "    \n",
    "latin_word_probs = {}\n",
    "with open(os.path.join('../building_language_model', 'freq_dist.latin.pkl'), 'rb') as reader:\n",
    "    latin_word_probs = pickle.load(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll create a list of tuples containing (the word, the words probability in Latin, the words probability in Greek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('et', 0.9999900000000002, 0.001594731640385874)\n",
      "('in', 0.6195366094371269, 0.002322993118442995)\n",
      "('est', 0.40047688283934835, 0.0015525135836869104)\n",
      "('non', 0.3655202190736462, 1.0000000000000006e-06)\n",
      "('de', 0.16358695886651264, 0.5190825616279315)\n",
      "('a', 0.13448064535122392, 0.002914045912228485)\n",
      "('ex', 0.13319985495959372, 0.02580678715724147)\n",
      "('per', 0.11551978686121114, 0.0049721761763029575)\n",
      "('esse', 0.11489112051513624, 1e-06)\n",
      "('se', 0.10337573584020457, 0.019231324826377898)\n",
      "('aut', 0.09289014417246275, 0.0011619965592214974)\n",
      "('me', 0.06858796751076134, 0.016392160513372597)\n",
      "('te', 0.068280671647568, 0.1623927550925633)\n",
      "('id', 0.059277606586987885, 0.00039151702446541285)\n",
      "('ne', 0.05901722612275537, 2.2109028349481774e-05)\n",
      "('eo', 0.05369232834070303, 1.1554514174740887e-05)\n",
      "('pro', 0.051006421750196464, 0.00823352105629789)\n",
      "('ea', 0.04345773405740157, 0.0006131618221349714)\n",
      "('erat', 0.03941362666697944, 1e-06)\n",
      "('ei', 0.034644676362612745, 0.07388259922318621)\n",
      "('ego', 0.0335984629657866, 1e-06)\n",
      "('eos', 0.03038241236702284, 1e-06)\n",
      "('ante', 0.030340188507958103, 0.00012765417009689064)\n",
      "('ergo', 0.029326815890404534, 1e-06)\n",
      "('an', 0.027738729635580994, 0.10501841603867183)\n",
      "('ipsa', 0.021074397213197437, 1.1554514174740887e-05)\n",
      "('die', 0.019035923128350087, 8.54361133979271e-05)\n",
      "('eis', 0.018604301457910602, 0.08208345673695988)\n",
      "('dies', 0.015604061694366579, 1e-06)\n",
      "('is', 0.015097375385589792, 0.0012464326726194246)\n",
      "('e', 0.014745509893383692, 0.0011619965592214974)\n",
      "('pater', 0.01401128389964696, 0.0017002767821332827)\n",
      "('homo', 0.012092444082149687, 2.2109028349481774e-05)\n",
      "('suos', 0.010846840239740087, 9.5990627572668e-05)\n",
      "('es', 0.010710785582753728, 0.1048812073544002)\n",
      "('at', 0.010438676268781009, 0.0005287257087370443)\n",
      "('duo', 0.009596544857434407, 0.010365532919595551)\n",
      "('genus', 0.009535554838785348, 1.1554514174740887e-05)\n",
      "('patris', 0.00863712494868577, 0.00028597188271800393)\n",
      "('seu', 0.008449463352842516, 0.00016987222679585418)\n",
      "('lege', 0.00787240394562451, 0.0016580587254343192)\n",
      "('tot', 0.00781141392697545, 0.002069684778249214)\n",
      "('dei', 0.007619060791236115, 0.012793071179785955)\n",
      "('eas', 0.007398558416120292, 0.00010654514174740887)\n",
      "('o', 0.00659395932394234, 0.0006131618221349714)\n",
      "('suas', 0.006291355000645093, 5.3772570873704435e-05)\n",
      "('deos', 0.006263205761268605, 0.001109223988347793)\n",
      "('ore', 0.005735407522959452, 1e-06)\n",
      "('mala', 0.005711949823479046, 0.00654479878833935)\n",
      "('deo', 0.005665034424518232, 1e-06)\n",
      "('iste', 0.005374158950961189, 0.001014233360775125)\n",
      "('meis', 0.0050715546276639415, 2.2109028349481774e-05)\n",
      "('hos', 0.004811174163431426, 0.022239361366179048)\n",
      "('domo', 0.004691539896081352, 1e-06)\n",
      "('leges', 0.0043537490235634945, 1e-06)\n",
      "('mi', 0.004102751639123142, 1.1554514174740887e-05)\n",
      "('patri', 0.003940893512708336, 0.0019958031790260275)\n",
      "('duas', 0.00381656770546218, 6.432708504844532e-05)\n",
      "('dolo', 0.0035163091521129737, 1e-06)\n",
      "('has', 0.0033943291148148583, 0.0036211983619361243)\n",
      "('hostis', 0.003326301786321679, 0.004433895953391172)\n",
      "('isto', 0.00287825972624591, 1e-06)\n",
      "('porro', 0.0028243070174409743, 1e-06)\n",
      "('par', 0.002765662768739957, 0.016128297659004077)\n",
      "('tria', 0.002758625458895835, 0.0014680774702889832)\n",
      "('patria', 0.0027492423791036728, 0.0003176354252422266)\n",
      "('sin', 0.002629608111753598, 3.266354252422266e-05)\n",
      "('mori', 0.0026084961822212317, 1.0000000000000006e-06)\n",
      "('aetas', 0.0025662723231565, 1e-06)\n",
      "('erga', 0.002556889243364337, 0.00464498623688599)\n",
      "('os', 0.002521702694143727, 0.00010654514174740887)\n",
      "('hora', 0.0024912076848191984, 0.001584177126211133)\n",
      "('di', 0.0024137972765338562, 0.014207376079201233)\n",
      "('nota', 0.0024067599666897335, 1e-06)\n",
      "('parentes', 0.0021862575915739106, 2.2109028349481774e-05)\n",
      "('esto', 0.001998595995730656, 1e-06)\n",
      "('hoste', 0.0018461209491080122, 5.3772570873704435e-05)\n",
      "('domos', 0.0018132801698354428, 0.00012765417009689064)\n",
      "('penes', 0.0017053747522255717, 1e-06)\n",
      "('eris', 0.0016021608745117818, 0.00018042674097059507)\n",
      "('dein', 0.001588086254823538, 0.0036211983619361243)\n",
      "('alto', 0.001550553935654887, 5.3772570873704435e-05)\n",
      "('dis', 0.001522404696278399, 0.001404750385240538)\n",
      "('aedes', 0.0014801808372136667, 1e-06)\n",
      "('en', 0.0014403027480969753, 0.1989535921938657)\n",
      "('ero', 0.0014004246589802837, 1e-06)\n",
      "('Eo', 0.0013417804102792668, 1e-06)\n",
      "('Asia', 0.0013159769408508193, 0.0007925885631055665)\n",
      "('age', 0.0012479496123576398, 0.0016052861545606148)\n",
      "('pares', 0.0012479496123576398, 1.1554514174740887e-05)\n",
      "('ant', 0.0011728849740203382, 0.001014233360775125)\n",
      "('aer', 0.0011588103543320942, 1e-06)\n",
      "('det', 0.0011588103543320942, 1e-06)\n",
      "('aera', 0.001130661114955606, 0.0003070809110674857)\n",
      "('astra', 0.0011025118755791177, 0.00016987222679585418)\n",
      "('dote', 0.001013372617553572, 0.00021209028349481775)\n",
      "('spei', 0.0009570741388005958, 1e-06)\n",
      "('Plato', 0.000912504509787823, 2.2109028349481774e-05)\n",
      "('oro', 0.0008890468103074162, 1e-06)\n",
      "('philosophia', 0.0008843552704113349, 0.0003492989677664493)\n",
      "('potero', 0.0008820095004632942, 1e-06)\n",
      "('Hos', 0.0008608975709309282, 9.5990627572668e-05)\n",
      "('historia', 0.0008421314113466028, 0.0001487631984463724)\n",
      "('daemones', 0.0008280567916583586, 1e-06)\n",
      "('Apollo', 0.0008045990921779519, 1e-06)\n",
      "('dos', 0.0007717583129053824, 0.0005181711945623034)\n",
      "('eras', 0.0007576836932171383, 0.00011709965592214976)\n",
      "('ago', 0.000745954843476935, 1.0000000000000006e-06)\n",
      "('muri', 0.0007107682942563249, 6.432708504844532e-05)\n",
      "('teste', 0.0006943479046200401, 1e-06)\n",
      "('idea', 0.0006615071253474707, 0.0002437538260190404)\n",
      "('deest', 0.0006521240455553079, 1e-06)\n",
      "('horas', 0.0006450867357111859, 0.0016475042112595783)\n",
      "('do', 0.0006192832662827385, 0.0001487631984463724)\n",
      "('deis', 0.0005958255668023318, 1.1554514174740887e-05)\n",
      "('r', 0.0005747136372699656, 0.0001593177126211133)\n",
      "('poma', 0.0005606390175817216, 1.1554514174740887e-05)\n",
      "('pio', 0.0005536017077375996, 1e-06)\n",
      "('lis', 0.000551255937789559, 2.2109028349481774e-05)\n",
      "('med', 0.0005418728579973961, 1e-06)\n",
      "('omen', 0.0005348355481532742, 1e-06)\n",
      "('limen', 0.0005277982383091521, 2.2109028349481774e-05)\n",
      "('lego', 0.0005207609284650301, 1e-06)\n",
      "('mero', 0.0005184151585169894, 1e-06)\n",
      "('Athenas', 0.0005184151585169894, 1e-06)\n",
      "('melle', 0.0005160693885689487, 1.0000000000000006e-06)\n",
      "('prophetas', 0.0005019947688807047, 1e-06)\n",
      "('tris', 0.0004902659191405013, 0.0007503705064066029)\n",
      "('ara', 0.0004879201491924607, 0.017859237983661583)\n",
      "('ponto', 0.0004644624497120539, 1e-06)\n",
      "('Troia', 0.0004550793699198912, 0.00020153576932007686)\n",
      "('probata', 0.0004480420600757691, 0.000655379878833935)\n",
      "('luto', 0.00043631321033556577, 9.5990627572668e-05)\n",
      "('heros', 0.0004292759004914438, 1e-06)\n",
      "('mel', 0.0004245843605953624, 1e-06)\n",
      "('philosophos', 0.0004245843605953624, 0.0002648628543685222)\n",
      "('prophetis', 0.0004222385906473217, 1e-06)\n",
      "('theatro', 0.0004198928206992811, 1e-06)\n",
      "('times', 0.000412855510855159, 1.0000000000000006e-06)\n",
      "('ales', 0.000412855510855159, 1.0000000000000006e-06)\n",
      "('plana', 0.0004081639709590777, 0.00010654514174740887)\n",
      "('Eos', 0.0004034724310629963, 1e-06)\n",
      "('aure', 0.00040112666111495566, 1e-06)\n",
      "('pie', 0.0003847062714786709, 1.1554514174740887e-05)\n",
      "('opi', 0.0003847062714786709, 0.00012765417009689064)\n",
      "('antro', 0.00038001473158258956, 1e-06)\n",
      "('dolos', 0.00037297742173846753, 7.488159922318621e-05)\n",
      "('Thebas', 0.0003448281823619794, 1e-06)\n",
      "('polo', 0.000340136642465898, 1e-06)\n",
      "('thalamos', 0.00033075356267373533, 6.432708504844532e-05)\n",
      "('Argos', 0.000326062022777654, 0.0010775604458235703)\n",
      "('limo', 0.0003143331730374506, 1e-06)\n",
      "('stantes', 0.0003072958631933286, 1.1554514174740887e-05)\n",
      "('aget', 0.00029087547355704385, 9.5990627572668e-05)\n",
      "('philosopho', 0.0002791466238168405, 1e-06)\n",
      "('tauros', 0.0002744550839207592, 0.00020153576932007686)\n",
      "('elephanti', 0.0002744550839207592, 0.00010654514174740887)\n",
      "('orto', 0.00027210931397271846, 1e-06)\n",
      "('elephantos', 0.00026976354402467775, 0.0005287257087370443)\n",
      "('paries', 0.0002580346942844744, 1e-06)\n",
      "('toros', 0.00024630584454427105, 1.1554514174740887e-05)\n",
      "('labe', 0.0002439600745962303, 0.0003492989677664493)\n",
      "('agnos', 0.0002439600745962303, 1.0000000000000006e-06)\n",
      "('Homero', 0.00023926853470014893, 1e-06)\n",
      "('ae', 0.00023223122485602693, 3.266354252422266e-05)\n",
      "('metalla', 0.0002251939150119049, 0.00010654514174740887)\n",
      "('thalamo', 0.00022050237511582355, 1e-06)\n",
      "('io', 0.00021815660516778286, 1.0000000000000006e-06)\n",
      "('molis', 0.00021346506527170154, 0.0017108312963080236)\n",
      "('Theseus', 0.0002087735253756202, 1e-06)\n",
      "('pote', 0.00020408198547953882, 0.012581980896291136)\n",
      "('Aristotele', 0.00019939044558345747, 1e-06)\n",
      "('nautas', 0.00018297005594717272, 0.0001487631984463724)\n",
      "('anathema', 0.00017827851605109138, 3.266354252422266e-05)\n",
      "('Minos', 0.00017358697615501003, 1e-06)\n",
      "('dogma', 0.00017358697615501003, 0.0003176354252422266)\n",
      "('Orpheus', 0.00017124120620696934, 0.0001593177126211133)\n",
      "('dogmata', 0.00017124120620696934, 4.321805669896355e-05)\n",
      "('sus', 0.00016889543625892866, 0.00012765417009689064)\n",
      "('throno', 0.00016889543625892866, 1e-06)\n",
      "('notio', 0.00016889543625892866, 1e-06)\n",
      "('Pergama', 0.00016889543625892866, 1.0000000000000006e-06)\n",
      "('sues', 0.00016889543625892866, 0.00010654514174740887)\n",
      "('labes', 0.00016889543625892866, 1e-06)\n",
      "('axioma', 0.0001642038963628473, 1e-06)\n",
      "('ere', 0.0001642038963628473, 1.0000000000000006e-06)\n",
      "('petas', 0.00016185812641480665, 1.0000000000000006e-06)\n",
      "('Orestes', 0.00016185812641480665, 1e-06)\n",
      "('phantasma', 0.00016185812641480665, 8.54361133979271e-05)\n",
      "('eant', 0.0001571665865187253, 1.1554514174740887e-05)\n",
      "('no', 0.0001571665865187253, 0.0003598534819411902)\n",
      "('noto', 0.00015482081657068462, 1e-06)\n",
      "('pauet', 0.00015012927667460324, 1.0000000000000006e-06)\n",
      "('hebes', 0.00015012927667460324, 1e-06)\n",
      "('emat', 0.0001454377367785219, 1e-06)\n",
      "('Orion', 0.0001384004269343999, 1e-06)\n",
      "('rhetor', 0.00013605465698635923, 1e-06)\n",
      "('leget', 0.00013370888703831852, 1.0000000000000006e-06)\n",
      "('metro', 0.00013370888703831852, 1e-06)\n",
      "('pono', 0.00012901734714223717, 1.0000000000000006e-06)\n",
      "('olet', 0.0001266715771941965, 1.0000000000000006e-06)\n",
      "('lino', 0.00012432580724615583, 1e-06)\n",
      "('mones', 0.00012198003729811518, 1e-06)\n",
      "('domat', 0.00012198003729811518, 1e-06)\n",
      "('Delo', 0.00011494272745399313, 1e-06)\n",
      "('Gai', 0.00011259695750595246, 3.266354252422266e-05)\n",
      "('geometria', 0.00011259695750595246, 1e-06)\n",
      "('pharetras', 0.00011259695750595246, 2.2109028349481774e-05)\n",
      "('Perses', 0.00011025118755791177, 1e-06)\n",
      "('strato', 0.0001079054176098711, 1e-06)\n",
      "('ted', 0.0001079054176098711, 1e-06)\n",
      "('Rhodope', 0.00010555964766183044, 1e-06)\n",
      "('ede', 0.00010555964766183044, 1e-06)\n",
      "('aristas', 9.852233781770841e-05, 6.432708504844532e-05)\n",
      "('Dione', 9.852233781770841e-05, 1e-06)\n",
      "('eri', 9.383079792162705e-05, 1e-06)\n",
      "('Euripides', 9.148502797358637e-05, 1.1554514174740887e-05)\n",
      "('mela', 9.148502797358637e-05, 1e-06)\n",
      "('meri', 9.148502797358637e-05, 1.0000000000000006e-06)\n",
      "('polos', 8.91392580255457e-05, 1e-06)\n",
      "('Anaxagoras', 8.91392580255457e-05, 5.3772570873704435e-05)\n",
      "('bibas', 8.679348807750503e-05, 3.266354252422266e-05)\n",
      "('proi', 8.679348807750503e-05, 1e-06)\n",
      "('rhetores', 8.444771812946434e-05, 1e-06)\n",
      "('phantasmata', 8.444771812946434e-05, 4.321805669896355e-05)\n",
      "('Maia', 8.444771812946434e-05, 7.488159922318621e-05)\n",
      "('statera', 8.210194818142367e-05, 1e-06)\n",
      "('messe', 7.9756178233383e-05, 1e-06)\n",
      "('Hermes', 7.9756178233383e-05, 1.1554514174740887e-05)\n",
      "('phantasia', 7.9756178233383e-05, 2.2109028349481774e-05)\n",
      "('Atreus', 7.9756178233383e-05, 6.432708504844532e-05)\n",
      "('metis', 7.741040828534232e-05, 1.0000000000000006e-06)\n",
      "('Nestor', 7.506463833730164e-05, 3.266354252422266e-05)\n",
      "('Helene', 7.506463833730164e-05, 1e-06)\n",
      "('Gorgias', 7.506463833730164e-05, 7.488159922318621e-05)\n",
      "('organo', 7.271886838926096e-05, 1e-06)\n",
      "('Troes', 7.037309844122029e-05, 1e-06)\n",
      "('Delos', 6.802732849317962e-05, 1e-06)\n",
      "('sto', 6.568155854513893e-05, 1e-06)\n",
      "('Theodoro', 6.333578859709826e-05, 1e-06)\n",
      "('aude', 6.333578859709826e-05, 1e-06)\n",
      "('Europe', 6.333578859709826e-05, 1e-06)\n",
      "('tende', 6.099001864905758e-05, 1e-06)\n",
      "('hei', 6.099001864905758e-05, 1.0000000000000006e-06)\n",
      "('emo', 5.86442487010169e-05, 1e-06)\n",
      "('Troas', 5.629847875297623e-05, 1e-06)\n",
      "('diametro', 5.629847875297623e-05, 1e-06)\n",
      "('Lemnos', 5.629847875297623e-05, 1e-06)\n",
      "('Menelao', 5.629847875297623e-05, 1e-06)\n",
      "('pe', 5.395270880493555e-05, 1e-06)\n",
      "('osse', 5.395270880493555e-05, 0.0001382086842716315)\n",
      "('lotos', 5.395270880493555e-05, 1e-06)\n",
      "('alueis', 5.395270880493555e-05, 1.0000000000000006e-06)\n",
      "('Zenoni', 5.1606938856894884e-05, 1e-06)\n",
      "('Eoi', 4.9261168908854204e-05, 1e-06)\n",
      "('Phalereus', 4.9261168908854204e-05, 1e-06)\n",
      "('Pausania', 4.691539896081353e-05, 4.321805669896355e-05)\n",
      "('torno', 4.691539896081353e-05, 1e-06)\n",
      "('rhetori', 4.691539896081353e-05, 1e-06)\n",
      "('Hesiodo', 4.691539896081353e-05, 1e-06)\n",
      "('eges', 4.691539896081353e-05, 1e-06)\n",
      "('Lesbo', 4.691539896081353e-05, 1e-06)\n",
      "('Sparte', 4.456962901277285e-05, 1e-06)\n",
      "('pompas', 4.456962901277285e-05, 0.0001593177126211133)\n",
      "('Apolloni', 4.456962901277285e-05, 1e-06)\n",
      "('Mida', 4.456962901277285e-05, 2.2109028349481774e-05)\n",
      "('heroas', 4.456962901277285e-05, 1e-06)\n",
      "('Eros', 4.222385906473218e-05, 1.0000000000000006e-06)\n",
      "('Naxos', 3.98780891166915e-05, 2.2109028349481774e-05)\n",
      "('sei', 3.98780891166915e-05, 1.0000000000000006e-06)\n",
      "('athletas', 3.98780891166915e-05, 1e-06)\n",
      "('siderea', 3.98780891166915e-05, 1e-06)\n",
      "('metri', 3.98780891166915e-05, 1.0000000000000006e-06)\n",
      "('Thebe', 3.7532319168650824e-05, 1e-06)\n",
      "('heroes', 3.7532319168650824e-05, 1e-06)\n",
      "('logo', 3.7532319168650824e-05, 1e-06)\n",
      "('hamos', 3.7532319168650824e-05, 1e-06)\n",
      "('Gorgia', 3.7532319168650824e-05, 3.266354252422266e-05)\n",
      "('lita', 3.7532319168650824e-05, 1.0000000000000006e-06)\n",
      "('antas', 3.5186549220610144e-05, 1.0000000000000006e-06)\n",
      "('ito', 3.5186549220610144e-05, 1e-06)\n",
      "('dele', 3.5186549220610144e-05, 1e-06)\n",
      "('Eous', 3.5186549220610144e-05, 1e-06)\n",
      "('eita', 3.5186549220610144e-05, 0.0027557281996073717)\n",
      "('Paros', 3.5186549220610144e-05, 1.1554514174740887e-05)\n",
      "('Protagoras', 3.5186549220610144e-05, 1e-06)\n",
      "('mone', 3.5186549220610144e-05, 1e-06)\n",
      "('Persephone', 3.284077927256947e-05, 1e-06)\n",
      "('pare', 3.284077927256947e-05, 1.0000000000000006e-06)\n",
      "('polite', 3.284077927256947e-05, 1e-06)\n",
      "('sidereos', 3.284077927256947e-05, 1e-06)\n",
      "('lu', 3.284077927256947e-05, 1.1554514174740887e-05)\n",
      "('manenti', 3.0495009324528788e-05, 1.1554514174740887e-05)\n",
      "('metere', 3.0495009324528788e-05, 1e-06)\n",
      "('aspis', 3.0495009324528788e-05, 0.00018042674097059507)\n",
      "('doma', 3.0495009324528788e-05, 1.0000000000000006e-06)\n",
      "('Atrei', 3.0495009324528788e-05, 1.1554514174740887e-05)\n",
      "('Parthenio', 3.0495009324528788e-05, 1e-06)\n",
      "('adamas', 3.0495009324528788e-05, 1.0000000000000006e-06)\n",
      "('tropo', 3.0495009324528788e-05, 1e-06)\n",
      "('Leontinos', 2.814923937648811e-05, 2.2109028349481774e-05)\n",
      "('Lapithas', 2.814923937648811e-05, 1.0000000000000006e-06)\n",
      "('opta', 2.814923937648811e-05, 2.2109028349481774e-05)\n",
      "('diae', 2.814923937648811e-05, 1e-06)\n",
      "('asto', 2.814923937648811e-05, 1e-06)\n",
      "('Minoa', 2.814923937648811e-05, 1e-06)\n",
      "('Cypria', 2.5803469428447435e-05, 1.0000000000000006e-06)\n",
      "('mere', 2.5803469428447435e-05, 1.1554514174740887e-05)\n",
      "('auge', 2.5803469428447435e-05, 1e-06)\n",
      "('stes', 2.5803469428447435e-05, 1e-06)\n",
      "('problema', 2.5803469428447435e-05, 1e-06)\n",
      "('ano', 2.5803469428447435e-05, 1e-06)\n",
      "('Diones', 2.5803469428447435e-05, 1e-06)\n",
      "('Thebes', 2.3457699480406762e-05, 1e-06)\n",
      "('Herme', 2.3457699480406762e-05, 1e-06)\n",
      "('pedali', 2.3457699480406762e-05, 1e-06)\n",
      "('nomisma', 2.3457699480406762e-05, 0.0002437538260190404)\n",
      "('beta', 2.3457699480406762e-05, 1e-06)\n",
      "('Naxo', 2.3457699480406762e-05, 1e-06)\n",
      "('temo', 2.3457699480406762e-05, 1e-06)\n",
      "('liga', 2.3457699480406762e-05, 2.2109028349481774e-05)\n",
      "('Arete', 2.1111929532366085e-05, 1e-06)\n",
      "('nae', 2.1111929532366085e-05, 1e-06)\n",
      "('topo', 2.1111929532366085e-05, 1e-06)\n",
      "('Tantalo', 2.1111929532366085e-05, 1e-06)\n",
      "('iota', 2.1111929532366085e-05, 1e-06)\n",
      "('elate', 2.1111929532366085e-05, 1e-06)\n",
      "('problemata', 2.1111929532366085e-05, 1e-06)\n",
      "('Euripo', 1.876615958432541e-05, 1e-06)\n",
      "('heroi', 1.876615958432541e-05, 1e-06)\n",
      "('gemet', 1.876615958432541e-05, 1e-06)\n",
      "('Mimas', 1.876615958432541e-05, 1.0000000000000006e-06)\n",
      "('auges', 1.876615958432541e-05, 1e-06)\n",
      "('Amphitrite', 1.6420389636284732e-05, 1e-06)\n",
      "('dogmati', 1.6420389636284732e-05, 1.1554514174740887e-05)\n",
      "('agones', 1.6420389636284732e-05, 1e-06)\n",
      "('Euripide', 1.6420389636284732e-05, 1e-06)\n",
      "('heroa', 1.6420389636284732e-05, 1e-06)\n",
      "('periodo', 1.6420389636284732e-05, 1e-06)\n",
      "('demo', 1.6420389636284732e-05, 1e-06)\n",
      "('emes', 1.6420389636284732e-05, 1e-06)\n",
      "('geme', 1.6420389636284732e-05, 1e-06)\n",
      "('Epigenes', 1.6420389636284732e-05, 1.0000000000000006e-06)\n",
      "('mede', 1.6420389636284732e-05, 1e-06)\n",
      "('sito', 1.6420389636284732e-05, 1e-06)\n",
      "('assa', 1.6420389636284732e-05, 1.0000000000000006e-06)\n",
      "('Hermogene', 1.6420389636284732e-05, 1e-06)\n",
      "('Miletos', 1.6420389636284732e-05, 1e-06)\n",
      "('Melite', 1.6420389636284732e-05, 1e-06)\n",
      "('sape', 1.6420389636284732e-05, 1e-06)\n",
      "('Same', 1.6420389636284732e-05, 1e-06)\n",
      "('saturo', 1.4074619688244056e-05, 1e-06)\n",
      "('Aristophane', 1.4074619688244056e-05, 1e-06)\n",
      "('Protagora', 1.4074619688244056e-05, 1e-06)\n",
      "('Triptolemo', 1.4074619688244056e-05, 1e-06)\n",
      "('aloes', 1.4074619688244056e-05, 1e-06)\n",
      "('Delio', 1.4074619688244056e-05, 1e-06)\n",
      "('neo', 1.4074619688244056e-05, 1e-06)\n",
      "('ese', 1.4074619688244056e-05, 1e-06)\n",
      "('Xanthippe', 1.4074619688244056e-05, 1e-06)\n",
      "('Leto', 1.4074619688244056e-05, 1e-06)\n",
      "('Lede', 1.172884974020338e-05, 1e-06)\n",
      "('philo', 1.172884974020338e-05, 1e-06)\n",
      "('aloe', 1.172884974020338e-05, 1e-06)\n",
      "('protero', 1.172884974020338e-05, 1e-06)\n",
      "('polles', 1.172884974020338e-05, 1e-06)\n",
      "('morio', 1.172884974020338e-05, 1e-06)\n",
      "('Laertes', 1.172884974020338e-05, 1e-06)\n",
      "('Selene', 1.172884974020338e-05, 1e-06)\n",
      "('exerat', 1.172884974020338e-05, 1e-06)\n",
      "('heroos', 1.172884974020338e-05, 1e-06)\n",
      "('Hippotades', 9.383079792162704e-06, 1e-06)\n",
      "('exe', 9.383079792162704e-06, 1e-06)\n",
      "('ale', 9.383079792162704e-06, 1e-06)\n",
      "('aleis', 9.383079792162704e-06, 1.0000000000000006e-06)\n",
      "('Melantho', 9.383079792162704e-06, 1e-06)\n",
      "('oles', 9.383079792162704e-06, 1e-06)\n",
      "('glossas', 9.383079792162704e-06, 1e-06)\n",
      "('lexeos', 9.383079792162704e-06, 1e-06)\n",
      "('lupe', 9.383079792162704e-06, 1e-06)\n",
      "('mete', 9.383079792162704e-06, 1e-06)\n",
      "('ambrosio', 9.383079792162704e-06, 1e-06)\n",
      "('mese', 7.037309844122028e-06, 1e-06)\n",
      "('metreta', 7.037309844122028e-06, 1e-06)\n",
      "('lae', 7.037309844122028e-06, 1.0000000000000006e-06)\n",
      "('poesi', 7.037309844122028e-06, 1e-06)\n",
      "('pedo', 7.037309844122028e-06, 1e-06)\n",
      "('Ares', 7.037309844122028e-06, 1.0000000000000006e-06)\n",
      "('lebes', 7.037309844122028e-06, 1e-06)\n",
      "('anathemata', 7.037309844122028e-06, 1e-06)\n",
      "('trite', 7.037309844122028e-06, 1e-06)\n",
      "('Admete', 4.691539896081351e-06, 1e-06)\n",
      "('Erote', 4.691539896081351e-06, 1e-06)\n",
      "('erosa', 4.691539896081351e-06, 1e-06)\n",
      "('Theodore', 4.691539896081351e-06, 1e-06)\n",
      "('Pheres', 4.691539896081351e-06, 1e-06)\n",
      "('Polites', 4.691539896081351e-06, 1e-06)\n",
      "('aule', 4.691539896081351e-06, 1e-06)\n",
      "('Laertiade', 4.691539896081351e-06, 1e-06)\n",
      "('bio', 4.691539896081351e-06, 1e-06)\n",
      "('Melete', 4.691539896081351e-06, 1e-06)\n",
      "('agonia', 4.691539896081351e-06, 1e-06)\n",
      "('Aretes', 4.691539896081351e-06, 1e-06)\n",
      "('Eroti', 4.691539896081351e-06, 1e-06)\n",
      "('psephismata', 4.691539896081351e-06, 1e-06)\n",
      "('diastemata', 4.691539896081351e-06, 1e-06)\n",
      "('etis', 4.691539896081351e-06, 1.0000000000000006e-06)\n",
      "('bie', 4.691539896081351e-06, 1e-06)\n",
      "('Parmenide', 2.345769948040676e-06, 1e-06)\n",
      "('peribolo', 2.345769948040676e-06, 1e-06)\n",
      "('parergo', 2.345769948040676e-06, 1e-06)\n",
      "('pedalia', 2.345769948040676e-06, 1e-06)\n",
      "('pino', 2.345769948040676e-06, 1e-06)\n",
      "('pero', 2.345769948040676e-06, 1e-06)\n",
      "('stephano', 2.345769948040676e-06, 1e-06)\n",
      "('oleto', 2.345769948040676e-06, 1e-06)\n",
      "('metabole', 2.345769948040676e-06, 1e-06)\n",
      "('Diomedeos', 2.345769948040676e-06, 1e-06)\n",
      "('phthisis', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('pontones', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('Lampetie', 2.345769948040676e-06, 1e-06)\n",
      "('dromo', 2.345769948040676e-06, 1e-06)\n",
      "('nassa', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('Europes', 2.345769948040676e-06, 1e-06)\n",
      "('Are', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('horai', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('topazo', 2.345769948040676e-06, 1e-06)\n",
      "('Euenos', 2.345769948040676e-06, 1e-06)\n",
      "('melo', 2.345769948040676e-06, 1e-06)\n",
      "('geometre', 2.345769948040676e-06, 1e-06)\n",
      "('Aristodemo', 2.345769948040676e-06, 1e-06)\n",
      "('Neritos', 2.345769948040676e-06, 1e-06)\n",
      "('Elpenor', 2.345769948040676e-06, 1e-06)\n",
      "('Delon', 2.345769948040676e-06, 1e-06)\n",
      "('Dardanide', 2.345769948040676e-06, 1e-06)\n",
      "('hippo', 2.345769948040676e-06, 1e-06)\n",
      "('Stesichorus', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('bolos', 2.345769948040676e-06, 1e-06)\n",
      "('pale', 2.345769948040676e-06, 1e-06)\n",
      "('Letoi', 1e-06, 1e-06)\n",
      "('doto', 1e-06, 1e-06)\n",
      "('pedalio', 1e-06, 1e-06)\n",
      "('epistate', 1e-06, 1e-06)\n",
      "('aletes', 1e-06, 1e-06)\n",
      "('agre', 1e-06, 1e-06)\n",
      "('ereto', 1e-06, 1e-06)\n",
      "('iambe', 1e-06, 1e-06)\n",
      "('halo', 1e-06, 1e-06)\n",
      "('meni', 1e-06, 1e-06)\n",
      "('peren', 1e-06, 1e-06)\n",
      "('pege', 1e-06, 1e-06)\n",
      "('laro', 1e-06, 1e-06)\n",
      "('agoni', 1e-06, 1e-06)\n",
      "('osi', 1e-06, 1e-06)\n",
      "('daphne', 1e-06, 1e-06)\n",
      "('geode', 1e-06, 1e-06)\n",
      "('emata', 1e-06, 1e-06)\n",
      "('pales', 1e-06, 1e-06)\n",
      "('pleto', 1e-06, 1e-06)\n",
      "('melie', 1e-06, 1e-06)\n",
      "('potho', 1e-06, 1e-06)\n",
      "('admete', 1e-06, 1e-06)\n",
      "('elateri', 1e-06, 1e-06)\n",
      "('depote', 1e-06, 1e-06)\n",
      "('edos', 1e-06, 1e-06)\n",
      "('agrio', 1e-06, 1e-06)\n",
      "('orse', 1e-06, 1e-06)\n",
      "('enen', 1e-06, 1e-06)\n",
      "('xanthe', 1e-06, 1e-06)\n",
      "('anterotas', 1e-06, 1e-06)\n",
      "('hebe', 1e-06, 1e-06)\n",
      "('apate', 1e-06, 1e-06)\n",
      "('thalle', 1e-06, 1e-06)\n",
      "('dido', 1e-06, 1e-06)\n",
      "('phono', 1e-06, 1e-06)\n",
      "('ose', 1e-06, 1e-06)\n",
      "('Agamemnonides', 1e-06, 1e-06)\n",
      "('telamon', 1e-06, 1e-06)\n",
      "('erato', 1e-06, 1e-06)\n",
      "('daphnes', 1e-06, 1e-06)\n",
      "('lethe', 1e-06, 1e-06)\n",
      "('psoras', 1e-06, 1e-06)\n",
      "('zeta', 1e-06, 1e-06)\n",
      "('ites', 1e-06, 1e-06)\n",
      "('ambrosie', 1e-06, 1e-06)\n",
      "('polle', 1e-06, 1e-06)\n",
      "('aeto', 1e-06, 1e-06)\n",
      "('aniso', 1e-06, 1e-06)\n",
      "('nome', 1e-06, 1e-06)\n",
      "('hesperos', 0.0, 2.2109028349481774e-05)\n"
     ]
    }
   ],
   "source": [
    "shared_latin_greek = [(word, \n",
    "                       latin_word_probs.get(word, 0.000001),\n",
    "                       greek_transliterated_word_probs.get(word, 0.000001)) \n",
    "                      for word in shared_words]\n",
    "\n",
    "shared_latin_greek.sort(key=lambda a: a[1], reverse=True)\n",
    "for item in shared_latin_greek:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kai` is the most common word in the Greek corpus, so we could also divide the shared words by the threshold of this probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011142407253193212"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latin_word_probs.get('kai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['et',\n",
       " 'in',\n",
       " 'est',\n",
       " 'non',\n",
       " 'de',\n",
       " 'a',\n",
       " 'ex',\n",
       " 'per',\n",
       " 'esse',\n",
       " 'se',\n",
       " 'aut',\n",
       " 'me',\n",
       " 'te',\n",
       " 'id',\n",
       " 'ne',\n",
       " 'eo',\n",
       " 'pro',\n",
       " 'ea',\n",
       " 'erat',\n",
       " 'ei',\n",
       " 'ego',\n",
       " 'eos',\n",
       " 'ante',\n",
       " 'ergo',\n",
       " 'an',\n",
       " 'ipsa',\n",
       " 'die',\n",
       " 'eis',\n",
       " 'dies',\n",
       " 'is',\n",
       " 'e',\n",
       " 'pater',\n",
       " 'homo',\n",
       " 'suos',\n",
       " 'es',\n",
       " 'at',\n",
       " 'duo',\n",
       " 'genus',\n",
       " 'patris',\n",
       " 'seu',\n",
       " 'lege',\n",
       " 'tot',\n",
       " 'dei',\n",
       " 'eas',\n",
       " 'o',\n",
       " 'suas',\n",
       " 'deos',\n",
       " 'ore',\n",
       " 'mala',\n",
       " 'deo',\n",
       " 'iste',\n",
       " 'meis',\n",
       " 'hos',\n",
       " 'domo',\n",
       " 'leges',\n",
       " 'mi',\n",
       " 'patri',\n",
       " 'duas',\n",
       " 'dolo',\n",
       " 'has',\n",
       " 'hostis',\n",
       " 'isto',\n",
       " 'porro',\n",
       " 'par',\n",
       " 'tria',\n",
       " 'patria',\n",
       " 'sin',\n",
       " 'mori',\n",
       " 'aetas',\n",
       " 'erga',\n",
       " 'os',\n",
       " 'hora',\n",
       " 'di',\n",
       " 'nota',\n",
       " 'parentes',\n",
       " 'esto',\n",
       " 'hoste',\n",
       " 'domos',\n",
       " 'penes',\n",
       " 'eris',\n",
       " 'dein',\n",
       " 'alto',\n",
       " 'dis',\n",
       " 'aedes',\n",
       " 'en',\n",
       " 'ero',\n",
       " 'Eo',\n",
       " 'Asia',\n",
       " 'age',\n",
       " 'pares',\n",
       " 'ant',\n",
       " 'aer',\n",
       " 'det',\n",
       " 'aera']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likely_latin = [word for word, latin_prob, greek_prob \n",
    "                in shared_latin_greek \n",
    "                if latin_prob >= latin_word_probs['kai']]\n",
    "print(len(likely_latin))\n",
    "likely_latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['et',\n",
       " 'in',\n",
       " 'est',\n",
       " 'non',\n",
       " 'a',\n",
       " 'ex',\n",
       " 'per',\n",
       " 'esse',\n",
       " 'se',\n",
       " 'aut',\n",
       " 'me',\n",
       " 'id',\n",
       " 'ne',\n",
       " 'eo',\n",
       " 'pro',\n",
       " 'ea',\n",
       " 'erat',\n",
       " 'ego',\n",
       " 'eos',\n",
       " 'ante',\n",
       " 'ergo',\n",
       " 'ipsa',\n",
       " 'die',\n",
       " 'dies',\n",
       " 'is',\n",
       " 'e',\n",
       " 'pater',\n",
       " 'homo',\n",
       " 'suos',\n",
       " 'at',\n",
       " 'genus',\n",
       " 'patris',\n",
       " 'seu',\n",
       " 'lege',\n",
       " 'tot',\n",
       " 'eas',\n",
       " 'o',\n",
       " 'suas',\n",
       " 'deos',\n",
       " 'ore',\n",
       " 'deo',\n",
       " 'iste',\n",
       " 'meis',\n",
       " 'domo',\n",
       " 'leges',\n",
       " 'mi',\n",
       " 'patri',\n",
       " 'duas',\n",
       " 'dolo',\n",
       " 'isto',\n",
       " 'porro',\n",
       " 'tria',\n",
       " 'patria',\n",
       " 'sin',\n",
       " 'mori',\n",
       " 'aetas',\n",
       " 'os',\n",
       " 'hora',\n",
       " 'nota',\n",
       " 'parentes',\n",
       " 'esto',\n",
       " 'hoste',\n",
       " 'domos',\n",
       " 'penes',\n",
       " 'eris',\n",
       " 'alto',\n",
       " 'dis',\n",
       " 'aedes',\n",
       " 'ero',\n",
       " 'Eo',\n",
       " 'Asia',\n",
       " 'pares',\n",
       " 'ant',\n",
       " 'aer',\n",
       " 'det',\n",
       " 'aera',\n",
       " 'astra',\n",
       " 'dote',\n",
       " 'spei',\n",
       " 'Plato',\n",
       " 'oro',\n",
       " 'philosophia',\n",
       " 'potero',\n",
       " 'Hos',\n",
       " 'historia',\n",
       " 'daemones',\n",
       " 'Apollo',\n",
       " 'dos',\n",
       " 'eras',\n",
       " 'ago',\n",
       " 'muri',\n",
       " 'teste',\n",
       " 'idea',\n",
       " 'deest',\n",
       " 'do',\n",
       " 'deis',\n",
       " 'r',\n",
       " 'poma',\n",
       " 'pio',\n",
       " 'lis',\n",
       " 'med',\n",
       " 'omen',\n",
       " 'limen',\n",
       " 'lego',\n",
       " 'mero',\n",
       " 'Athenas',\n",
       " 'melle',\n",
       " 'prophetas',\n",
       " 'ponto',\n",
       " 'Troia',\n",
       " 'luto',\n",
       " 'heros',\n",
       " 'mel',\n",
       " 'philosophos',\n",
       " 'prophetis',\n",
       " 'theatro',\n",
       " 'times',\n",
       " 'ales',\n",
       " 'plana',\n",
       " 'Eos',\n",
       " 'aure',\n",
       " 'pie',\n",
       " 'opi',\n",
       " 'antro',\n",
       " 'dolos',\n",
       " 'Thebas',\n",
       " 'polo',\n",
       " 'thalamos',\n",
       " 'limo',\n",
       " 'stantes',\n",
       " 'aget',\n",
       " 'philosopho',\n",
       " 'tauros',\n",
       " 'elephanti',\n",
       " 'orto',\n",
       " 'paries',\n",
       " 'toros',\n",
       " 'agnos',\n",
       " 'Homero',\n",
       " 'ae',\n",
       " 'metalla',\n",
       " 'thalamo',\n",
       " 'io',\n",
       " 'Theseus',\n",
       " 'Aristotele',\n",
       " 'nautas',\n",
       " 'anathema',\n",
       " 'Minos',\n",
       " 'Orpheus',\n",
       " 'dogmata',\n",
       " 'sus',\n",
       " 'throno',\n",
       " 'notio',\n",
       " 'Pergama',\n",
       " 'sues',\n",
       " 'labes',\n",
       " 'axioma',\n",
       " 'ere',\n",
       " 'petas',\n",
       " 'Orestes',\n",
       " 'phantasma',\n",
       " 'eant',\n",
       " 'noto',\n",
       " 'pauet',\n",
       " 'hebes',\n",
       " 'emat',\n",
       " 'Orion',\n",
       " 'rhetor',\n",
       " 'leget',\n",
       " 'metro',\n",
       " 'pono',\n",
       " 'olet',\n",
       " 'lino',\n",
       " 'mones',\n",
       " 'domat',\n",
       " 'Delo',\n",
       " 'Gai',\n",
       " 'geometria',\n",
       " 'pharetras',\n",
       " 'Perses',\n",
       " 'strato',\n",
       " 'ted',\n",
       " 'Rhodope',\n",
       " 'ede',\n",
       " 'aristas',\n",
       " 'Dione',\n",
       " 'eri',\n",
       " 'Euripides',\n",
       " 'mela',\n",
       " 'meri',\n",
       " 'polos',\n",
       " 'Anaxagoras',\n",
       " 'bibas',\n",
       " 'proi',\n",
       " 'rhetores',\n",
       " 'phantasmata',\n",
       " 'Maia',\n",
       " 'statera',\n",
       " 'messe',\n",
       " 'Hermes',\n",
       " 'phantasia',\n",
       " 'Atreus',\n",
       " 'metis',\n",
       " 'Nestor',\n",
       " 'Helene',\n",
       " 'Gorgias',\n",
       " 'organo',\n",
       " 'Troes',\n",
       " 'Delos',\n",
       " 'sto',\n",
       " 'Theodoro',\n",
       " 'aude',\n",
       " 'Europe',\n",
       " 'tende',\n",
       " 'hei',\n",
       " 'emo',\n",
       " 'Troas',\n",
       " 'diametro',\n",
       " 'Lemnos',\n",
       " 'Menelao',\n",
       " 'pe',\n",
       " 'lotos',\n",
       " 'alueis',\n",
       " 'Zenoni',\n",
       " 'Eoi',\n",
       " 'Phalereus',\n",
       " 'Pausania',\n",
       " 'torno',\n",
       " 'rhetori',\n",
       " 'Hesiodo',\n",
       " 'eges',\n",
       " 'Lesbo',\n",
       " 'Sparte',\n",
       " 'Apolloni',\n",
       " 'Mida',\n",
       " 'heroas',\n",
       " 'Eros',\n",
       " 'Naxos',\n",
       " 'sei',\n",
       " 'athletas',\n",
       " 'siderea',\n",
       " 'metri',\n",
       " 'Thebe',\n",
       " 'heroes',\n",
       " 'logo',\n",
       " 'hamos',\n",
       " 'Gorgia',\n",
       " 'lita',\n",
       " 'antas',\n",
       " 'ito',\n",
       " 'dele',\n",
       " 'Eous',\n",
       " 'Paros',\n",
       " 'Protagoras',\n",
       " 'mone',\n",
       " 'Persephone',\n",
       " 'pare',\n",
       " 'polite',\n",
       " 'sidereos',\n",
       " 'lu',\n",
       " 'manenti',\n",
       " 'metere',\n",
       " 'doma',\n",
       " 'Atrei',\n",
       " 'Parthenio',\n",
       " 'adamas',\n",
       " 'tropo',\n",
       " 'Leontinos',\n",
       " 'Lapithas',\n",
       " 'opta',\n",
       " 'diae',\n",
       " 'asto',\n",
       " 'Minoa',\n",
       " 'Cypria',\n",
       " 'mere',\n",
       " 'auge',\n",
       " 'stes',\n",
       " 'problema',\n",
       " 'ano',\n",
       " 'Diones',\n",
       " 'Thebes',\n",
       " 'Herme',\n",
       " 'pedali',\n",
       " 'beta',\n",
       " 'Naxo',\n",
       " 'temo',\n",
       " 'liga',\n",
       " 'Arete',\n",
       " 'nae',\n",
       " 'topo',\n",
       " 'Tantalo',\n",
       " 'iota',\n",
       " 'elate',\n",
       " 'problemata',\n",
       " 'Euripo',\n",
       " 'heroi',\n",
       " 'gemet',\n",
       " 'Mimas',\n",
       " 'auges',\n",
       " 'Amphitrite',\n",
       " 'dogmati',\n",
       " 'agones',\n",
       " 'Euripide',\n",
       " 'heroa',\n",
       " 'periodo',\n",
       " 'demo',\n",
       " 'emes',\n",
       " 'geme',\n",
       " 'Epigenes',\n",
       " 'mede',\n",
       " 'sito',\n",
       " 'assa',\n",
       " 'Hermogene',\n",
       " 'Miletos',\n",
       " 'Melite',\n",
       " 'sape',\n",
       " 'Same',\n",
       " 'saturo',\n",
       " 'Aristophane',\n",
       " 'Protagora',\n",
       " 'Triptolemo',\n",
       " 'aloes',\n",
       " 'Delio',\n",
       " 'neo',\n",
       " 'ese',\n",
       " 'Xanthippe',\n",
       " 'Leto',\n",
       " 'Lede',\n",
       " 'philo',\n",
       " 'aloe',\n",
       " 'protero',\n",
       " 'polles',\n",
       " 'morio',\n",
       " 'Laertes',\n",
       " 'Selene',\n",
       " 'exerat',\n",
       " 'heroos',\n",
       " 'Hippotades',\n",
       " 'exe',\n",
       " 'ale',\n",
       " 'aleis',\n",
       " 'Melantho',\n",
       " 'oles',\n",
       " 'glossas',\n",
       " 'lexeos',\n",
       " 'lupe',\n",
       " 'mete',\n",
       " 'ambrosio',\n",
       " 'mese',\n",
       " 'metreta',\n",
       " 'lae',\n",
       " 'poesi',\n",
       " 'pedo',\n",
       " 'Ares',\n",
       " 'lebes',\n",
       " 'anathemata',\n",
       " 'trite',\n",
       " 'Admete',\n",
       " 'Erote',\n",
       " 'erosa',\n",
       " 'Theodore',\n",
       " 'Pheres',\n",
       " 'Polites',\n",
       " 'aule',\n",
       " 'Laertiade',\n",
       " 'bio',\n",
       " 'Melete',\n",
       " 'agonia',\n",
       " 'Aretes',\n",
       " 'Eroti',\n",
       " 'psephismata',\n",
       " 'diastemata',\n",
       " 'etis',\n",
       " 'bie',\n",
       " 'Parmenide',\n",
       " 'peribolo',\n",
       " 'parergo',\n",
       " 'pedalia',\n",
       " 'pino',\n",
       " 'pero',\n",
       " 'stephano',\n",
       " 'oleto',\n",
       " 'metabole',\n",
       " 'Diomedeos',\n",
       " 'phthisis',\n",
       " 'pontones',\n",
       " 'Lampetie',\n",
       " 'dromo',\n",
       " 'nassa',\n",
       " 'Europes',\n",
       " 'Are',\n",
       " 'horai',\n",
       " 'topazo',\n",
       " 'Euenos',\n",
       " 'melo',\n",
       " 'geometre',\n",
       " 'Aristodemo',\n",
       " 'Neritos',\n",
       " 'Elpenor',\n",
       " 'Delon',\n",
       " 'Dardanide',\n",
       " 'hippo',\n",
       " 'Stesichorus',\n",
       " 'bolos',\n",
       " 'pale',\n",
       " 'Letoi',\n",
       " 'doto',\n",
       " 'pedalio',\n",
       " 'epistate',\n",
       " 'aletes',\n",
       " 'agre',\n",
       " 'ereto',\n",
       " 'iambe',\n",
       " 'halo',\n",
       " 'meni',\n",
       " 'peren',\n",
       " 'pege',\n",
       " 'laro',\n",
       " 'agoni',\n",
       " 'osi',\n",
       " 'daphne',\n",
       " 'geode',\n",
       " 'emata',\n",
       " 'pales',\n",
       " 'pleto',\n",
       " 'melie',\n",
       " 'potho',\n",
       " 'admete',\n",
       " 'elateri',\n",
       " 'depote',\n",
       " 'edos',\n",
       " 'agrio',\n",
       " 'orse',\n",
       " 'enen',\n",
       " 'xanthe',\n",
       " 'anterotas',\n",
       " 'hebe',\n",
       " 'apate',\n",
       " 'thalle',\n",
       " 'dido',\n",
       " 'phono',\n",
       " 'ose',\n",
       " 'Agamemnonides',\n",
       " 'telamon',\n",
       " 'erato',\n",
       " 'daphnes',\n",
       " 'lethe',\n",
       " 'psoras',\n",
       " 'zeta',\n",
       " 'ites',\n",
       " 'ambrosie',\n",
       " 'polle',\n",
       " 'aeto',\n",
       " 'aniso',\n",
       " 'nome']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greater_prob_latin = [word for word, latin_prob, greek_prob\n",
    "                     in shared_latin_greek\n",
    "                     if latin_prob >= greek_prob]\n",
    "print(len(greater_prob_latin))\n",
    "greater_prob_latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46,929 distinct transliterated Greek words without matches in the Latin corpus\n"
     ]
    }
   ],
   "source": [
    "# We'll remove the words that have a high probability of being Latin \n",
    "# from the collection of demacronized transliterated Greek words\n",
    "only_greek_transliterated = distinct_demacronized_greek - set(likely_latin)\n",
    "likely_greek = shared_words - set(likely_latin)\n",
    "\n",
    "# Likewise, let's remove the transliterated words that are likely Greek \n",
    "# from the collection of good Latin words\n",
    "distinct_good_latin = set(distinct_good_latin) - likely_greek\n",
    "\n",
    "print(f'{len(only_greek_transliterated):,} distinct transliterated Greek words without matches in the Latin corpus')\n",
    "\n",
    "# NOTE: we are toggling this on to see the difference\n",
    "# only_greek_transliterated=  distinct_demacronized_greek\n",
    "# distinct_latin_wo_greek_matches = distinct_good_latin - shared_words\n",
    "# The following had low precision high recall:\n",
    "# only_greek_transliterated = distinct_demacronized_greek - shared_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normally these gaps might concern us, but since we are more interested in groups of loanwords in phrases, we can rely on smoothing over clusters of loanwords to screen out misses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple data matrix of the single words, transliterated Greek examples followed by the Latin words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226601"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [list(only_greek_transliterated) + list(distinct_good_latin)]\n",
    "len(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we featurize our data matrix, let's check on the max word lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 Max word length in distinct good Latin sample\n",
      "25 Max word length in transliterated Greek sample\n"
     ]
    }
   ],
   "source": [
    "print(f'{sorted([len(tmp) for tmp in distinct_good_latin])[-1]} Max word length in distinct good Latin sample')\n",
    "print(f'{sorted([len(tmp) for tmp in only_greek_transliterated])[-1] } Max word length in transliterated Greek sample')\n",
    "max_len = sorted([len(tmp) for tmp in only_greek_transliterated])[-1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization\n",
    "#### We'll use a generic character to integer transform so that we can reuse our encoding process for unseen character data combinations (as opposed to building a dictionary mapping for a discrete sample space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create the X,y feature matrix and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (226601,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mlyoucanuse.featurize_text_fun:Excessive word length 28 for Thensaurochrysonicochrysides, truncating to 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (226601, 25)\n"
     ]
    }
   ],
   "source": [
    "all_y = np.array([1] * len(only_greek_transliterated) + [0] * len(distinct_good_latin), dtype=float)\n",
    "print(f'y shape: {all_y.shape}')\n",
    "# We use a label encoder to automatically capture the range of values for provenance\n",
    "# Although it's true we're only doing binary classification, \n",
    "# it's a good practice to use this so that we can automate recording our model's provenance\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_y)\n",
    "all_words = list(only_greek_transliterated) + list(distinct_good_latin)\n",
    "all_X = np.array([word_to_features(word, max_len) for word in all_words])\n",
    "print(f'X shape: {all_X.shape}')\n",
    "all_X = sparse.csr_matrix(all_X)\n",
    "num_samples = all_y.shape[0] # to be used later by model provenance\n",
    "num_features = all_X.shape[1] # to be used later by model provenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a DummyClassifier to show the baseline which we must improve above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier: 0.6731390443240189\n"
     ]
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy='stratified', random_state=0)\n",
    "features_train, features_test, target_train, target_test = train_test_split(all_X, all_y,\n",
    "                                                                            random_state=0)\n",
    "dummy.fit(features_train, target_train)\n",
    "dummy_score = dummy.score(features_test, target_test)\n",
    "print(f'Dummy classifier: {dummy_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and classify the data using several classifiers, printing out the cross validation score results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [11:30<34:32, 690.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.neighbors.classification.KNeighborsClassifier'> 0.8614789960399858 [0.86017519 0.85986629 0.86343778 0.86337158 0.86054414]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [12:41<16:49, 504.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.bagging.BaggingClassifier'> 0.9546030198770448 [0.95595861 0.95366386 0.95509709 0.95436893 0.95392661]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [13:40<06:11, 371.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.forest.ExtraTreesClassifier'> 0.9354460045235105 [0.93658569 0.93422475 0.93523831 0.93574581 0.93543547]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 4/4 [14:15<00:00, 270.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'> 0.9470787901939074 [0.94691203 0.94706648 0.94655781 0.94702118 0.94783645]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier,\n",
    "    BaggingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestClassifier\n",
    "]\n",
    "for cls in tqdm(classifiers):\n",
    "    scores = cross_val_score(cls(), all_X, all_y,\n",
    "                             scoring='accuracy',\n",
    "                             n_jobs=multiprocessing.cpu_count(),\n",
    "                             cv=5)\n",
    "    print('{} {} {}'.format(str(cls), scores.mean(), scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Grid Search to optimize one of the best classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: %s 0.9615006112064819\n",
      "Best params %s {'criterion': 'entropy', 'max_features': 0.4, 'n_estimators': 800}\n"
     ]
    }
   ],
   "source": [
    "grids = GridSearchCV(cv=5, error_score='raise',\n",
    "                     estimator=RandomForestClassifier()\n",
    "                     , n_jobs=2,\n",
    "                     param_grid={\n",
    "                         'criterion': ['entropy'],  # also tried'gini', \n",
    "                         'n_estimators': [ 750, 800], #600, 650, 700,\n",
    "                     # also tried 100, 300, 500, 550, 850, 900\n",
    "                         'max_features': [0.4]  # 0.2, 0.3,  0.7, 1.0                      \n",
    "                     })\n",
    "grids.fit(all_X, all_y)\n",
    "print('Best score: %s', grids.best_score_)\n",
    "print('Best params %s', grids.best_params_)\n",
    "\n",
    "# Best score: %s 0.9505224737836736\n",
    "# Best params %s {'criterion': 'entropy', 'max_features': 0.3, 'n_estimators': 600}\n",
    "# Best score: %s 0.9446973552104967\n",
    "# Best params %s {'criterion': 'entropy', 'max_features': 0.3, 'n_estimators': 700}\n",
    "# Best score: %s 0.9613020242629114\n",
    "# Best params %s {'criterion': 'entropy', 'max_features': 0.4, 'n_estimators': 750}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the best parameters from GridSearch, build the optimal classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features=0.4, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=800, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's copy the parameters for the provenance file\n",
    "mdl_params = deepcopy(grids.best_params_)\n",
    "# Let's also remove the base_estimator parameters, since they aren't honored by the constructor, unlike GridSearch\n",
    "if 'base_estimator__criterion' in mdl_params:\n",
    "    del mdl_params['base_estimator__criterion']\n",
    "classifier = RandomForestClassifier(**mdl_params)\n",
    "classifier.fit(all_X, all_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the classifier, so it can be loaded without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is_transliterated_greek.mdl.0.20.2.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_file = 'is_transliterated_greek.mdl.{}.joblib'.format(sklearn.__version__)\n",
    "dump(classifier, model_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model provenance \n",
    "#### Provenance records:\n",
    "* History of the classifier\n",
    "* What data was used\n",
    "* Which hyperparameters had what values\n",
    "* What algorithm was used\n",
    "* What score was achieved\n",
    "\n",
    "#### Important for allowing others:\n",
    "* To use the classifier in the future without rebuilding from scratch\n",
    "* To determining how it may be retrained for better performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote provenance file: is_transliterated_greek.mdl.0.20.2.joblib.prov.json\n"
     ]
    }
   ],
   "source": [
    "data_files = {}\n",
    "idx = 1\n",
    "\n",
    "for idx, file in enumerate(good_files, 1):\n",
    "    data_files[idx] = {\"filename\": file[file.rfind(\"/\") + 1:],\n",
    "                       \"md5\": md5(os.path.join(latin_reader.root, file))\n",
    "                       }\n",
    "\n",
    "for idx, file in enumerate(greek_texts, len(good_files)):\n",
    "    data_files[idx + 1] = {\n",
    "        \"filename\": file[file.rfind(\"/\") + 1:],\n",
    "        \"md5\": md5(os.path.join(perseus_greek.root, file))\n",
    "    }\n",
    "\n",
    "provenance_file = '{}.prov.json'.format(model_output_file)\n",
    "\n",
    "params = {\n",
    "    \"provenance_data\": provenance_file,\n",
    "    \"date_created\": str(datetime.datetime.now()),\n",
    "    \"model_parameters\": mdl_params,\n",
    "    \"max_word_length\": max_len,\n",
    "    \"num_samples\": num_samples,\n",
    "    \"num_features\": num_features,\n",
    "    \"library_version\": sklearn.__version__,\n",
    "    \"classifier_class\": \"{}\".format(str(classifier.__class__)),\n",
    "    \"classifier_best_score\": grids.best_score_,\n",
    "    \"data_files\": data_files,\n",
    "    \"model_output_file\": model_output_file,\n",
    "    \"model_output_md5\": md5(model_output_file),\n",
    "    \"labels\": label_encoder.classes_.tolist(),\n",
    "    \"best_score\": grids.best_score_,\n",
    "    \"best_params\": grids.best_params_,\n",
    "    # manually added information\n",
    "    \"comment\": \"Transliterated Greek Classifier\",\n",
    "    \"code_generated_by\": \"loanwords_problems_solutions.ipynb\",\n",
    "    \"feature_encoding_fun\": \"word_to_features\",\n",
    "    \"author\": \"Todd Cook\",\n",
    "    \"sample_pipeline\": process_latin_text_pipeline_file\n",
    "}\n",
    "\n",
    "with open(provenance_file, 'wt') as writer:\n",
    "    json.dump(params, writer, indent=2)\n",
    "    print('Wrote provenance file: {}'.format(provenance_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about that model provenance file? It should be readable, here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"provenance_data\": \"is_transliterated_greek.mdl.0.20.2.joblib.prov.json\",\n",
      "  \"date_created\": \"2019-02-27 23:11:35.141046\",\n",
      "  \"model_parameters\": {\n",
      "    \"criterion\": \"entropy\",\n",
      "    \"max_features\": 0.4,\n",
      "    \"n_estimators\": 800\n",
      "  },\n",
      "  \"max_word_length\": 25,\n",
      "  \"num_samples\": 226601,\n",
      "  \"num_features\": 25,\n",
      "  \"library_version\": \"0.20.2\",\n",
      "  \"classifier_class\": \"<class 'sklearn.ensemble.forest.RandomForestClassifier'>\",\n",
      "  \"classifier_best_score\": 0.9615006112064819,\n",
      "  \"data_files\": {\n",
      "    \"1\": {\n",
      "      \"filename\": \"bc1.txt\",\n",
      "      \"md5\": \"3bbb7a0755de2c548bd42a286374d809\"\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"filename\": \"bc2.txt\",\n",
      "      \"md5\": \"1d41b8371fb64407bf2701fc2567d8c3\"\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"filename\": \"bc3.txt\",\n",
      "      \"md5\": \"43ba4f905803f74a6c7c79ceec5b991f\"\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"filename\": \"bellafr.txt\",\n",
      "      \"md5\": \"e42fb6ac2b7b5060bad4d86cbdef6d53\"\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"filename\": \"gall1.txt\",\n",
      "      \"md5\": \"1b4bf3b593e6a7d33dadc518719e3e8d\"\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"filename\": \"gall2.txt\",\n",
      "      \"md5\": \"ad7deff6f37a869c095b66ec5f974fec\"\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"filename\": \"gall3.txt\",\n",
      "      \"md5\": \"a68dadc4d308e1e47ca714a00b9db36c\"\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"filename\": \"gall4.txt\",\n",
      "      \"md5\": \"5d59a6fc9d6d66c7f505d208e9b90eaa\"\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"filename\": \"gall5.txt\",\n",
      "      \"md5\": \"875b11f7ba474f2e682ebf57cdf2af91\"\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"filename\": \"gall6.txt\",\n",
      "      \"md5\": \"e8645e46f3a8ed30fba3ad582cd6bb84\"\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"filename\": \"gall7.txt\",\n",
      "      \"md5\": \"db56d8ca46c77ee856e6c2ea72556579\"\n",
      "    },\n",
      "    \"12\": {\n",
      "      \"filename\": \"gall8.txt\",\n",
      "      \"md5\": \"bfc174bca341688b801e1252b39c4fe8\"\n",
      "    },\n",
      "    \"13\": {\n",
      "      \"filename\": \"eutropius1.txt\",\n",
      "      \"md5\": \"0dba231b290b45abd77a5c49c95f5a27\"\n",
      "    },\n",
      "    \"14\": {\n",
      "      \"filename\": \"eutropius10.txt\",\n",
      "      \"md5\": \"d41751aec253d7168bd0bb7ff961c794\"\n",
      "    },\n",
      "    \"15\": {\n",
      "      \"filename\": \"eutropius2.txt\",\n",
      "      \"md5\": \"40600e2dcf920c695eb4cac484635a7a\"\n",
      "    },\n",
      "    \"16\": {\n",
      "      \"filename\": \"eutropius3.txt\",\n",
      "      \"md5\": \"1b2a672fc1432c5dac2b4283f072b338\"\n",
      "    },\n",
      "    \"17\": {\n",
      "      \"filename\": \"eutropius4.txt\",\n",
      "      \"md5\": \"d76caf78179cc29fdc6847010d0b76e8\"\n",
      "    },\n",
      "    \"18\": {\n",
      "      \"filename\": \"eutropius5.txt\",\n",
      "      \"md5\": \"d63d9bfef33046d4d047319b4b004a84\"\n",
      "    },\n",
      "    \"19\": {\n",
      "      \"filename\": \"eutropius6.txt\",\n",
      "      \"md5\": \"3235ad10251940c0119bc257625274c3\"\n",
      "    },\n",
      "    \"20\": {\n",
      "      \"filename\": \"eutropius7.txt\",\n",
      "      \"md5\": \"731dc1b29e08b7261b0c165dcfcfb793\"\n",
      "    },\n",
      "    \"21\": {\n",
      "      \"filename\": \"eutropius8.txt\",\n",
      "      \"md5\": \"16033e61f59202679367a1bb7d5919e3\"\n",
      "    },\n",
      "    \"22\": {\n",
      "      \"filename\": \"eutropius9.txt\",\n",
      "      \"md5\": \"48bd302b927c6379d697150dd56c9189\"\n",
      "    },\n",
      "    \"23\": {\n",
      "      \"filename\": \"prud1.txt\",\n",
      "      \"md5\": \"57588fae1677a2f206941f38deb345b0\"\n",
      "    },\n",
      "    \"24\": {\n",
      "      \"filename\": \"prud10.txt\",\n",
      "      \"md5\": \"32f15144437f2bd3d9a6ed26ffc76a19\"\n",
      "    },\n",
      "    \"25\": {\n",
      "      \"filename\": \"prud11.txt\",\n",
      "      \"md5\": \"1286ea56d5390e69674e0c2dea8e3ea4\"\n",
      "    },\n",
      "    \"26\": {\n",
      "      \"filename\": \"prud12.txt\",\n",
      "      \"md5\": \"b15f7a1c485e0084e8f3d32c0174766e\"\n",
      "    },\n",
      "    \"27\": {\n",
      "      \"filename\": \"prud13.txt\",\n",
      "      \"md5\": \"b8b4d17ee7bf78150a0dc5e0bfdc183a\"\n",
      "    },\n",
      "    \"28\": {\n",
      "      \"filename\": \"prud14.txt\",\n",
      "      \"md5\": \"228712225e89a3f1b49533d0d32da392\"\n",
      "    },\n",
      "    \"29\": {\n",
      "      \"filename\": \"prud2.txt\",\n",
      "      \"md5\": \"38a6f1cc241547c79aeb7a1c25f8509d\"\n",
      "    },\n",
      "    \"30\": {\n",
      "      \"filename\": \"prud3.txt\",\n",
      "      \"md5\": \"bee85a06ab5df2134b9a2b9205c1d965\"\n",
      "    },\n",
      "    \"31\": {\n",
      "      \"filename\": \"prud4.txt\",\n",
      "      \"md5\": \"08871540d4d70703402b9c7ba036d92d\"\n",
      "    },\n",
      "    \"32\": {\n",
      "      \"filename\": \"prud5.txt\",\n",
      "      \"md5\": \"aebf4e7ad7e51bab93d5657daea4d28c\"\n",
      "    },\n",
      "    \"33\": {\n",
      "      \"filename\": \"prud6.txt\",\n",
      "      \"md5\": \"9a4a6e6e75d74cc91add3e3c5a01e59b\"\n",
      "    },\n",
      "    \"34\": {\n",
      "      \"filename\": \"prud7.txt\",\n",
      "      \"md5\": \"966108346530f9b55bdc42532d9d8595\"\n",
      "    },\n",
      "    \"35\": {\n",
      "      \"filename\": \"prud8.txt\",\n",
      "      \"md5\": \"7a16e3673f6c115a8fdf8d6a0d6a0ea3\"\n",
      "    },\n",
      "    \"36\": {\n",
      "      \"filename\": \"prud9.txt\",\n",
      "      \"md5\": \"3e12bd972124a4507b727a69e1af91cd\"\n",
      "    },\n",
      "    \"37\": {\n",
      "      \"filename\": \"plato__alcibiades-1__grc.json\",\n",
      "      \"md5\": \"751fdf9b9626863d5dacacb6509d4a40\"\n",
      "    },\n",
      "    \"38\": {\n",
      "      \"filename\": \"plato__alcibiades-2__grc.json\",\n",
      "      \"md5\": \"dd4b96417126597098b8a96af2944e13\"\n",
      "    },\n",
      "    \"39\": {\n",
      "      \"filename\": \"plato__apology__grc.json\",\n",
      "      \"md5\": \"e4b546c37bac017b9890d627ec1043ff\"\n",
      "    },\n",
      "    \"40\": {\n",
      "      \"filename\": \"plato__charmides__grc.json\",\n",
      "      \"md5\": \"3c61ef4f4ef3e3af8072e91db49a83d0\"\n",
      "    },\n",
      "    \"41\": {\n",
      "      \"filename\": \"plato__cratylus__grc.json\",\n",
      "      \"md5\": \"51b914a758394750b76732ea39491eb2\"\n",
      "    },\n",
      "    \"42\": {\n",
      "      \"filename\": \"plato__crito__grc.json\",\n",
      "      \"md5\": \"b0cd1626398dd3afbf908bfe17d50ed7\"\n",
      "    },\n",
      "    \"43\": {\n",
      "      \"filename\": \"plato__euthydemus__grc.json\",\n",
      "      \"md5\": \"74adeebf47d56c53c7a82883c66d18c0\"\n",
      "    },\n",
      "    \"44\": {\n",
      "      \"filename\": \"plato__euthyphro__grc.json\",\n",
      "      \"md5\": \"dc6b9eaca2f05d9467e3e655bbd27d38\"\n",
      "    },\n",
      "    \"45\": {\n",
      "      \"filename\": \"plato__hipparchus__grc.json\",\n",
      "      \"md5\": \"e913d77f04e17a4dcd051fcec55bb25a\"\n",
      "    },\n",
      "    \"46\": {\n",
      "      \"filename\": \"plato__laches__grc.json\",\n",
      "      \"md5\": \"9ae676aa76485db29ce62abf2316b44d\"\n",
      "    },\n",
      "    \"47\": {\n",
      "      \"filename\": \"plato__lovers__grc.json\",\n",
      "      \"md5\": \"ef4ba45a873378b971d3abcdcc6d56d7\"\n",
      "    },\n",
      "    \"48\": {\n",
      "      \"filename\": \"plato__lysis__grc.json\",\n",
      "      \"md5\": \"0c0c5e68524d10248a8cf3a9e62a6c4d\"\n",
      "    },\n",
      "    \"49\": {\n",
      "      \"filename\": \"plato__parmenides__grc.json\",\n",
      "      \"md5\": \"418c9f8c21ac0f1af65dedbc71938a0f\"\n",
      "    },\n",
      "    \"50\": {\n",
      "      \"filename\": \"plato__phaedo__grc.json\",\n",
      "      \"md5\": \"f593b5b5c114982fd1fda8779f7158d7\"\n",
      "    },\n",
      "    \"51\": {\n",
      "      \"filename\": \"plato__phaedrus__grc.json\",\n",
      "      \"md5\": \"7ddcaf234a08dfb6a87e0e168540833e\"\n",
      "    },\n",
      "    \"52\": {\n",
      "      \"filename\": \"plato__philebus__grc.json\",\n",
      "      \"md5\": \"f5098a2471ec46c33a4861e7c86d9d39\"\n",
      "    },\n",
      "    \"53\": {\n",
      "      \"filename\": \"plato__sophist__grc.json\",\n",
      "      \"md5\": \"ab4cbdbdcdeca510d230396151302d71\"\n",
      "    },\n",
      "    \"54\": {\n",
      "      \"filename\": \"plato__statesman__grc.json\",\n",
      "      \"md5\": \"9707acc3a72834aa63017a743bb3afec\"\n",
      "    },\n",
      "    \"55\": {\n",
      "      \"filename\": \"plato__symposium__grc.json\",\n",
      "      \"md5\": \"eafbf58e7624372a71e2cdd0d39d2525\"\n",
      "    },\n",
      "    \"56\": {\n",
      "      \"filename\": \"plato__theaetetus__grc.json\",\n",
      "      \"md5\": \"fd50f4a9bbdaf4ab7ccf630c5d9db223\"\n",
      "    },\n",
      "    \"57\": {\n",
      "      \"filename\": \"plato__theages__grc.json\",\n",
      "      \"md5\": \"53d891aa6cc8cce2eb83cc6d3ec35a18\"\n",
      "    },\n",
      "    \"58\": {\n",
      "      \"filename\": \"homer__odyssey__grc.json\",\n",
      "      \"md5\": \"f46270638106d2daaa0a2828cf268422\"\n",
      "    },\n",
      "    \"59\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-1-to-dionysus__grc.json\",\n",
      "      \"md5\": \"ff88ac17a6adbeebe517971e576f6e27\"\n",
      "    },\n",
      "    \"60\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-10-to-aphrodite__grc.json\",\n",
      "      \"md5\": \"9bbe5c96b81dd9660f4e0a8be0662ea6\"\n",
      "    },\n",
      "    \"61\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-11-to-athena__grc.json\",\n",
      "      \"md5\": \"d3bbcacc93dbc839579a8717a46ad3e3\"\n",
      "    },\n",
      "    \"62\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-12-to-hera__grc.json\",\n",
      "      \"md5\": \"d7248209627c5b489b63019665558cfe\"\n",
      "    },\n",
      "    \"63\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-13-to-demeter__grc.json\",\n",
      "      \"md5\": \"fa044a41a8fecc6bba0ce18e9f49c13d\"\n",
      "    },\n",
      "    \"64\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-14-to-the-mother-of-the-gods__grc.json\",\n",
      "      \"md5\": \"d4d7d72adb171149d136740b06a7d286\"\n",
      "    },\n",
      "    \"65\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-15-to-heracles__grc.json\",\n",
      "      \"md5\": \"a9c509199cde7bca8405bc4b9e422bd7\"\n",
      "    },\n",
      "    \"66\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-16-to-asclepius__grc.json\",\n",
      "      \"md5\": \"d2ef4f2efdb107980eeceb2f4943c19b\"\n",
      "    },\n",
      "    \"67\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-17-to-the-dioscuri__grc.json\",\n",
      "      \"md5\": \"243ef6d48ed0ee0c7d224cc78bbf6f7e\"\n",
      "    },\n",
      "    \"68\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-18-to-hermes__grc.json\",\n",
      "      \"md5\": \"9f558df2a69cb840bf6f878be0dc49f8\"\n",
      "    },\n",
      "    \"69\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-19-to-pan__grc.json\",\n",
      "      \"md5\": \"5528b4957c582a1176834090cf800d5e\"\n",
      "    },\n",
      "    \"70\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-2-to-demeter__grc.json\",\n",
      "      \"md5\": \"5c75d7ceb1e478998e9cf09caa8296b5\"\n",
      "    },\n",
      "    \"71\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-20-to-hephaestus__grc.json\",\n",
      "      \"md5\": \"4f59a40d728477a5c68ed2865af81ee6\"\n",
      "    },\n",
      "    \"72\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-21-to-apollo__grc.json\",\n",
      "      \"md5\": \"6fb6d54dc1860c619cd824c4f5d86cf7\"\n",
      "    },\n",
      "    \"73\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-22-to-poseidon__grc.json\",\n",
      "      \"md5\": \"a16791d3134f78e99d7ca7fd6b9c42bb\"\n",
      "    },\n",
      "    \"74\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-23-to-zeus__grc.json\",\n",
      "      \"md5\": \"086a9c28056e32599ae7a4b6761fa4e0\"\n",
      "    },\n",
      "    \"75\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-24-to-hestia__grc.json\",\n",
      "      \"md5\": \"fc88b8a9601833a2c21cdcd7d88801b1\"\n",
      "    },\n",
      "    \"76\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-25-to-the-muses-and-apollo__grc.json\",\n",
      "      \"md5\": \"53e24924a14265a3320e479797432523\"\n",
      "    },\n",
      "    \"77\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-26-to-dionysus__grc.json\",\n",
      "      \"md5\": \"34446d2ae6668ee82629b6c47738e815\"\n",
      "    },\n",
      "    \"78\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-27-to-artemis__grc.json\",\n",
      "      \"md5\": \"9cfaad773cb8232718f78c2d0a07306b\"\n",
      "    },\n",
      "    \"79\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-28-to-athena__grc.json\",\n",
      "      \"md5\": \"229ec0914943fd5c848330e268c22105\"\n",
      "    },\n",
      "    \"80\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-29-to-hestia__grc.json\",\n",
      "      \"md5\": \"c7e40337b22b15f623d0b6d54bc82817\"\n",
      "    },\n",
      "    \"81\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-3-to-delian-and-pythian-apollo__grc.json\",\n",
      "      \"md5\": \"e92882a9d1e686860cb5c45bb4f97d9e\"\n",
      "    },\n",
      "    \"82\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-30-to-earth__grc.json\",\n",
      "      \"md5\": \"f663c38411303148ed4d1349141e13c3\"\n",
      "    },\n",
      "    \"83\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-31-to-helios__grc.json\",\n",
      "      \"md5\": \"31300fe836e4086d906ee7bd2033089f\"\n",
      "    },\n",
      "    \"84\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-32-to-selene__grc.json\",\n",
      "      \"md5\": \"a296d0d31380776958e637f08d3dd810\"\n",
      "    },\n",
      "    \"85\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-33-to-the-dioscuri__grc.json\",\n",
      "      \"md5\": \"a64bc1a297a83f974c0a633fe874f789\"\n",
      "    },\n",
      "    \"86\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-4-to-hermes__grc.json\",\n",
      "      \"md5\": \"8a994c0d80e5234823b1e7c4cf2fca0b\"\n",
      "    },\n",
      "    \"87\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-5-to-aphrodite__grc.json\",\n",
      "      \"md5\": \"0e53b784d0a0acc3b18a5c5ee6441d83\"\n",
      "    },\n",
      "    \"88\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-6-to-aphrodite__grc.json\",\n",
      "      \"md5\": \"8ed4d1deef268e3fdfc3a15c4d529665\"\n",
      "    },\n",
      "    \"89\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-7-to-dionysus__grc.json\",\n",
      "      \"md5\": \"8c88adbb3511b074e95b4f9b5e21f91b\"\n",
      "    },\n",
      "    \"90\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-8-to-ares__grc.json\",\n",
      "      \"md5\": \"904d1b6c2d4f20a314b9a89e5433bdec\"\n",
      "    },\n",
      "    \"91\": {\n",
      "      \"filename\": \"homeric-hymns__hymn-9-to-artemis__grc.json\",\n",
      "      \"md5\": \"95ae7ea977ccd0130ac3950aade30db4\"\n",
      "    }\n",
      "  },\n",
      "  \"model_output_file\": \"is_transliterated_greek.mdl.0.20.2.joblib\",\n",
      "  \"model_output_md5\": \"568c122fc792ca4833d600648ed9dd10\",\n",
      "  \"labels\": [\n",
      "    0.0,\n",
      "    1.0\n",
      "  ],\n",
      "  \"best_score\": 0.9615006112064819,\n",
      "  \"best_params\": {\n",
      "    \"criterion\": \"entropy\",\n",
      "    \"max_features\": 0.4,\n",
      "    \"n_estimators\": 800\n",
      "  },\n",
      "  \"comment\": \"Transliterated Greek Classifier\",\n",
      "  \"code_generated_by\": \"loanwords_problems_solutions.ipynb\",\n",
      "  \"feature_encoding_fun\": \"word_to_features\",\n",
      "  \"author\": \"Todd Cook\",\n",
      "  \"sample_pipeline\": \"process_latin_text_pipeline.0.20.2.joblib\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(params, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstitute the classifier for use at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = load(model_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some demo examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(\n",
    "    sparse.csr_matrix(np.array([word_to_features(word, max_len) for word in ['quid', 'est', 'veritas']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(\n",
    "    sparse.csr_matrix(np.array([word_to_features(word, max_len) for word in 'ou eis panta ton'.split()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Cluster Holes\n",
    "as you can see from the example above, sometime the classifier will drop out and not classify something correctly in a sequence. Quite often these dropouts are words that could be found in either language. If two good classifications bookend a dropout, then the dropout should probably be filled in.  We've got a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_cluster_holes(classifier.predict(\n",
    "    sparse.csr_matrix(np.array([word_to_features(word, max_len) for word in 'ou eis panta ton'.split()]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at the author Pliny the Younger for transliterated Greek words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [50:54<00:00, 293.48s/it]\n"
     ]
    }
   ],
   "source": [
    "greek_in_pliny = set()\n",
    "latin_reader = get_corpus_reader(corpus_name='latin_text_latin_library', language='latin')\n",
    "results = defaultdict(list)\n",
    "\n",
    "selected_files = [file for file in latin_reader.fileids()\n",
    "                  if 'pliny.ep' in file]\n",
    "\n",
    "for file in tqdm(selected_files):\n",
    "    for sent in latin_reader.sents(file):\n",
    "        unseen_X = process_latin_text_pipeline.fit_transform([(sent)])\n",
    "        if unseen_X and len(unseen_X[0][0]) > 1:\n",
    "            arr = classifier.predict(\n",
    "                sparse.csr_matrix(np.array([word_to_features(word, max_len) for sentence in unseen_X for word in\n",
    "                          sentence])))\n",
    "            arr = patch_cluster_holes(arr)\n",
    "            found_greek = extract_words(unseen_X[0], *run_length_encoding(arr))  # works with sent\n",
    "            if found_greek:\n",
    "                results[file].append(found_greek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['gar', 'hoi', 'euzêloi']],\n",
       "  [['kata', 'Meidiou']],\n",
       "  [['akousma', 'epainos']]],\n",
       " [[['logismos', 'de', 'okno', 'ne', 'pherei']],\n",
       "  [['phônê', 'ne', 'kai', 'gegêthôs', 'kai']],\n",
       "  [['keitai', 'Patroklos']],\n",
       "  [['tauta', 'hyper', 'melêsei']],\n",
       "  [['hoti', 'toioutos', 'esti', 'ne', 'hoisper']]],\n",
       " [[['kakos', 'êde', 'kai', 'esthlos']]],\n",
       " [[['nomou', 'Memphitou']]],\n",
       " [[['edôke', 'patêr', 'hetero', 'ne', 'aneneusen']],\n",
       "  [['kai', 'kyaneêsi', 'ne', 'ep', 'ophrysi', 'neuse']],\n",
       "  [['kai', 'gar', 'onar', 'ek', 'Dios', 'esti']],\n",
       "  [['logisamenos',\n",
       "    'illud',\n",
       "    'heis',\n",
       "    'oiônos',\n",
       "    'aristos',\n",
       "    'amynesthai',\n",
       "    'peri']],\n",
       "  [['panta', 'denique', 'litho', 'ne', 'kinô']],\n",
       "  [['pros',\n",
       "    'de',\n",
       "    'autou',\n",
       "    'taxei',\n",
       "    'peithô',\n",
       "    'tis',\n",
       "    'epekathêto',\n",
       "    'toisi',\n",
       "    'cheilesi']],\n",
       "  [['ekêlei', 'kai', 'monos'], ['enkateleipe', 'tois', 'akroômenois']],\n",
       "  [['kai', 'epea', 'niphadessi', 'ne', 'eoikota']]]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list(results.values()), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['autou', 'tou', 'thêriou', 'êkousate']],\n",
       "  [['Sophokleis', 'uocantur', 'apo', 'tou', 'sophôs', 'kai', 'kaleisthai']],\n",
       "  [['ti', 'diateinomai']]],\n",
       " [[['kakos', 'êde', 'kai', 'esthlos']]],\n",
       " [[['hosiê', 'phthimenoisi']],\n",
       "  [['neoi', 'teirousi', 'machêtai']],\n",
       "  [['amphi', 'de', 'salpinxe', 'ne', 'megas', 'ouranos'],\n",
       "   ['oute', 'thalassês', 'kyma', 'toso']],\n",
       "  [['anthrôpoi', 'miaroi', 'kolakes', 'kai'],\n",
       "   ['ou', 'lithois'],\n",
       "   ['oude', 'plinthois', 'egô'],\n",
       "   ['ouk', 'ek'],\n",
       "   ['Euboia', 'ne', 'probebalesthai'],\n",
       "   ['Ìegô', 'de', 'oimai'],\n",
       "   ['andres',\n",
       "    'Athênaioi',\n",
       "    'tous',\n",
       "    'theous',\n",
       "    'ekeino',\n",
       "    'ne',\n",
       "    'methyei',\n",
       "    'ne',\n",
       "    'megethei']],\n",
       "  [['tote', 'egô'], ['kai', 'pollô', 'reonti', 'kath']],\n",
       "  [['ek', 'pleonexias', 'kai', 'ponêrias', 'tis', 'houtos'],\n",
       "   ['kai', 'mikro'],\n",
       "   ['hapanta', 'anechaitise', 'kai', 'dielyse']],\n",
       "  [['apeschoinismenos', 'hapasi', 'tois', 'en', 'polei', 'dikaiois'],\n",
       "   ['tauta', 'eleo', 'ne', 'proudôkas', 'Aristogeito']],\n",
       "  [['pros', 'hous', 'autos', 'echôsas', 'limenas', 'kai'],\n",
       "   ['pros', 'toutous', 'hormizou']],\n",
       "  [['oudena', 'horô'], ['onta', 'alla', 'panta']],\n",
       "  [['dedoika',\n",
       "    'doxête',\n",
       "    'tisi',\n",
       "    'to',\n",
       "    'ne',\n",
       "    'aei',\n",
       "    'boulomeno',\n",
       "    'ne',\n",
       "    'einai'],\n",
       "   ['polei', 'paidotribei']],\n",
       "  [['oude',\n",
       "    'gar',\n",
       "    'tous',\n",
       "    'progonous',\n",
       "    'hypolambanô',\n",
       "    'ta',\n",
       "    'dikastêria',\n",
       "    'tauta'],\n",
       "   ['oikodomêsai', 'hina', 'tous', 'toioutous', 'en', 'autois']],\n",
       "  [['esti', 'ponêrias', 'kai', 'palinkapêlos', 'kai']],\n",
       "  [['gar',\n",
       "    'andres',\n",
       "    'Athênaioi',\n",
       "    'to',\n",
       "    'auto',\n",
       "    'phthengesthai',\n",
       "    'to',\n",
       "    'ne',\n",
       "    'kai',\n",
       "    'to',\n",
       "    'ne',\n",
       "    'nomonAE',\n",
       "    'hota'],\n",
       "   ['phônê', 'ne', 'aphiê', 'ho', 'nomos', 'hetera']],\n",
       "  [['epeita', 'anaphainetai', 'peri']],\n",
       "  [['all',\n",
       "    'enkathêmenoi',\n",
       "    'kai',\n",
       "    'enedreuontes',\n",
       "    'en',\n",
       "    'akroasei',\n",
       "    'eiselaunete',\n",
       "    'auto'],\n",
       "   ['tous', 'paranomous', 'logous']],\n",
       "  [['all',\n",
       "    'en',\n",
       "    'tais',\n",
       "    'hippodromiais',\n",
       "    'eis',\n",
       "    'to',\n",
       "    'ne',\n",
       "    'tou',\n",
       "    'pragmatos',\n",
       "    'auto',\n",
       "    'ne',\n",
       "    'dromo',\n",
       "    'ne',\n",
       "    'eiselaunete']],\n",
       "  [['helkopoieis', 'kai'], ['soi', 'melei'], ['logô', 'ne', 'poleôs']],\n",
       "  [['ouk', 'apopempsesthe', 'to'],\n",
       "   ['symphora', 'ne', 'syllabontes'],\n",
       "   ['ep', 'onomatôn', 'dia', 'politeias', 'pleonta', 'timôrêsesthe']]],\n",
       " [[['gar', 'aoidê'],\n",
       "   ['epikleious',\n",
       "    'anthrôpoi',\n",
       "    'tis',\n",
       "    'akouontessi',\n",
       "    'neôtatê',\n",
       "    'amphipelêtai']]],\n",
       " [[['ou', 'tis', 'emeu']],\n",
       "  [['kata', 'kephalaio']],\n",
       "  [['hyper', 'Ktêsiphôntos']]]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list(results.values()), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the model \n",
    "We can assess the classifier's performance by showing how it behaves with different amounts of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Learning Curves (RandomForest)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)  # changed 100 to 50\n",
    "estimator = RandomForestClassifier(**mdl_params)\n",
    "ylim = (0.7, 1.01)\n",
    "n_jobs = 7\n",
    "train_sizes = np.linspace(.1, 1.0, 5)\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "plt.title(title)\n",
    "if ylim is not None:\n",
    "    plt.ylim(*ylim)\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator, all_X, all_y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.grid()\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout(pad=0.5, w_pad=20, h_pad=0.5)\n",
    "plt.show()\n",
    "# plt.savefig('loanwords.prob.solutions.learningcurve.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find which texts in the Latin Library have the most transliterated Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = {}\n",
    "latin_reader = get_corpus_reader(corpus_name='latin_text_latin_library', language='latin')\n",
    "greek_in_corpus_selection = set()\n",
    "found_greek = []\n",
    "\n",
    "for full_path in tqdm(latin_reader.fileids(), total=len(latin_reader.fileids()), unit='files'):\n",
    "    filename = full_path[full_path.rfind('/') + 1:]\n",
    "    unseen_X = process_latin_text_pipeline.fit_transform(list(latin_reader.sents([full_path])))\n",
    "    distinct_unseen = distinct_words(unseen_X)\n",
    "    unseen_words = list(distinct_unseen)\n",
    "    total_words = [word\n",
    "                   for sentence in unseen_X\n",
    "                   for word in sentence]\n",
    "    arr = classifier.predict(sparse(np.array([word_to_features(word, max_len) for word in total_words])))\n",
    "    total_greek_words = np.count_nonzero(arr)\n",
    "    marks = arr.tolist()\n",
    "    if marks:\n",
    "        found_greek = [total_words[idx]\n",
    "                       for idx, point in enumerate(marks)\n",
    "                       if point == 1]\n",
    "        greek_in_corpus_selection |= set(found_greek)\n",
    "        corpus_files[filename] = (len(total_words), total_greek_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total Greek words found in Latin selection {len(greek_in_corpus_selection):,}')\n",
    "print(f'Number of Greek words not in training data: {len(greek_in_corpus_selection - only_greek_transliterated):,}')\n",
    "print(f'Random sample: {random.sample(greek_in_corpus_selection, 20)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus_files ))\n",
    "rankings = [(key, val[0], val[1], val[1]/val[0]) for key, val in corpus_files.items()]\n",
    "rankings.sort(key=lambda x: x[3])\n",
    "for rank in rankings:\n",
    "    print (rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all for now folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
