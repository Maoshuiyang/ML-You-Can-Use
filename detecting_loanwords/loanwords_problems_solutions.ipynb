{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem of Loanwords: Detection and Remedies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Loanword?\n",
    "They are real: https://en.wikipedia.org/wiki/Loanword but they often overlooked in everyday use. Take this example conversation:\n",
    "* \"It was nice seeing you again. Adios amigo.\"\n",
    "* \"Vaya con Dios!\"\n",
    "\n",
    "#### The last two sentences are loanwords, borrowings from Spanish. Most participants can navigate and interpret conversations even if they don't know the exact translations of the loanwords.\n",
    "\n",
    "However, when doing Natural Language Processing loanwords may become a pesky problem, for example:\n",
    "* If you are trying to generate a high quality embedding\n",
    "    * you many need to detect and drop out the loanwords since they are out of scope for typical monolingual embeddings.\n",
    "* If your application is translating a sequence of sentences\n",
    "    * you may need to switch models to create a better translation of a loanword sequence.\n",
    "* Some loanwords may occur in both languages \n",
    "    * you will need to use other heuristic measures to increase your precision and recall in detection.\n",
    "#### etc.\n",
    "### So let's look at how we can detect and work around these problem areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook you will:\n",
    "1. Perform ETL (Extract-Transform-Load)\n",
    "    1. Extract data using a Corpus Reader object\n",
    "    1. Transform the data using a reusable, composable Scikit-Learn Pipeline object\n",
    "    1. Load the data into a matrix and train a classifier\n",
    "1. Train several classifiers and select the best algorithm for the data\n",
    "1. Use GridSearch to tune hyperparameters to achieve the best algorithm performance\n",
    "1. Use the classifier to predict the classification of each word\n",
    "1. Extract whole phrases by clustering occurences of predicted words \n",
    "1. Save the pipelines and classifier for reuse at runtime\n",
    "1. Record the trained classifier model's provenance\n",
    "1. Examine some unseen, untrained data to discover how well the classifier generalizes to unseen data\n",
    "\n",
    "## Our problem: Detect Transliterated Greek in Classical Latin Authors\n",
    "### Why? \n",
    "### To make a high quality word embedding, using a relatively limited corpus, it's important to filter out foreign words. Often Latin authors will quote Greek authors and transliterate the Greek into Latin equivalents; however the resulting words often haven't been generally adopted in the language. True, some transliterated Greek words are valid Latin words, but we aren't concerned about a word here or there, but rather clusters of foreign words in the source language. \n",
    "### The data sets are from CLTK (Classical Language Toolkit http://cltk.org/ ):\n",
    "* The Latin Library\n",
    "    * Julius Caesar\n",
    "    * Prudentius\n",
    "    * Eutropius\n",
    "* The Perseus Library Greek Texts\n",
    "    * The Works of Plato\n",
    "    * Homer's Odyssey\n",
    "* We will augment our Latin data set using probability distributions in tandem with CLTK's lemmatization dictionary.\n",
    "\n",
    "#### The probability distributions come from our other notebooks:\n",
    "* `building_language_model/make_frequency_distribution.ipynb` \n",
    "* `detecting_loanwords/make_frequency_distribution_greek_transliterated.ipynb`\n",
    "\n",
    "#### The Greek will be transliterated into Latin. \n",
    "#### We will use our classifier to examine the corpus of Pliny the Younger to detect the use of transliterated Greek words, and we'll assess the classifier's effectiveness on the entire Latin Library corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import site\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from scipy import sparse\n",
    " \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from joblib import dump, load\n",
    "import sklearn\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.prosody.latin.scansion_constants import ScansionConstants\n",
    "from cltk.prosody.latin.string_utils import remove_punctuation_dict\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "from cltk.utils.featurization import word_to_features\n",
    "from cltk.utils.file_operations import md5\n",
    "from cltk.utils.matrix_corpus_fun import (\n",
    "    distinct_words,\n",
    "    separate_camel_cases,\n",
    "    drop_empty_lists,\n",
    "    drop_non_lower,\n",
    "    drop_arabic_numeric,\n",
    "    drop_all_caps,\n",
    "    drop_empty_strings,\n",
    "    jv_transform,\n",
    "    splice_hyphens,\n",
    "    accept_editorial,\n",
    "    profile_chars,\n",
    "    demacronize,\n",
    "    drop_enclitics,\n",
    "    drop_fringe_punctuation,\n",
    "    divide_separate_words,\n",
    "    drop_all_punctuation)\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add parent directory to path so we can access our common code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlyoucanuse.romanizer import Romanizer, romanizer_transform  \n",
    "from mlyoucanuse.aeoe_replacer import aeoe_transform\n",
    "from mlyoucanuse.matrix_fun import (run_length_encoding,\n",
    "                                    extract_words,                                     \n",
    "                                    patch_cluster_holes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn on logging, primarily so that library methods may report warnings, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = logging.getLogger('make_model')\n",
    "LOG.addHandler(logging.NullHandler())\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a CorpusReader and select only the text files of Prudentius, Caesar and Eutropius. \n",
    "#### As shown in the appendix of this notebook, the authors of this seed set have a low incidence of using transliterated Greek words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:make_model:available good files 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['caesar/alex.txt',\n",
       " 'caesar/bc1.txt',\n",
       " 'caesar/bc2.txt',\n",
       " 'caesar/bc3.txt',\n",
       " 'caesar/bellafr.txt',\n",
       " 'caesar/gall1.txt',\n",
       " 'caesar/gall2.txt',\n",
       " 'caesar/gall3.txt',\n",
       " 'caesar/gall4.txt',\n",
       " 'caesar/gall5.txt',\n",
       " 'caesar/gall6.txt',\n",
       " 'caesar/gall7.txt',\n",
       " 'caesar/gall8.txt',\n",
       " 'caesar/hisp.txt',\n",
       " 'eutropius/eutropius1.txt',\n",
       " 'eutropius/eutropius10.txt',\n",
       " 'eutropius/eutropius2.txt',\n",
       " 'eutropius/eutropius3.txt',\n",
       " 'eutropius/eutropius4.txt',\n",
       " 'eutropius/eutropius5.txt',\n",
       " 'eutropius/eutropius6.txt',\n",
       " 'eutropius/eutropius7.txt',\n",
       " 'eutropius/eutropius8.txt',\n",
       " 'eutropius/eutropius9.txt',\n",
       " 'prudentius/prud.psycho.txt',\n",
       " 'prudentius/prud1.txt',\n",
       " 'prudentius/prud10.txt',\n",
       " 'prudentius/prud11.txt',\n",
       " 'prudentius/prud12.txt',\n",
       " 'prudentius/prud13.txt',\n",
       " 'prudentius/prud14.txt',\n",
       " 'prudentius/prud2.txt',\n",
       " 'prudentius/prud3.txt',\n",
       " 'prudentius/prud4.txt',\n",
       " 'prudentius/prud5.txt',\n",
       " 'prudentius/prud6.txt',\n",
       " 'prudentius/prud7.txt',\n",
       " 'prudentius/prud8.txt',\n",
       " 'prudentius/prud9.txt',\n",
       " 'suetonius/suet.caesar.txt',\n",
       " 'xylander/caesar.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latin_reader = get_corpus_reader('latin_text_latin_library', language='latin')\n",
    "ALL_FILE_IDS = list(latin_reader.fileids())\n",
    "good_files = [file for file in ALL_FILE_IDS\n",
    "              if 'prudentius' in file or\n",
    "              'caesar' in file or\n",
    "              'eutropius' in file]\n",
    "LOG.info('available good files %s', len(good_files))\n",
    "latin_reader._fileids = good_files\n",
    "good_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some unfamiliar entries\n",
    "questionable = ['caesar/alex.txt',\n",
    "                'caesar/hisp.txt',\n",
    "                'prudentius/prud.psycho.txt',\n",
    "                'suetonius/suet.caesar.txt',\n",
    "                'xylander/caesar.txt']\n",
    "for file in questionable:\n",
    "    good_files.remove(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom Scikit-learn Pipeline, and call the CorpusReader `words()` method to process the texts\n",
    "#### The functions used in the pipelines are doctest documented in the `corpus_cleaning` module\n",
    "#### The functions used and their order was developed iteratively by running the pipelines on actual data and carefully inspecting the results prior to runnning it through featurization. Always know your data!\n",
    "\n",
    "#### Lastly, we use the joblib library to save/pickle the pipeline so that it can be reloaded and reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['process_latin_text_pipeline.0.20.2.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_latin_text_pipeline = Pipeline([\n",
    "    ('separate_camel_cases', FunctionTransformer(separate_camel_cases, validate=False)),\n",
    "    ('splice_hyphens', FunctionTransformer(splice_hyphens, validate=False)),\n",
    "    ('jv_transform', FunctionTransformer(jv_transform, validate=False)),\n",
    "    ('aeoe_transform', FunctionTransformer(aeoe_transform, validate=False)),\n",
    "    ('accept_editorial', FunctionTransformer(accept_editorial, validate=False)),\n",
    "    ('drop_enclitics', FunctionTransformer(drop_enclitics, validate=False)),\n",
    "    ('drop_fringe_punctuation', FunctionTransformer(drop_fringe_punctuation, validate=False)),\n",
    "    ('drop_all_punctuation', FunctionTransformer(drop_all_punctuation, validate=False)),\n",
    "    ('drop_non_lower', FunctionTransformer(drop_non_lower, validate=False)),\n",
    "    ('drop_arabic_numeric', FunctionTransformer(drop_arabic_numeric, validate=False)),\n",
    "    ('drop_all_caps', FunctionTransformer(drop_all_caps, validate=False)),\n",
    "    ('divide_separate_words', FunctionTransformer(divide_separate_words, validate=False)),\n",
    "    ('drop_empty_lists', FunctionTransformer(drop_empty_lists, validate=False)),\n",
    "    ('drop_empty_strings', FunctionTransformer(drop_empty_strings, validate=False))])\n",
    "\n",
    "process_latin_text_pipeline_file = 'process_latin_text_pipeline.{}.joblib'.format(\n",
    "    sklearn.__version__)\n",
    "dump(process_latin_text_pipeline, process_latin_text_pipeline_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\n"
     ]
    }
   ],
   "source": [
    "X = process_latin_text_pipeline.fit_transform(tqdm([list(latin_reader.words())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyze the resulting matrix, by profiling the character occurences\n",
    "* Go back and adjust the pipeline as necessary\n",
    "* Turn the output into a distinct set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character distribution profile, total chars: 810414\n",
      "Counter({'e': 93827, 'i': 90246, 'u': 75553, 't': 67540, 'a': 65821, 's': 62825, 'r': 54357, 'n': 49472, 'o': 44779, 'm': 41221, 'c': 31563, 'p': 23317, 'l': 23269, 'd': 20970, 'b': 13064, 'q': 12561, 'g': 8357, 'f': 7372, 'x': 4467, 'h': 4172, 'C': 2521, 'A': 1473, 'I': 1185, 'P': 1119, 'H': 1079, 'S': 1078, 'R': 965, 'G': 770, 'Q': 607, 'M': 601, 'T': 557, 'U': 546, 'N': 529, 'L': 475, 'E': 469, 'D': 459, 'y': 399, 'B': 351, 'F': 211, 'O': 203, 'z': 23, 'Z': 16, 'K': 11, 'k': 10, 'X': 3, 'Å': 1})\n",
      "Number of distinct words in Eutropius/Prudentius/Caesar sample: 25,840\n"
     ]
    }
   ],
   "source": [
    "char_count = profile_chars(X)\n",
    "print('Character distribution profile, total chars:', sum(char_count.values()))\n",
    "print(char_count)\n",
    "distinct_good_latin = distinct_words(X)\n",
    "print(f'Number of distinct words in Eutropius/Prudentius/Caesar sample: {len(distinct_good_latin):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After running this notebook several times, we've decided to load in more training data, which is provide by the notebook:\n",
    "* `boosting_training_data.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nonnullius', 'iubere', 'cantabo', 'conditore', 'intercedendo']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_latin_words = []\n",
    "with open('latin.lemma.forms.txt', 'rt') as reader:\n",
    "    additional_latin_words = reader.read().split('\\n')\n",
    "random.sample(additional_latin_words, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional_latin_words: 175,970\n",
      "distinct_good_latin now: 180,068\n"
     ]
    }
   ],
   "source": [
    "print(f'additional_latin_words: {len(additional_latin_words):,}')\n",
    "distinct_good_latin= list(set(distinct_good_latin) | set(additional_latin_words))\n",
    "print(f'distinct_good_latin now: {len(distinct_good_latin):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the Greek texts of Homer and Plato (two of the most commonly quoted Greek authors)\n",
    "* Preprocess the text\n",
    "* Transliterate into Classical Latin\n",
    "\n",
    "#### We save this pipeline for reuse too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['process_greek_text_pipeline.0.20.2.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perseus_greek = get_corpus_reader(language='greek', corpus_name='greek_text_perseus')\n",
    "plato = [tmp for tmp in perseus_greek.fileids() if 'plato' in tmp]\n",
    "homer = [tmp for tmp in perseus_greek.fileids() if 'homer' in tmp]\n",
    "greek_texts = plato + homer\n",
    "\n",
    "process_greek_pipeline = Pipeline([\n",
    "    ('accept_editorial', FunctionTransformer(accept_editorial, validate=False)),  # problematic\n",
    "    ('romanizer', FunctionTransformer(romanizer_transform, validate=False)),\n",
    "    ('drop_fringe_punctuation', FunctionTransformer(drop_fringe_punctuation, validate=False)),\n",
    "    ('drop_all_punctuation', FunctionTransformer(drop_all_punctuation, validate=False)),\n",
    "    ('drop_arabic_numeric', FunctionTransformer(drop_arabic_numeric, validate=False)),  #ok\n",
    "    ('drop_empty_lists', FunctionTransformer(drop_empty_lists, validate=False)),  # problem?\n",
    "    ('drop_empty_strings', FunctionTransformer(drop_empty_strings, validate=False))  # problem?\n",
    "])\n",
    "\n",
    "process_greek_text_pipeline_file = 'process_greek_text_pipeline.{}.joblib'.format(\n",
    "    sklearn.__version__)\n",
    "dump(process_greek_pipeline, process_greek_text_pipeline_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:34<00:00, 34.30s/it]\n"
     ]
    }
   ],
   "source": [
    "X_greek_transliterated = process_greek_pipeline.fit_transform(tqdm([list(perseus_greek.words(greek_texts))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyze the transliterated Greek examples\n",
    "* Check character profiles for tuning\n",
    "* Create a set distinct words, with and without macrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character distribution profile of transliterated Greek:  Counter({'a': 6947729, 'e': 6728409, 'o': 6340811, 'i': 6283152, 't': 5699742, 'n': 5653401, 's': 4160857, 'h': 3578959, 'u': 2971810, 'p': 2577039, 'k': 2513678, 'ē': 2467247, 'r': 2167104, 'm': 2055580, 'ō': 1964400, 'l': 1932188, 'd': 1624732, 'g': 1175425, 'b': 240982, 'S': 191454, 'x': 175041, 'Ō': 162113, 'E': 148151, 'A': 136839, 'z': 110228, 'T': 78620, 'P': 70134, 'I': 51507, 'R': 47009, 'L': 46766, 'K': 40902, 'X': 40362, 'O': 29058, 'M': 27506, 'D': 27135, 'N': 25385, 'H': 17705, 'Z': 7541, 'Y': 6705, 'Ē': 5036, 'G': 1808, 'B': 1293, 'y': 719, 'F': 434, 'f': 227, 'c': 192, ' ': 120, 'V': 95, 'C': 85, 'v': 81, 'U': 37})\n",
      "48,478 distinct_transliterated_greek_examples\n",
      "47,023 distinct_demacronized_greek\n"
     ]
    }
   ],
   "source": [
    "print('Character distribution profile of transliterated Greek: ', profile_chars(X_greek_transliterated))\n",
    "distinct_transliterated_greek_examples = distinct_words(X_greek_transliterated)\n",
    "print(f'{len(distinct_transliterated_greek_examples):,} distinct_transliterated_greek_examples')\n",
    "distinct_demacronized_greek = distinct_words(demacronize(X_greek_transliterated))\n",
    "print(f'{len(distinct_demacronized_greek):,} distinct_demacronized_greek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how many words from the transliterated Greek words which have also appear in the Latin corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared_words: 490 : {'agoni', 'ted', 'lethe', 'aletes', 'lege', 'mone', 'parentes', 'Melantho', 'dido', 'doto', 'eas', 'heroi', 'aedes', 'meni', 'proi', 'patria', 'is', 'topazo', 'adamas', 'nassa', 'noto', 'mese', 'iambe', 'aera', 'stephano', 'Sparte', 'bolos', 'sto', 'suas', 'age', 'daphnes', 'ale', 'Dione', 'Gorgia', 'peribolo', 'me', 'aer', 'ere', 'Euenos', 'Hermogene', 'se', 'nomisma', 'dolo', 'agrio', 'omen', 'Thebe', 'ano', 'pauet', 'e', 'mete', 'pleto', 'thalamos', 'muri', 'Naxos', 'aure', 'pege', 'melie', 'Atreus', 'dogmata', 'dogma', 'aget', 'meri', 'Argos', 'Eous', 'pe', 'axioma', 'Elpenor', 'aude', 'Minos', 'domos', 'Erote', 'labe', 'Euripide', 'polite', 'Maia', 'med', 'phono', 'metri', 'Anaxagoras', 'Parmenide', 'dogmati', 'ara', 'mere', 'no', 'hesperos', 'pale', 'oleto', 'ago', 'thalamo', 'hostis', 'depote', 'Epigenes', 'Nestor', 'Letoi', 'est', 'mori', 'rhetori', 'leges', 'dis', 'Orion', 'Miletos', 'psephismata', 'Orpheus', 'bibas', 'Theodore', 'strato', 'Helene', 'pono', 'aleis', 'duo', 'pio', 'statera', 'r', 'saturo', 'Herme', 'Polites', 'domo', 'meis', 'leget', 'opta', 'throno', 'Naxo', 'antas', 'etis', 'alueis', 'pares', 'heros', 'heroos', 'Aristophane', 'hebe', 'potho', 'an', 'lotos', 'agnos', 'daemones', 'toros', 'zeta', 'Delio', 'agonia', 'aeto', 'polles', 'Asia', 'metis', 'elate', 'nota', 'o', 'es', 'porro', 'Leto', 'horas', 'polos', 'metalla', 'pharetras', 'Leontinos', 'heroas', 'prophetas', 'dolos', 'emat', 'parergo', 'Eo', 'Aristodemo', 'Euripo', 'osi', 'lino', 'die', 'potero', 'elephantos', 'Hesiodo', 'ergo', 'pedalio', 'morio', 'melle', 'Paros', 'periodo', 'Athenas', 'Pausania', 'tria', 'par', 'Laertiade', 'Dardanide', 'Lesbo', 'daphne', 'patri', 'lu', 'ex', 'philosophos', 'hos', 'Arete', 'Hippotades', 'aniso', 'glossas', 'manenti', 'apate', 'agre', 'ede', 'pro', 'phantasma', 'Aristotele', 'dos', 'Pergama', 'Melite', 'polle', 'phthisis', 'per', 'Gai', 'aspis', 'torno', 'Thebes', 'pedo', 'ero', 'ese', 'Plato', 'mones', 'ambrosie', 'eita', 'ant', 'stes', 'liga', 'Lede', 'ae', 'Theodoro', 'limo', 'pater', 'esse', 'pedalia', 'logo', 'penes', 'Parthenio', 'suos', 'eant', 'philo', 'Protagora', 'pie', 'mero', 'philosophia', 'metabole', 'Troas', 'has', 'bio', 'Menelao', 'eis', 'aule', 'geme', 'melo', 'anathemata', 'tris', 'Same', 'assa', 'do', 'ante', 'mede', 'patris', 'nome', 'siderea', 'philosopho', 'Troia', 'ego', 'limen', 'det', 'tropo', 'lis', 'ne', 'historia', 'et', 'alto', 'protero', 'pero', 'diae', 'Melete', 'Xanthippe', 'sape', 'Europes', 'Cypria', 'Apolloni', 'ites', 'te', 'plana', 'Diomedeos', 'deest', 'Lemnos', 'mi', 'agones', 'erato', 'Theseus', 'Atrei', 'bie', 'Neritos', 'sin', 'luto', 'Delos', 'geometre', 'Zenoni', 'problemata', 'eris', 'Hermes', 'di', 'eo', 'lebes', 'topo', 'oles', 'homo', 'doma', 'petas', 'io', 'ore', 'elephanti', 'pote', 'Phalereus', 'deos', 'pare', 'antro', 'hamos', 'hei', 'Admete', 'Eros', 'rhetores', 'opi', 'halo', 'erosa', 'auge', 'dei', 'mela', 'ose', 'osse', 'times', 'aloes', 'geometria', 'problema', 'iste', 'orse', 'Diones', 'lupe', 'emes', 'Lampetie', 'hebes', 'molis', 'prophetis', 'tende', 'ales', 'ambrosio', 'heroa', 'epistate', 'temo', 'nautas', 'heroes', 'ei', 'gemet', 'at', 'Are', 'deis', 'Orestes', 'aloe', 'oro', 'hoste', 'erga', 'Europe', 'pontones', 'dein', 'Laertes', 'os', 'phantasmata', 'tot', 'Rhodope', 'ito', 'idea', 'geode', 'Delo', 'Minoa', 'sito', 'sei', 'laro', 'peren', 'messe', 'Ares', 'phantasia', 'Hos', 'lae', 'duas', 'erat', 'diametro', 'eos', 'rhetor', 'Apollo', 'Triptolemo', 'ipsa', 'spei', 'Selene', 'poesi', 'sus', 'aristas', 'Eos', 'Homero', 'hippo', 'mala', 'labes', 'poma', 'Mimas', 'in', 'Protagoras', 'edos', 'orto', 'tauros', 'exe', 'stantes', 'deo', 'lita', 'Tantalo', 'dote', 'Gorgias', 'psoras', 'neo', 'Aretes', 'probata', 'sidereos', 'Delon', 'olet', 'paries', 'metreta', 'dele', 'dromo', 'hora', 'mel', 'pedali', 'id', 'Perses', 'athletas', 'a', 'Agamemnonides', 'eri', 'diastemata', 'en', 'thalle', 'eras', 'xanthe', 'horai', 'polo', 'iota', 'demo', 'Persephone', 'emata', 'astra', 'dies', 'Pheres', 'de', 'Eoi', 'isto', 'Mida', 'teste', 'admete', 'elateri', 'genus', 'pompas', 'organo', 'non', 'aut', 'anterotas', 'trite', 'enen', 'auges', 'metere', 'aetas', 'Eroti', 'ea', 'Lapithas', 'beta', 'esto', 'metro', 'anathema', 'pales', 'ponto', 'Thebas', 'sues', 'theatro', 'lexeos', 'exerat', 'lego', 'Troes', 'notio', 'telamon', 'ereto', 'nae', 'asto', 'Amphitrite', 'Stesichorus', 'emo', 'domat', 'pino', 'seu', 'eges', 'Euripides'}\n"
     ]
    }
   ],
   "source": [
    "shared_words = distinct_demacronized_greek & set(distinct_good_latin)\n",
    "print(f'Shared_words: {len(shared_words)} : {shared_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These shared words appear in both language corpora; however, intuitively, we know each word will have a different probability of occurrence in each language. So, rather than arbitrarily excluding some or all of the words from one language or the other, we should split them into the most common probable groups. We can do this by loading the probability distribution pickle objects we have created in the notebooks:\n",
    "* `building_language_model/make_frequency_distribution.ipynb` \n",
    "* `detecting_loanwords/make_frequency_distribution_greek_transliterated.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Frequency Distributions for Latin and transliterated Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_transliterated_word_probs = {}\n",
    "with open('freq_dist.greek.transliterated.pkl', 'rb') as reader:\n",
    "    greek_transliterated_word_probs = pickle.load(reader)\n",
    "    \n",
    "latin_word_probs = {}\n",
    "with open(os.path.join('../building_language_model', 'freq_dist.latin.pkl'), 'rb') as reader:\n",
    "    latin_word_probs = pickle.load(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll create a list of tuples containing (the word, the words probability in Latin, the words probability in Greek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('et', 0.9999900000000002, 0.001594731640385874)\n",
      "('in', 0.6195366094371269, 0.002322993118442995)\n",
      "('est', 0.40047688283934835, 0.0015525135836869104)\n",
      "('non', 0.3655202190736462, 1.0000000000000006e-06)\n",
      "('de', 0.16358695886651264, 0.5190825616279315)\n",
      "('a', 0.13448064535122392, 0.002914045912228485)\n",
      "('ex', 0.13319985495959372, 0.02580678715724147)\n",
      "('per', 0.11551978686121114, 0.0049721761763029575)\n",
      "('esse', 0.11489112051513624, 1e-06)\n",
      "('se', 0.10337573584020457, 0.019231324826377898)\n",
      "('aut', 0.09289014417246275, 0.0011619965592214974)\n",
      "('me', 0.06858796751076134, 0.016392160513372597)\n",
      "('te', 0.068280671647568, 0.1623927550925633)\n",
      "('id', 0.059277606586987885, 0.00039151702446541285)\n",
      "('ne', 0.05901722612275537, 2.2109028349481774e-05)\n",
      "('eo', 0.05369232834070303, 1.1554514174740887e-05)\n",
      "('pro', 0.051006421750196464, 0.00823352105629789)\n",
      "('ea', 0.04345773405740157, 0.0006131618221349714)\n",
      "('erat', 0.03941362666697944, 1e-06)\n",
      "('ei', 0.034644676362612745, 0.07388259922318621)\n",
      "('ego', 0.0335984629657866, 1e-06)\n",
      "('eos', 0.03038241236702284, 1e-06)\n",
      "('ante', 0.030340188507958103, 0.00012765417009689064)\n",
      "('ergo', 0.029326815890404534, 1e-06)\n",
      "('an', 0.027738729635580994, 0.10501841603867183)\n",
      "('ipsa', 0.021074397213197437, 1.1554514174740887e-05)\n",
      "('die', 0.019035923128350087, 8.54361133979271e-05)\n",
      "('eis', 0.018604301457910602, 0.08208345673695988)\n",
      "('dies', 0.015604061694366579, 1e-06)\n",
      "('is', 0.015097375385589792, 0.0012464326726194246)\n",
      "('e', 0.014745509893383692, 0.0011619965592214974)\n",
      "('pater', 0.01401128389964696, 0.0017002767821332827)\n",
      "('homo', 0.012092444082149687, 2.2109028349481774e-05)\n",
      "('suos', 0.010846840239740087, 9.5990627572668e-05)\n",
      "('es', 0.010710785582753728, 0.1048812073544002)\n",
      "('at', 0.010438676268781009, 0.0005287257087370443)\n",
      "('duo', 0.009596544857434407, 0.010365532919595551)\n",
      "('genus', 0.009535554838785348, 1.1554514174740887e-05)\n",
      "('patris', 0.00863712494868577, 0.00028597188271800393)\n",
      "('seu', 0.008449463352842516, 0.00016987222679585418)\n",
      "('lege', 0.00787240394562451, 0.0016580587254343192)\n",
      "('tot', 0.00781141392697545, 0.002069684778249214)\n",
      "('dei', 0.007619060791236115, 0.012793071179785955)\n",
      "('eas', 0.007398558416120292, 0.00010654514174740887)\n",
      "('o', 0.00659395932394234, 0.0006131618221349714)\n",
      "('suas', 0.006291355000645093, 5.3772570873704435e-05)\n",
      "('deos', 0.006263205761268605, 0.001109223988347793)\n",
      "('ore', 0.005735407522959452, 1e-06)\n",
      "('mala', 0.005711949823479046, 0.00654479878833935)\n",
      "('deo', 0.005665034424518232, 1e-06)\n",
      "('iste', 0.005374158950961189, 0.001014233360775125)\n",
      "('meis', 0.0050715546276639415, 2.2109028349481774e-05)\n",
      "('hos', 0.004811174163431426, 0.022239361366179048)\n",
      "('domo', 0.004691539896081352, 1e-06)\n",
      "('leges', 0.0043537490235634945, 1e-06)\n",
      "('mi', 0.004102751639123142, 1.1554514174740887e-05)\n",
      "('patri', 0.003940893512708336, 0.0019958031790260275)\n",
      "('duas', 0.00381656770546218, 6.432708504844532e-05)\n",
      "('dolo', 0.0035163091521129737, 1e-06)\n",
      "('has', 0.0033943291148148583, 0.0036211983619361243)\n",
      "('hostis', 0.003326301786321679, 0.004433895953391172)\n",
      "('isto', 0.00287825972624591, 1e-06)\n",
      "('porro', 0.0028243070174409743, 1e-06)\n",
      "('par', 0.002765662768739957, 0.016128297659004077)\n",
      "('tria', 0.002758625458895835, 0.0014680774702889832)\n",
      "('patria', 0.0027492423791036728, 0.0003176354252422266)\n",
      "('sin', 0.002629608111753598, 3.266354252422266e-05)\n",
      "('mori', 0.0026084961822212317, 1.0000000000000006e-06)\n",
      "('aetas', 0.0025662723231565, 1e-06)\n",
      "('erga', 0.002556889243364337, 0.00464498623688599)\n",
      "('os', 0.002521702694143727, 0.00010654514174740887)\n",
      "('hora', 0.0024912076848191984, 0.001584177126211133)\n",
      "('di', 0.0024137972765338562, 0.014207376079201233)\n",
      "('nota', 0.0024067599666897335, 1e-06)\n",
      "('parentes', 0.0021862575915739106, 2.2109028349481774e-05)\n",
      "('esto', 0.001998595995730656, 1e-06)\n",
      "('hoste', 0.0018461209491080122, 5.3772570873704435e-05)\n",
      "('domos', 0.0018132801698354428, 0.00012765417009689064)\n",
      "('penes', 0.0017053747522255717, 1e-06)\n",
      "('eris', 0.0016021608745117818, 0.00018042674097059507)\n",
      "('dein', 0.001588086254823538, 0.0036211983619361243)\n",
      "('alto', 0.001550553935654887, 5.3772570873704435e-05)\n",
      "('dis', 0.001522404696278399, 0.001404750385240538)\n",
      "('aedes', 0.0014801808372136667, 1e-06)\n",
      "('en', 0.0014403027480969753, 0.1989535921938657)\n",
      "('ero', 0.0014004246589802837, 1e-06)\n",
      "('Eo', 0.0013417804102792668, 1e-06)\n",
      "('Asia', 0.0013159769408508193, 0.0007925885631055665)\n",
      "('age', 0.0012479496123576398, 0.0016052861545606148)\n",
      "('pares', 0.0012479496123576398, 1.1554514174740887e-05)\n",
      "('ant', 0.0011728849740203382, 0.001014233360775125)\n",
      "('aer', 0.0011588103543320942, 1e-06)\n",
      "('det', 0.0011588103543320942, 1e-06)\n",
      "('aera', 0.001130661114955606, 0.0003070809110674857)\n",
      "('astra', 0.0011025118755791177, 0.00016987222679585418)\n",
      "('dote', 0.001013372617553572, 0.00021209028349481775)\n",
      "('spei', 0.0009570741388005958, 1e-06)\n",
      "('Plato', 0.000912504509787823, 2.2109028349481774e-05)\n",
      "('oro', 0.0008890468103074162, 1e-06)\n",
      "('philosophia', 0.0008843552704113349, 0.0003492989677664493)\n",
      "('potero', 0.0008820095004632942, 1e-06)\n",
      "('Hos', 0.0008608975709309282, 9.5990627572668e-05)\n",
      "('historia', 0.0008421314113466028, 0.0001487631984463724)\n",
      "('daemones', 0.0008280567916583586, 1e-06)\n",
      "('Apollo', 0.0008045990921779519, 1e-06)\n",
      "('dos', 0.0007717583129053824, 0.0005181711945623034)\n",
      "('eras', 0.0007576836932171383, 0.00011709965592214976)\n",
      "('ago', 0.000745954843476935, 1.0000000000000006e-06)\n",
      "('muri', 0.0007107682942563249, 6.432708504844532e-05)\n",
      "('teste', 0.0006943479046200401, 1e-06)\n",
      "('idea', 0.0006615071253474707, 0.0002437538260190404)\n",
      "('deest', 0.0006521240455553079, 1e-06)\n",
      "('horas', 0.0006450867357111859, 0.0016475042112595783)\n",
      "('do', 0.0006192832662827385, 0.0001487631984463724)\n",
      "('deis', 0.0005958255668023318, 1.1554514174740887e-05)\n",
      "('r', 0.0005747136372699656, 0.0001593177126211133)\n",
      "('poma', 0.0005606390175817216, 1.1554514174740887e-05)\n",
      "('pio', 0.0005536017077375996, 1e-06)\n",
      "('lis', 0.000551255937789559, 2.2109028349481774e-05)\n",
      "('med', 0.0005418728579973961, 1e-06)\n",
      "('omen', 0.0005348355481532742, 1e-06)\n",
      "('limen', 0.0005277982383091521, 2.2109028349481774e-05)\n",
      "('lego', 0.0005207609284650301, 1e-06)\n",
      "('Athenas', 0.0005184151585169894, 1e-06)\n",
      "('mero', 0.0005184151585169894, 1e-06)\n",
      "('melle', 0.0005160693885689487, 1.0000000000000006e-06)\n",
      "('prophetas', 0.0005019947688807047, 1e-06)\n",
      "('tris', 0.0004902659191405013, 0.0007503705064066029)\n",
      "('ara', 0.0004879201491924607, 0.017859237983661583)\n",
      "('ponto', 0.0004644624497120539, 1e-06)\n",
      "('Troia', 0.0004550793699198912, 0.00020153576932007686)\n",
      "('probata', 0.0004480420600757691, 0.000655379878833935)\n",
      "('luto', 0.00043631321033556577, 9.5990627572668e-05)\n",
      "('heros', 0.0004292759004914438, 1e-06)\n",
      "('philosophos', 0.0004245843605953624, 0.0002648628543685222)\n",
      "('mel', 0.0004245843605953624, 1e-06)\n",
      "('prophetis', 0.0004222385906473217, 1e-06)\n",
      "('theatro', 0.0004198928206992811, 1e-06)\n",
      "('times', 0.000412855510855159, 1.0000000000000006e-06)\n",
      "('ales', 0.000412855510855159, 1.0000000000000006e-06)\n",
      "('plana', 0.0004081639709590777, 0.00010654514174740887)\n",
      "('Eos', 0.0004034724310629963, 1e-06)\n",
      "('aure', 0.00040112666111495566, 1e-06)\n",
      "('pie', 0.0003847062714786709, 1.1554514174740887e-05)\n",
      "('opi', 0.0003847062714786709, 0.00012765417009689064)\n",
      "('antro', 0.00038001473158258956, 1e-06)\n",
      "('dolos', 0.00037297742173846753, 7.488159922318621e-05)\n",
      "('Thebas', 0.0003448281823619794, 1e-06)\n",
      "('polo', 0.000340136642465898, 1e-06)\n",
      "('thalamos', 0.00033075356267373533, 6.432708504844532e-05)\n",
      "('Argos', 0.000326062022777654, 0.0010775604458235703)\n",
      "('limo', 0.0003143331730374506, 1e-06)\n",
      "('stantes', 0.0003072958631933286, 1.1554514174740887e-05)\n",
      "('aget', 0.00029087547355704385, 9.5990627572668e-05)\n",
      "('philosopho', 0.0002791466238168405, 1e-06)\n",
      "('elephanti', 0.0002744550839207592, 0.00010654514174740887)\n",
      "('tauros', 0.0002744550839207592, 0.00020153576932007686)\n",
      "('orto', 0.00027210931397271846, 1e-06)\n",
      "('elephantos', 0.00026976354402467775, 0.0005287257087370443)\n",
      "('paries', 0.0002580346942844744, 1e-06)\n",
      "('toros', 0.00024630584454427105, 1.1554514174740887e-05)\n",
      "('labe', 0.0002439600745962303, 0.0003492989677664493)\n",
      "('agnos', 0.0002439600745962303, 1.0000000000000006e-06)\n",
      "('Homero', 0.00023926853470014893, 1e-06)\n",
      "('ae', 0.00023223122485602693, 3.266354252422266e-05)\n",
      "('metalla', 0.0002251939150119049, 0.00010654514174740887)\n",
      "('thalamo', 0.00022050237511582355, 1e-06)\n",
      "('io', 0.00021815660516778286, 1.0000000000000006e-06)\n",
      "('molis', 0.00021346506527170154, 0.0017108312963080236)\n",
      "('Theseus', 0.0002087735253756202, 1e-06)\n",
      "('pote', 0.00020408198547953882, 0.012581980896291136)\n",
      "('Aristotele', 0.00019939044558345747, 1e-06)\n",
      "('nautas', 0.00018297005594717272, 0.0001487631984463724)\n",
      "('anathema', 0.00017827851605109138, 3.266354252422266e-05)\n",
      "('dogma', 0.00017358697615501003, 0.0003176354252422266)\n",
      "('Minos', 0.00017358697615501003, 1e-06)\n",
      "('dogmata', 0.00017124120620696934, 4.321805669896355e-05)\n",
      "('Orpheus', 0.00017124120620696934, 0.0001593177126211133)\n",
      "('throno', 0.00016889543625892866, 1e-06)\n",
      "('Pergama', 0.00016889543625892866, 1.0000000000000006e-06)\n",
      "('sus', 0.00016889543625892866, 0.00012765417009689064)\n",
      "('labes', 0.00016889543625892866, 1e-06)\n",
      "('sues', 0.00016889543625892866, 0.00010654514174740887)\n",
      "('notio', 0.00016889543625892866, 1e-06)\n",
      "('ere', 0.0001642038963628473, 1.0000000000000006e-06)\n",
      "('axioma', 0.0001642038963628473, 1e-06)\n",
      "('phantasma', 0.00016185812641480665, 8.54361133979271e-05)\n",
      "('petas', 0.00016185812641480665, 1.0000000000000006e-06)\n",
      "('Orestes', 0.00016185812641480665, 1e-06)\n",
      "('no', 0.0001571665865187253, 0.0003598534819411902)\n",
      "('eant', 0.0001571665865187253, 1.1554514174740887e-05)\n",
      "('noto', 0.00015482081657068462, 1e-06)\n",
      "('pauet', 0.00015012927667460324, 1.0000000000000006e-06)\n",
      "('hebes', 0.00015012927667460324, 1e-06)\n",
      "('emat', 0.0001454377367785219, 1e-06)\n",
      "('Orion', 0.0001384004269343999, 1e-06)\n",
      "('rhetor', 0.00013605465698635923, 1e-06)\n",
      "('leget', 0.00013370888703831852, 1.0000000000000006e-06)\n",
      "('metro', 0.00013370888703831852, 1e-06)\n",
      "('pono', 0.00012901734714223717, 1.0000000000000006e-06)\n",
      "('olet', 0.0001266715771941965, 1.0000000000000006e-06)\n",
      "('lino', 0.00012432580724615583, 1e-06)\n",
      "('mones', 0.00012198003729811518, 1e-06)\n",
      "('domat', 0.00012198003729811518, 1e-06)\n",
      "('Delo', 0.00011494272745399313, 1e-06)\n",
      "('pharetras', 0.00011259695750595246, 2.2109028349481774e-05)\n",
      "('Gai', 0.00011259695750595246, 3.266354252422266e-05)\n",
      "('geometria', 0.00011259695750595246, 1e-06)\n",
      "('Perses', 0.00011025118755791177, 1e-06)\n",
      "('ted', 0.0001079054176098711, 1e-06)\n",
      "('strato', 0.0001079054176098711, 1e-06)\n",
      "('ede', 0.00010555964766183044, 1e-06)\n",
      "('Rhodope', 0.00010555964766183044, 1e-06)\n",
      "('Dione', 9.852233781770841e-05, 1e-06)\n",
      "('aristas', 9.852233781770841e-05, 6.432708504844532e-05)\n",
      "('eri', 9.383079792162705e-05, 1e-06)\n",
      "('meri', 9.148502797358637e-05, 1.0000000000000006e-06)\n",
      "('mela', 9.148502797358637e-05, 1e-06)\n",
      "('Euripides', 9.148502797358637e-05, 1.1554514174740887e-05)\n",
      "('Anaxagoras', 8.91392580255457e-05, 5.3772570873704435e-05)\n",
      "('polos', 8.91392580255457e-05, 1e-06)\n",
      "('proi', 8.679348807750503e-05, 1e-06)\n",
      "('bibas', 8.679348807750503e-05, 3.266354252422266e-05)\n",
      "('Maia', 8.444771812946434e-05, 7.488159922318621e-05)\n",
      "('rhetores', 8.444771812946434e-05, 1e-06)\n",
      "('phantasmata', 8.444771812946434e-05, 4.321805669896355e-05)\n",
      "('statera', 8.210194818142367e-05, 1e-06)\n",
      "('Atreus', 7.9756178233383e-05, 6.432708504844532e-05)\n",
      "('Hermes', 7.9756178233383e-05, 1.1554514174740887e-05)\n",
      "('messe', 7.9756178233383e-05, 1e-06)\n",
      "('phantasia', 7.9756178233383e-05, 2.2109028349481774e-05)\n",
      "('metis', 7.741040828534232e-05, 1.0000000000000006e-06)\n",
      "('Nestor', 7.506463833730164e-05, 3.266354252422266e-05)\n",
      "('Helene', 7.506463833730164e-05, 1e-06)\n",
      "('Gorgias', 7.506463833730164e-05, 7.488159922318621e-05)\n",
      "('organo', 7.271886838926096e-05, 1e-06)\n",
      "('Troes', 7.037309844122029e-05, 1e-06)\n",
      "('Delos', 6.802732849317962e-05, 1e-06)\n",
      "('sto', 6.568155854513893e-05, 1e-06)\n",
      "('aude', 6.333578859709826e-05, 1e-06)\n",
      "('Theodoro', 6.333578859709826e-05, 1e-06)\n",
      "('Europe', 6.333578859709826e-05, 1e-06)\n",
      "('hei', 6.099001864905758e-05, 1.0000000000000006e-06)\n",
      "('tende', 6.099001864905758e-05, 1e-06)\n",
      "('emo', 5.86442487010169e-05, 1e-06)\n",
      "('Troas', 5.629847875297623e-05, 1e-06)\n",
      "('Menelao', 5.629847875297623e-05, 1e-06)\n",
      "('Lemnos', 5.629847875297623e-05, 1e-06)\n",
      "('diametro', 5.629847875297623e-05, 1e-06)\n",
      "('pe', 5.395270880493555e-05, 1e-06)\n",
      "('alueis', 5.395270880493555e-05, 1.0000000000000006e-06)\n",
      "('lotos', 5.395270880493555e-05, 1e-06)\n",
      "('osse', 5.395270880493555e-05, 0.0001382086842716315)\n",
      "('Zenoni', 5.1606938856894884e-05, 1e-06)\n",
      "('Phalereus', 4.9261168908854204e-05, 1e-06)\n",
      "('Eoi', 4.9261168908854204e-05, 1e-06)\n",
      "('rhetori', 4.691539896081353e-05, 1e-06)\n",
      "('Hesiodo', 4.691539896081353e-05, 1e-06)\n",
      "('Pausania', 4.691539896081353e-05, 4.321805669896355e-05)\n",
      "('Lesbo', 4.691539896081353e-05, 1e-06)\n",
      "('torno', 4.691539896081353e-05, 1e-06)\n",
      "('eges', 4.691539896081353e-05, 1e-06)\n",
      "('Sparte', 4.456962901277285e-05, 1e-06)\n",
      "('heroas', 4.456962901277285e-05, 1e-06)\n",
      "('Apolloni', 4.456962901277285e-05, 1e-06)\n",
      "('Mida', 4.456962901277285e-05, 2.2109028349481774e-05)\n",
      "('pompas', 4.456962901277285e-05, 0.0001593177126211133)\n",
      "('Eros', 4.222385906473218e-05, 1.0000000000000006e-06)\n",
      "('Naxos', 3.98780891166915e-05, 2.2109028349481774e-05)\n",
      "('metri', 3.98780891166915e-05, 1.0000000000000006e-06)\n",
      "('siderea', 3.98780891166915e-05, 1e-06)\n",
      "('sei', 3.98780891166915e-05, 1.0000000000000006e-06)\n",
      "('athletas', 3.98780891166915e-05, 1e-06)\n",
      "('Gorgia', 3.7532319168650824e-05, 3.266354252422266e-05)\n",
      "('Thebe', 3.7532319168650824e-05, 1e-06)\n",
      "('logo', 3.7532319168650824e-05, 1e-06)\n",
      "('hamos', 3.7532319168650824e-05, 1e-06)\n",
      "('heroes', 3.7532319168650824e-05, 1e-06)\n",
      "('lita', 3.7532319168650824e-05, 1.0000000000000006e-06)\n",
      "('mone', 3.5186549220610144e-05, 1e-06)\n",
      "('Eous', 3.5186549220610144e-05, 1e-06)\n",
      "('antas', 3.5186549220610144e-05, 1.0000000000000006e-06)\n",
      "('Paros', 3.5186549220610144e-05, 1.1554514174740887e-05)\n",
      "('eita', 3.5186549220610144e-05, 0.0027557281996073717)\n",
      "('ito', 3.5186549220610144e-05, 1e-06)\n",
      "('Protagoras', 3.5186549220610144e-05, 1e-06)\n",
      "('dele', 3.5186549220610144e-05, 1e-06)\n",
      "('polite', 3.284077927256947e-05, 1e-06)\n",
      "('lu', 3.284077927256947e-05, 1.1554514174740887e-05)\n",
      "('pare', 3.284077927256947e-05, 1.0000000000000006e-06)\n",
      "('sidereos', 3.284077927256947e-05, 1e-06)\n",
      "('Persephone', 3.284077927256947e-05, 1e-06)\n",
      "('adamas', 3.0495009324528788e-05, 1.0000000000000006e-06)\n",
      "('manenti', 3.0495009324528788e-05, 1.1554514174740887e-05)\n",
      "('aspis', 3.0495009324528788e-05, 0.00018042674097059507)\n",
      "('Parthenio', 3.0495009324528788e-05, 1e-06)\n",
      "('tropo', 3.0495009324528788e-05, 1e-06)\n",
      "('Atrei', 3.0495009324528788e-05, 1.1554514174740887e-05)\n",
      "('doma', 3.0495009324528788e-05, 1.0000000000000006e-06)\n",
      "('metere', 3.0495009324528788e-05, 1e-06)\n",
      "('opta', 2.814923937648811e-05, 2.2109028349481774e-05)\n",
      "('Leontinos', 2.814923937648811e-05, 2.2109028349481774e-05)\n",
      "('diae', 2.814923937648811e-05, 1e-06)\n",
      "('Minoa', 2.814923937648811e-05, 1e-06)\n",
      "('Lapithas', 2.814923937648811e-05, 1.0000000000000006e-06)\n",
      "('asto', 2.814923937648811e-05, 1e-06)\n",
      "('ano', 2.5803469428447435e-05, 1e-06)\n",
      "('mere', 2.5803469428447435e-05, 1.1554514174740887e-05)\n",
      "('stes', 2.5803469428447435e-05, 1e-06)\n",
      "('Cypria', 2.5803469428447435e-05, 1.0000000000000006e-06)\n",
      "('auge', 2.5803469428447435e-05, 1e-06)\n",
      "('problema', 2.5803469428447435e-05, 1e-06)\n",
      "('Diones', 2.5803469428447435e-05, 1e-06)\n",
      "('nomisma', 2.3457699480406762e-05, 0.0002437538260190404)\n",
      "('Herme', 2.3457699480406762e-05, 1e-06)\n",
      "('Naxo', 2.3457699480406762e-05, 1e-06)\n",
      "('Thebes', 2.3457699480406762e-05, 1e-06)\n",
      "('liga', 2.3457699480406762e-05, 2.2109028349481774e-05)\n",
      "('temo', 2.3457699480406762e-05, 1e-06)\n",
      "('pedali', 2.3457699480406762e-05, 1e-06)\n",
      "('beta', 2.3457699480406762e-05, 1e-06)\n",
      "('elate', 2.1111929532366085e-05, 1e-06)\n",
      "('Arete', 2.1111929532366085e-05, 1e-06)\n",
      "('problemata', 2.1111929532366085e-05, 1e-06)\n",
      "('topo', 2.1111929532366085e-05, 1e-06)\n",
      "('Tantalo', 2.1111929532366085e-05, 1e-06)\n",
      "('iota', 2.1111929532366085e-05, 1e-06)\n",
      "('nae', 2.1111929532366085e-05, 1e-06)\n",
      "('heroi', 1.876615958432541e-05, 1e-06)\n",
      "('Euripo', 1.876615958432541e-05, 1e-06)\n",
      "('gemet', 1.876615958432541e-05, 1e-06)\n",
      "('Mimas', 1.876615958432541e-05, 1.0000000000000006e-06)\n",
      "('auges', 1.876615958432541e-05, 1e-06)\n",
      "('Hermogene', 1.6420389636284732e-05, 1e-06)\n",
      "('Euripide', 1.6420389636284732e-05, 1e-06)\n",
      "('dogmati', 1.6420389636284732e-05, 1.1554514174740887e-05)\n",
      "('Epigenes', 1.6420389636284732e-05, 1.0000000000000006e-06)\n",
      "('Miletos', 1.6420389636284732e-05, 1e-06)\n",
      "('periodo', 1.6420389636284732e-05, 1e-06)\n",
      "('Melite', 1.6420389636284732e-05, 1e-06)\n",
      "('geme', 1.6420389636284732e-05, 1e-06)\n",
      "('Same', 1.6420389636284732e-05, 1e-06)\n",
      "('assa', 1.6420389636284732e-05, 1.0000000000000006e-06)\n",
      "('mede', 1.6420389636284732e-05, 1e-06)\n",
      "('sape', 1.6420389636284732e-05, 1e-06)\n",
      "('agones', 1.6420389636284732e-05, 1e-06)\n",
      "('emes', 1.6420389636284732e-05, 1e-06)\n",
      "('heroa', 1.6420389636284732e-05, 1e-06)\n",
      "('sito', 1.6420389636284732e-05, 1e-06)\n",
      "('demo', 1.6420389636284732e-05, 1e-06)\n",
      "('Amphitrite', 1.6420389636284732e-05, 1e-06)\n",
      "('saturo', 1.4074619688244056e-05, 1e-06)\n",
      "('Aristophane', 1.4074619688244056e-05, 1e-06)\n",
      "('Delio', 1.4074619688244056e-05, 1e-06)\n",
      "('Leto', 1.4074619688244056e-05, 1e-06)\n",
      "('ese', 1.4074619688244056e-05, 1e-06)\n",
      "('Protagora', 1.4074619688244056e-05, 1e-06)\n",
      "('Xanthippe', 1.4074619688244056e-05, 1e-06)\n",
      "('aloes', 1.4074619688244056e-05, 1e-06)\n",
      "('Triptolemo', 1.4074619688244056e-05, 1e-06)\n",
      "('neo', 1.4074619688244056e-05, 1e-06)\n",
      "('heroos', 1.172884974020338e-05, 1e-06)\n",
      "('polles', 1.172884974020338e-05, 1e-06)\n",
      "('morio', 1.172884974020338e-05, 1e-06)\n",
      "('Lede', 1.172884974020338e-05, 1e-06)\n",
      "('philo', 1.172884974020338e-05, 1e-06)\n",
      "('protero', 1.172884974020338e-05, 1e-06)\n",
      "('aloe', 1.172884974020338e-05, 1e-06)\n",
      "('Laertes', 1.172884974020338e-05, 1e-06)\n",
      "('Selene', 1.172884974020338e-05, 1e-06)\n",
      "('exerat', 1.172884974020338e-05, 1e-06)\n",
      "('Melantho', 9.383079792162704e-06, 1e-06)\n",
      "('ale', 9.383079792162704e-06, 1e-06)\n",
      "('mete', 9.383079792162704e-06, 1e-06)\n",
      "('aleis', 9.383079792162704e-06, 1.0000000000000006e-06)\n",
      "('Hippotades', 9.383079792162704e-06, 1e-06)\n",
      "('glossas', 9.383079792162704e-06, 1e-06)\n",
      "('oles', 9.383079792162704e-06, 1e-06)\n",
      "('lupe', 9.383079792162704e-06, 1e-06)\n",
      "('ambrosio', 9.383079792162704e-06, 1e-06)\n",
      "('exe', 9.383079792162704e-06, 1e-06)\n",
      "('lexeos', 9.383079792162704e-06, 1e-06)\n",
      "('mese', 7.037309844122028e-06, 1e-06)\n",
      "('pedo', 7.037309844122028e-06, 1e-06)\n",
      "('anathemata', 7.037309844122028e-06, 1e-06)\n",
      "('lebes', 7.037309844122028e-06, 1e-06)\n",
      "('Ares', 7.037309844122028e-06, 1.0000000000000006e-06)\n",
      "('lae', 7.037309844122028e-06, 1.0000000000000006e-06)\n",
      "('poesi', 7.037309844122028e-06, 1e-06)\n",
      "('metreta', 7.037309844122028e-06, 1e-06)\n",
      "('trite', 7.037309844122028e-06, 1e-06)\n",
      "('Erote', 4.691539896081351e-06, 1e-06)\n",
      "('psephismata', 4.691539896081351e-06, 1e-06)\n",
      "('Theodore', 4.691539896081351e-06, 1e-06)\n",
      "('Polites', 4.691539896081351e-06, 1e-06)\n",
      "('etis', 4.691539896081351e-06, 1.0000000000000006e-06)\n",
      "('agonia', 4.691539896081351e-06, 1e-06)\n",
      "('Laertiade', 4.691539896081351e-06, 1e-06)\n",
      "('bio', 4.691539896081351e-06, 1e-06)\n",
      "('aule', 4.691539896081351e-06, 1e-06)\n",
      "('Melete', 4.691539896081351e-06, 1e-06)\n",
      "('bie', 4.691539896081351e-06, 1e-06)\n",
      "('Admete', 4.691539896081351e-06, 1e-06)\n",
      "('erosa', 4.691539896081351e-06, 1e-06)\n",
      "('Aretes', 4.691539896081351e-06, 1e-06)\n",
      "('diastemata', 4.691539896081351e-06, 1e-06)\n",
      "('Pheres', 4.691539896081351e-06, 1e-06)\n",
      "('Eroti', 4.691539896081351e-06, 1e-06)\n",
      "('topazo', 2.345769948040676e-06, 1e-06)\n",
      "('nassa', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('stephano', 2.345769948040676e-06, 1e-06)\n",
      "('bolos', 2.345769948040676e-06, 1e-06)\n",
      "('peribolo', 2.345769948040676e-06, 1e-06)\n",
      "('Euenos', 2.345769948040676e-06, 1e-06)\n",
      "('Elpenor', 2.345769948040676e-06, 1e-06)\n",
      "('Parmenide', 2.345769948040676e-06, 1e-06)\n",
      "('pale', 2.345769948040676e-06, 1e-06)\n",
      "('oleto', 2.345769948040676e-06, 1e-06)\n",
      "('parergo', 2.345769948040676e-06, 1e-06)\n",
      "('Aristodemo', 2.345769948040676e-06, 1e-06)\n",
      "('Dardanide', 2.345769948040676e-06, 1e-06)\n",
      "('phthisis', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('pedalia', 2.345769948040676e-06, 1e-06)\n",
      "('metabole', 2.345769948040676e-06, 1e-06)\n",
      "('melo', 2.345769948040676e-06, 1e-06)\n",
      "('pero', 2.345769948040676e-06, 1e-06)\n",
      "('Europes', 2.345769948040676e-06, 1e-06)\n",
      "('Diomedeos', 2.345769948040676e-06, 1e-06)\n",
      "('Neritos', 2.345769948040676e-06, 1e-06)\n",
      "('geometre', 2.345769948040676e-06, 1e-06)\n",
      "('Lampetie', 2.345769948040676e-06, 1e-06)\n",
      "('Are', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('pontones', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('hippo', 2.345769948040676e-06, 1e-06)\n",
      "('Delon', 2.345769948040676e-06, 1e-06)\n",
      "('dromo', 2.345769948040676e-06, 1e-06)\n",
      "('horai', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('Stesichorus', 2.345769948040676e-06, 1.0000000000000006e-06)\n",
      "('pino', 2.345769948040676e-06, 1e-06)\n",
      "('agoni', 1e-06, 1e-06)\n",
      "('lethe', 1e-06, 1e-06)\n",
      "('aletes', 1e-06, 1e-06)\n",
      "('dido', 1e-06, 1e-06)\n",
      "('doto', 1e-06, 1e-06)\n",
      "('meni', 1e-06, 1e-06)\n",
      "('iambe', 1e-06, 1e-06)\n",
      "('daphnes', 1e-06, 1e-06)\n",
      "('agrio', 1e-06, 1e-06)\n",
      "('pleto', 1e-06, 1e-06)\n",
      "('pege', 1e-06, 1e-06)\n",
      "('melie', 1e-06, 1e-06)\n",
      "('phono', 1e-06, 1e-06)\n",
      "('depote', 1e-06, 1e-06)\n",
      "('Letoi', 1e-06, 1e-06)\n",
      "('hebe', 1e-06, 1e-06)\n",
      "('potho', 1e-06, 1e-06)\n",
      "('zeta', 1e-06, 1e-06)\n",
      "('aeto', 1e-06, 1e-06)\n",
      "('osi', 1e-06, 1e-06)\n",
      "('pedalio', 1e-06, 1e-06)\n",
      "('daphne', 1e-06, 1e-06)\n",
      "('aniso', 1e-06, 1e-06)\n",
      "('apate', 1e-06, 1e-06)\n",
      "('agre', 1e-06, 1e-06)\n",
      "('polle', 1e-06, 1e-06)\n",
      "('ambrosie', 1e-06, 1e-06)\n",
      "('nome', 1e-06, 1e-06)\n",
      "('ites', 1e-06, 1e-06)\n",
      "('erato', 1e-06, 1e-06)\n",
      "('halo', 1e-06, 1e-06)\n",
      "('ose', 1e-06, 1e-06)\n",
      "('orse', 1e-06, 1e-06)\n",
      "('epistate', 1e-06, 1e-06)\n",
      "('geode', 1e-06, 1e-06)\n",
      "('laro', 1e-06, 1e-06)\n",
      "('peren', 1e-06, 1e-06)\n",
      "('edos', 1e-06, 1e-06)\n",
      "('psoras', 1e-06, 1e-06)\n",
      "('Agamemnonides', 1e-06, 1e-06)\n",
      "('thalle', 1e-06, 1e-06)\n",
      "('xanthe', 1e-06, 1e-06)\n",
      "('emata', 1e-06, 1e-06)\n",
      "('admete', 1e-06, 1e-06)\n",
      "('elateri', 1e-06, 1e-06)\n",
      "('anterotas', 1e-06, 1e-06)\n",
      "('enen', 1e-06, 1e-06)\n",
      "('pales', 1e-06, 1e-06)\n",
      "('telamon', 1e-06, 1e-06)\n",
      "('ereto', 1e-06, 1e-06)\n",
      "('hesperos', 0.0, 2.2109028349481774e-05)\n"
     ]
    }
   ],
   "source": [
    "shared_latin_greek = [(word, \n",
    "                       latin_word_probs.get(word, 0.000001),\n",
    "                       greek_transliterated_word_probs.get(word, 0.000001)) \n",
    "                      for word in shared_words]\n",
    "\n",
    "shared_latin_greek.sort(key=lambda a: a[1], reverse=True)\n",
    "for item in shared_latin_greek:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kai` is the most common word in the Greek corpus, so we could also divide the shared words by the threshold of this probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011142407253193212"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latin_word_probs.get('kai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['et',\n",
       " 'in',\n",
       " 'est',\n",
       " 'non',\n",
       " 'de',\n",
       " 'a',\n",
       " 'ex',\n",
       " 'per',\n",
       " 'esse',\n",
       " 'se',\n",
       " 'aut',\n",
       " 'me',\n",
       " 'te',\n",
       " 'id',\n",
       " 'ne',\n",
       " 'eo',\n",
       " 'pro',\n",
       " 'ea',\n",
       " 'erat',\n",
       " 'ei',\n",
       " 'ego',\n",
       " 'eos',\n",
       " 'ante',\n",
       " 'ergo',\n",
       " 'an',\n",
       " 'ipsa',\n",
       " 'die',\n",
       " 'eis',\n",
       " 'dies',\n",
       " 'is',\n",
       " 'e',\n",
       " 'pater',\n",
       " 'homo',\n",
       " 'suos',\n",
       " 'es',\n",
       " 'at',\n",
       " 'duo',\n",
       " 'genus',\n",
       " 'patris',\n",
       " 'seu',\n",
       " 'lege',\n",
       " 'tot',\n",
       " 'dei',\n",
       " 'eas',\n",
       " 'o',\n",
       " 'suas',\n",
       " 'deos',\n",
       " 'ore',\n",
       " 'mala',\n",
       " 'deo',\n",
       " 'iste',\n",
       " 'meis',\n",
       " 'hos',\n",
       " 'domo',\n",
       " 'leges',\n",
       " 'mi',\n",
       " 'patri',\n",
       " 'duas',\n",
       " 'dolo',\n",
       " 'has',\n",
       " 'hostis',\n",
       " 'isto',\n",
       " 'porro',\n",
       " 'par',\n",
       " 'tria',\n",
       " 'patria',\n",
       " 'sin',\n",
       " 'mori',\n",
       " 'aetas',\n",
       " 'erga',\n",
       " 'os',\n",
       " 'hora',\n",
       " 'di',\n",
       " 'nota',\n",
       " 'parentes',\n",
       " 'esto',\n",
       " 'hoste',\n",
       " 'domos',\n",
       " 'penes',\n",
       " 'eris',\n",
       " 'dein',\n",
       " 'alto',\n",
       " 'dis',\n",
       " 'aedes',\n",
       " 'en',\n",
       " 'ero',\n",
       " 'Eo',\n",
       " 'Asia',\n",
       " 'age',\n",
       " 'pares',\n",
       " 'ant',\n",
       " 'aer',\n",
       " 'det',\n",
       " 'aera']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likely_latin = [word for word, latin_prob, greek_prob \n",
    "                in shared_latin_greek \n",
    "                if latin_prob >= latin_word_probs['kai']]\n",
    "print(len(likely_latin))\n",
    "likely_latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['et',\n",
       " 'in',\n",
       " 'est',\n",
       " 'non',\n",
       " 'a',\n",
       " 'ex',\n",
       " 'per',\n",
       " 'esse',\n",
       " 'se',\n",
       " 'aut',\n",
       " 'me',\n",
       " 'id',\n",
       " 'ne',\n",
       " 'eo',\n",
       " 'pro',\n",
       " 'ea',\n",
       " 'erat',\n",
       " 'ego',\n",
       " 'eos',\n",
       " 'ante',\n",
       " 'ergo',\n",
       " 'ipsa',\n",
       " 'die',\n",
       " 'dies',\n",
       " 'is',\n",
       " 'e',\n",
       " 'pater',\n",
       " 'homo',\n",
       " 'suos',\n",
       " 'at',\n",
       " 'genus',\n",
       " 'patris',\n",
       " 'seu',\n",
       " 'lege',\n",
       " 'tot',\n",
       " 'eas',\n",
       " 'o',\n",
       " 'suas',\n",
       " 'deos',\n",
       " 'ore',\n",
       " 'deo',\n",
       " 'iste',\n",
       " 'meis',\n",
       " 'domo',\n",
       " 'leges',\n",
       " 'mi',\n",
       " 'patri',\n",
       " 'duas',\n",
       " 'dolo',\n",
       " 'isto',\n",
       " 'porro',\n",
       " 'tria',\n",
       " 'patria',\n",
       " 'sin',\n",
       " 'mori',\n",
       " 'aetas',\n",
       " 'os',\n",
       " 'hora',\n",
       " 'nota',\n",
       " 'parentes',\n",
       " 'esto',\n",
       " 'hoste',\n",
       " 'domos',\n",
       " 'penes',\n",
       " 'eris',\n",
       " 'alto',\n",
       " 'dis',\n",
       " 'aedes',\n",
       " 'ero',\n",
       " 'Eo',\n",
       " 'Asia',\n",
       " 'pares',\n",
       " 'ant',\n",
       " 'aer',\n",
       " 'det',\n",
       " 'aera',\n",
       " 'astra',\n",
       " 'dote',\n",
       " 'spei',\n",
       " 'Plato',\n",
       " 'oro',\n",
       " 'philosophia',\n",
       " 'potero',\n",
       " 'Hos',\n",
       " 'historia',\n",
       " 'daemones',\n",
       " 'Apollo',\n",
       " 'dos',\n",
       " 'eras',\n",
       " 'ago',\n",
       " 'muri',\n",
       " 'teste',\n",
       " 'idea',\n",
       " 'deest',\n",
       " 'do',\n",
       " 'deis',\n",
       " 'r',\n",
       " 'poma',\n",
       " 'pio',\n",
       " 'lis',\n",
       " 'med',\n",
       " 'omen',\n",
       " 'limen',\n",
       " 'lego',\n",
       " 'Athenas',\n",
       " 'mero',\n",
       " 'melle',\n",
       " 'prophetas',\n",
       " 'ponto',\n",
       " 'Troia',\n",
       " 'luto',\n",
       " 'heros',\n",
       " 'philosophos',\n",
       " 'mel',\n",
       " 'prophetis',\n",
       " 'theatro',\n",
       " 'times',\n",
       " 'ales',\n",
       " 'plana',\n",
       " 'Eos',\n",
       " 'aure',\n",
       " 'pie',\n",
       " 'opi',\n",
       " 'antro',\n",
       " 'dolos',\n",
       " 'Thebas',\n",
       " 'polo',\n",
       " 'thalamos',\n",
       " 'limo',\n",
       " 'stantes',\n",
       " 'aget',\n",
       " 'philosopho',\n",
       " 'elephanti',\n",
       " 'tauros',\n",
       " 'orto',\n",
       " 'paries',\n",
       " 'toros',\n",
       " 'agnos',\n",
       " 'Homero',\n",
       " 'ae',\n",
       " 'metalla',\n",
       " 'thalamo',\n",
       " 'io',\n",
       " 'Theseus',\n",
       " 'Aristotele',\n",
       " 'nautas',\n",
       " 'anathema',\n",
       " 'Minos',\n",
       " 'dogmata',\n",
       " 'Orpheus',\n",
       " 'throno',\n",
       " 'Pergama',\n",
       " 'sus',\n",
       " 'labes',\n",
       " 'sues',\n",
       " 'notio',\n",
       " 'ere',\n",
       " 'axioma',\n",
       " 'phantasma',\n",
       " 'petas',\n",
       " 'Orestes',\n",
       " 'eant',\n",
       " 'noto',\n",
       " 'pauet',\n",
       " 'hebes',\n",
       " 'emat',\n",
       " 'Orion',\n",
       " 'rhetor',\n",
       " 'leget',\n",
       " 'metro',\n",
       " 'pono',\n",
       " 'olet',\n",
       " 'lino',\n",
       " 'mones',\n",
       " 'domat',\n",
       " 'Delo',\n",
       " 'pharetras',\n",
       " 'Gai',\n",
       " 'geometria',\n",
       " 'Perses',\n",
       " 'ted',\n",
       " 'strato',\n",
       " 'ede',\n",
       " 'Rhodope',\n",
       " 'Dione',\n",
       " 'aristas',\n",
       " 'eri',\n",
       " 'meri',\n",
       " 'mela',\n",
       " 'Euripides',\n",
       " 'Anaxagoras',\n",
       " 'polos',\n",
       " 'proi',\n",
       " 'bibas',\n",
       " 'Maia',\n",
       " 'rhetores',\n",
       " 'phantasmata',\n",
       " 'statera',\n",
       " 'Atreus',\n",
       " 'Hermes',\n",
       " 'messe',\n",
       " 'phantasia',\n",
       " 'metis',\n",
       " 'Nestor',\n",
       " 'Helene',\n",
       " 'Gorgias',\n",
       " 'organo',\n",
       " 'Troes',\n",
       " 'Delos',\n",
       " 'sto',\n",
       " 'aude',\n",
       " 'Theodoro',\n",
       " 'Europe',\n",
       " 'hei',\n",
       " 'tende',\n",
       " 'emo',\n",
       " 'Troas',\n",
       " 'Menelao',\n",
       " 'Lemnos',\n",
       " 'diametro',\n",
       " 'pe',\n",
       " 'alueis',\n",
       " 'lotos',\n",
       " 'Zenoni',\n",
       " 'Phalereus',\n",
       " 'Eoi',\n",
       " 'rhetori',\n",
       " 'Hesiodo',\n",
       " 'Pausania',\n",
       " 'Lesbo',\n",
       " 'torno',\n",
       " 'eges',\n",
       " 'Sparte',\n",
       " 'heroas',\n",
       " 'Apolloni',\n",
       " 'Mida',\n",
       " 'Eros',\n",
       " 'Naxos',\n",
       " 'metri',\n",
       " 'siderea',\n",
       " 'sei',\n",
       " 'athletas',\n",
       " 'Gorgia',\n",
       " 'Thebe',\n",
       " 'logo',\n",
       " 'hamos',\n",
       " 'heroes',\n",
       " 'lita',\n",
       " 'mone',\n",
       " 'Eous',\n",
       " 'antas',\n",
       " 'Paros',\n",
       " 'ito',\n",
       " 'Protagoras',\n",
       " 'dele',\n",
       " 'polite',\n",
       " 'lu',\n",
       " 'pare',\n",
       " 'sidereos',\n",
       " 'Persephone',\n",
       " 'adamas',\n",
       " 'manenti',\n",
       " 'Parthenio',\n",
       " 'tropo',\n",
       " 'Atrei',\n",
       " 'doma',\n",
       " 'metere',\n",
       " 'opta',\n",
       " 'Leontinos',\n",
       " 'diae',\n",
       " 'Minoa',\n",
       " 'Lapithas',\n",
       " 'asto',\n",
       " 'ano',\n",
       " 'mere',\n",
       " 'stes',\n",
       " 'Cypria',\n",
       " 'auge',\n",
       " 'problema',\n",
       " 'Diones',\n",
       " 'Herme',\n",
       " 'Naxo',\n",
       " 'Thebes',\n",
       " 'liga',\n",
       " 'temo',\n",
       " 'pedali',\n",
       " 'beta',\n",
       " 'elate',\n",
       " 'Arete',\n",
       " 'problemata',\n",
       " 'topo',\n",
       " 'Tantalo',\n",
       " 'iota',\n",
       " 'nae',\n",
       " 'heroi',\n",
       " 'Euripo',\n",
       " 'gemet',\n",
       " 'Mimas',\n",
       " 'auges',\n",
       " 'Hermogene',\n",
       " 'Euripide',\n",
       " 'dogmati',\n",
       " 'Epigenes',\n",
       " 'Miletos',\n",
       " 'periodo',\n",
       " 'Melite',\n",
       " 'geme',\n",
       " 'Same',\n",
       " 'assa',\n",
       " 'mede',\n",
       " 'sape',\n",
       " 'agones',\n",
       " 'emes',\n",
       " 'heroa',\n",
       " 'sito',\n",
       " 'demo',\n",
       " 'Amphitrite',\n",
       " 'saturo',\n",
       " 'Aristophane',\n",
       " 'Delio',\n",
       " 'Leto',\n",
       " 'ese',\n",
       " 'Protagora',\n",
       " 'Xanthippe',\n",
       " 'aloes',\n",
       " 'Triptolemo',\n",
       " 'neo',\n",
       " 'heroos',\n",
       " 'polles',\n",
       " 'morio',\n",
       " 'Lede',\n",
       " 'philo',\n",
       " 'protero',\n",
       " 'aloe',\n",
       " 'Laertes',\n",
       " 'Selene',\n",
       " 'exerat',\n",
       " 'Melantho',\n",
       " 'ale',\n",
       " 'mete',\n",
       " 'aleis',\n",
       " 'Hippotades',\n",
       " 'glossas',\n",
       " 'oles',\n",
       " 'lupe',\n",
       " 'ambrosio',\n",
       " 'exe',\n",
       " 'lexeos',\n",
       " 'mese',\n",
       " 'pedo',\n",
       " 'anathemata',\n",
       " 'lebes',\n",
       " 'Ares',\n",
       " 'lae',\n",
       " 'poesi',\n",
       " 'metreta',\n",
       " 'trite',\n",
       " 'Erote',\n",
       " 'psephismata',\n",
       " 'Theodore',\n",
       " 'Polites',\n",
       " 'etis',\n",
       " 'agonia',\n",
       " 'Laertiade',\n",
       " 'bio',\n",
       " 'aule',\n",
       " 'Melete',\n",
       " 'bie',\n",
       " 'Admete',\n",
       " 'erosa',\n",
       " 'Aretes',\n",
       " 'diastemata',\n",
       " 'Pheres',\n",
       " 'Eroti',\n",
       " 'topazo',\n",
       " 'nassa',\n",
       " 'stephano',\n",
       " 'bolos',\n",
       " 'peribolo',\n",
       " 'Euenos',\n",
       " 'Elpenor',\n",
       " 'Parmenide',\n",
       " 'pale',\n",
       " 'oleto',\n",
       " 'parergo',\n",
       " 'Aristodemo',\n",
       " 'Dardanide',\n",
       " 'phthisis',\n",
       " 'pedalia',\n",
       " 'metabole',\n",
       " 'melo',\n",
       " 'pero',\n",
       " 'Europes',\n",
       " 'Diomedeos',\n",
       " 'Neritos',\n",
       " 'geometre',\n",
       " 'Lampetie',\n",
       " 'Are',\n",
       " 'pontones',\n",
       " 'hippo',\n",
       " 'Delon',\n",
       " 'dromo',\n",
       " 'horai',\n",
       " 'Stesichorus',\n",
       " 'pino',\n",
       " 'agoni',\n",
       " 'lethe',\n",
       " 'aletes',\n",
       " 'dido',\n",
       " 'doto',\n",
       " 'meni',\n",
       " 'iambe',\n",
       " 'daphnes',\n",
       " 'agrio',\n",
       " 'pleto',\n",
       " 'pege',\n",
       " 'melie',\n",
       " 'phono',\n",
       " 'depote',\n",
       " 'Letoi',\n",
       " 'hebe',\n",
       " 'potho',\n",
       " 'zeta',\n",
       " 'aeto',\n",
       " 'osi',\n",
       " 'pedalio',\n",
       " 'daphne',\n",
       " 'aniso',\n",
       " 'apate',\n",
       " 'agre',\n",
       " 'polle',\n",
       " 'ambrosie',\n",
       " 'nome',\n",
       " 'ites',\n",
       " 'erato',\n",
       " 'halo',\n",
       " 'ose',\n",
       " 'orse',\n",
       " 'epistate',\n",
       " 'geode',\n",
       " 'laro',\n",
       " 'peren',\n",
       " 'edos',\n",
       " 'psoras',\n",
       " 'Agamemnonides',\n",
       " 'thalle',\n",
       " 'xanthe',\n",
       " 'emata',\n",
       " 'admete',\n",
       " 'elateri',\n",
       " 'anterotas',\n",
       " 'enen',\n",
       " 'pales',\n",
       " 'telamon',\n",
       " 'ereto']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greater_prob_latin = [word for word, latin_prob, greek_prob\n",
    "                     in shared_latin_greek\n",
    "                     if latin_prob >= greek_prob]\n",
    "print(len(greater_prob_latin))\n",
    "greater_prob_latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46,929 distinct transliterated Greek words without matches in the Latin corpus\n"
     ]
    }
   ],
   "source": [
    "# We'll remove the words that have a high probability of being Latin \n",
    "# from the collection of demacronized transliterated Greek words\n",
    "only_greek_transliterated = distinct_demacronized_greek - set(likely_latin)\n",
    "likely_greek = shared_words - set(likely_latin)\n",
    "\n",
    "# Likewise, let's remove the transliterated words that are likely Greek \n",
    "# from the collection of good Latin words\n",
    "distinct_good_latin = set(distinct_good_latin) - likely_greek\n",
    "\n",
    "print(f'{len(only_greek_transliterated):,} distinct transliterated Greek words without matches in the Latin corpus')\n",
    "\n",
    "# NOTE: we are toggling this on to see the difference\n",
    "# only_greek_transliterated=  distinct_demacronized_greek\n",
    "# distinct_latin_wo_greek_matches = distinct_good_latin - shared_words\n",
    "# The following had low precision high recall:\n",
    "# only_greek_transliterated = distinct_demacronized_greek - shared_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normally these gaps might concern us, but since we are more interested in groups of loanwords in phrases, we can rely on smoothing over clusters of loanwords to screen out misses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple data matrix of the single words, transliterated Greek examples followed by the Latin words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226601"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [list(only_greek_transliterated) + list(distinct_good_latin)]\n",
    "len(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we featurize our data matrix, let's check on the max word lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 Max word length in distinct good Latin sample\n",
      "25 Max word length in transliterated Greek sample\n"
     ]
    }
   ],
   "source": [
    "print(f'{sorted([len(tmp) for tmp in distinct_good_latin])[-1]} Max word length in distinct good Latin sample')\n",
    "print(f'{sorted([len(tmp) for tmp in only_greek_transliterated])[-1] } Max word length in transliterated Greek sample')\n",
    "max_len = sorted([len(tmp) for tmp in only_greek_transliterated])[-1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization\n",
    "#### We'll use a generic character to integer transform so that we can reuse our encoding process for unseen character data combinations (as opposed to building a dictionary mapping for a discrete sample space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is included in corpus_cleaning.py but we include it here for easy reference\n",
    "def word_to_features(word, max_word_length=20):\n",
    "    \"\"\"\n",
    "    Convert a single word into an array of numbers based on character ordinals, with padding\n",
    "    :param word: a single word\n",
    "    :param max_word_length: the maximum word length for the feature array\n",
    "    :return: a list of integers padded to the max word length\n",
    "\n",
    "    >>> word_to_features('far', 20)\n",
    "    [116, 114, 97, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32]\n",
    "    \"\"\"\n",
    "    if len(word) > max_word_length:\n",
    "        LOG.warning('Excessive word length {} for {}, truncating to {}'.format(len(word), word,\n",
    "                                                                               max_word_length))\n",
    "        word = word[:max_word_length]\n",
    "    word = list(word)\n",
    "    word.reverse() #: encourage aligning on word endings if possible\n",
    "    return [ord(c) for c in \"\".join(word).ljust(max_word_length, ' ')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create the X,y feature matrix and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (226601,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:make_model:Excessive word length 28 for Thensaurochrysonicochrysides, truncating to 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (226601, 25)\n"
     ]
    }
   ],
   "source": [
    "all_y = np.array([1] * len(only_greek_transliterated) + [0] * len(distinct_good_latin), dtype=float)\n",
    "print(f'y shape: {all_y.shape}')\n",
    "# We use a label encoder to automatically capture the range of values for provenance\n",
    "# Although it's true we're only doing binary classification, \n",
    "# it's a good practice to use this so that we can automate recording our model's provenance\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_y)\n",
    "all_words = list(only_greek_transliterated) + list(distinct_good_latin)\n",
    "all_X = np.array([word_to_features(word, max_len) for word in all_words])\n",
    "print(f'X shape: {all_X.shape}')\n",
    "all_X = sparse.csr_matrix(all_X)\n",
    "num_samples = all_y.shape[0] # to be used later by model provenance\n",
    "num_features = all_X.shape[1] # to be used later by model provenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a DummyClassifier to show the baseline which we must improve above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier: 0.6731390443240189\n"
     ]
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy='stratified', random_state=0)\n",
    "features_train, features_test, target_train, target_test = train_test_split(all_X, all_y,\n",
    "                                                                            random_state=0)\n",
    "dummy.fit(features_train, target_train)\n",
    "dummy_score = dummy.score(features_test, target_test)\n",
    "print(f'Dummy classifier: {dummy_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and classify the data using several classifiers, printing out the cross validation score results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:01<00:05,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.naive_bayes.MultinomialNB'> 0.6817975230031593 [0.68462744 0.68162662 0.68197264 0.67579435 0.68496657]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [22:30<20:17, 405.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.neighbors.classification.KNeighborsClassifier'> 0.8618761574633329 [0.86246994 0.86068269 0.86286408 0.86293027 0.86043381]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [24:11<10:28, 314.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.bagging.BaggingClassifier'> 0.9539896109825637 [0.95275921 0.95470091 0.95262577 0.9565534  0.95330877]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [25:24<04:01, 241.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.forest.ExtraTreesClassifier'> 0.9365889790879522 [0.93704905 0.93735796 0.93515004 0.93631951 0.93706834]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 5/5 [26:10<00:00, 183.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'> 0.946611003214381 [0.94629421 0.94810353 0.94461606 0.94631509 0.94772612]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier,\n",
    "    BaggingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestClassifier\n",
    "]\n",
    "for cls in tqdm(classifiers):\n",
    "    scores = cross_val_score(cls(), all_X, all_y,\n",
    "                             scoring='accuracy',\n",
    "                             n_jobs=multiprocessing.cpu_count(),\n",
    "                             cv=5)\n",
    "    print('{} {} {}'.format(str(cls), scores.mean(), scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Grid Search to optimize one of the best classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5c19e434f5e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                          \u001b[0;34m'max_features'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 0.2, 0.3,  0.7, 1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                      })\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgrids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best score: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best params %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/ML-You-Can-Use/p37/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/ML-You-Can-Use/p37/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/ML-You-Can-Use/p37/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/ML-You-Can-Use/p37/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/ML-You-Can-Use/p37/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/ML-You-Can-Use/p37/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "grids = GridSearchCV(cv=5, error_score='raise',\n",
    "                     estimator=RandomForestClassifier()\n",
    "                     , n_jobs=2,\n",
    "                     param_grid={\n",
    "                         'criterion': ['entropy'],  # also tried'gini', \n",
    "                         'n_estimators': [ 750, 800], #600, 650, 700,\n",
    "                     # also tried 100, 300, 500, 550, 850, 900\n",
    "                         'max_features': [0.4]  # 0.2, 0.3,  0.7, 1.0                      \n",
    "                     })\n",
    "grids.fit(all_X, all_y)\n",
    "print('Best score: %s', grids.best_score_)\n",
    "print('Best params %s', grids.best_params_)\n",
    "\n",
    "# Best score: %s 0.9505224737836736\n",
    "# Best params %s {'criterion': 'entropy', 'max_features': 0.3, 'n_estimators': 600}\n",
    "# Best score: %s 0.9446973552104967\n",
    "# Best params %s {'criterion': 'entropy', 'max_features': 0.3, 'n_estimators': 700}\n",
    "# Best score: %s 0.9613020242629114\n",
    "# Best params %s {'criterion': 'entropy', 'max_features': 0.4, 'n_estimators': 750}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the best parameters from GridSearch, build the optimal classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's copy the parameters for the provenance file\n",
    "mdl_params = deepcopy(grids.best_params_)\n",
    "# Let's also remove the base_estimator parameters, since they aren't honored by the constructor, unlike GridSearch\n",
    "if 'base_estimator__criterion' in mdl_params:\n",
    "    del mdl_params['base_estimator__criterion']\n",
    "classifier = RandomForestClassifier(**mdl_params)\n",
    "classifier.fit(all_X, all_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the classifier, so it can be loaded without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_file = 'is_transliterated_greek.mdl.{}.joblib'.format(sklearn.__version__)\n",
    "dump(classifier, model_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model provenance \n",
    "#### Provenance records:\n",
    "* History of the classifier\n",
    "* What data was used\n",
    "* Which hyperparameters had what values\n",
    "* What algorithm was used\n",
    "* What score was achieved\n",
    "\n",
    "#### Important for allowing others:\n",
    "* To use the classifier in the future without rebuilding from scratch\n",
    "* To determining how it may be retrained for better performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {}\n",
    "idx = 1\n",
    "\n",
    "for idx, file in enumerate(good_files, 1):\n",
    "    data_files[idx] = {\"filename\": file[file.rfind(\"/\") + 1:],\n",
    "                       \"md5\": md5(os.path.join(latin_reader.root, file))\n",
    "                       }\n",
    "\n",
    "for idx, file in enumerate(greek_texts, len(good_files)):\n",
    "    data_files[idx + 1] = {\n",
    "        \"filename\": file[file.rfind(\"/\") + 1:],\n",
    "        \"md5\": md5(os.path.join(perseus_greek.root, file))\n",
    "    }\n",
    "\n",
    "provenance_file = '{}.prov.json'.format(model_output_file)\n",
    "\n",
    "params = {\n",
    "    \"provenance_data\": provenance_file,\n",
    "    \"date_created\": str(datetime.datetime.now()),\n",
    "    \"model_parameters\": mdl_params,\n",
    "    \"max_word_length\": max_len,\n",
    "    \"num_samples\": num_samples,\n",
    "    \"num_features\": num_features,\n",
    "    \"library_version\": sklearn.__version__,\n",
    "    \"classifier_class\": \"{}\".format(str(classifier.__class__)),\n",
    "    \"classifier_best_score\": grids.best_score_,\n",
    "    \"data_files\": data_files,\n",
    "    \"model_output_file\": model_output_file,\n",
    "    \"model_output_md5\": md5(model_output_file),\n",
    "    \"labels\": label_encoder.classes_.tolist(),\n",
    "    \"best_score\": grids.best_score_,\n",
    "    \"best_params\": grids.best_params_,\n",
    "    # manually added information\n",
    "    \"comment\": \"Transliterated Greek Classifier\",\n",
    "    \"code_generated_by\": \"loanwords_problems_solutions.ipynb\",\n",
    "    \"feature_encoding_fun\": \"word_to_features\",\n",
    "    \"author\": \"Todd Cook\",\n",
    "    \"sample_pipeline\": process_latin_text_pipeline_file\n",
    "}\n",
    "\n",
    "with open(provenance_file, 'wt') as writer:\n",
    "    json.dump(params, writer, indent=2)\n",
    "    print('Wrote provenance file: {}'.format(provenance_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about that model provenance file? It should be readable, here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(params, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstitute the classifier for use at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = load(model_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some demo examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(\n",
    "    sparse(np.array([word_to_features(word, max_len) for word in ['quid', 'est', 'veritas']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(\n",
    "    sparse(np.array([word_to_features(word, max_len) for word in 'ou eis panta ton'.split()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Cluster Holes\n",
    "as you can see from the example above, sometime the classifier will drop out and not classify something correctly in a sequence. Quite often these dropouts are words that could be found in either language. If two good classifications bookend a dropout, then the dropout should probably be filled in.  We've got a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_cluster_holes(classifier.predict(\n",
    "    sparse(np.array([word_to_features(word, max_len) for word in 'ou eis panta ton'.split()]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at the author Pliny the Younger for transliterated Greek words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_in_pliny = set()\n",
    "latin_reader = get_corpus_reader(corpus_name='latin_text_latin_library', language='latin')\n",
    "results = defaultdict(list)\n",
    "\n",
    "selected_files = [file for file in latin_reader.fileids()\n",
    "                  if 'pliny.ep' in file]\n",
    "\n",
    "for file in tqdm(selected_files):\n",
    "    for sent in latin_reader.sents(file):\n",
    "        unseen_X = process_latin_text_pipeline.fit_transform([(sent)])\n",
    "        if unseen_X and len(unseen_X[0][0]) > 1:\n",
    "            arr = classifier.predict(\n",
    "                sparse.csr_matrix(np.array([word_to_features(word, max_len) for sentence in unseen_X for word in\n",
    "                          sentence])))\n",
    "            arr = patch_cluster_holes(arr)\n",
    "            found_greek = extract_words(unseen_X[0], *run_length_encoding(arr))  # works with sent\n",
    "            if found_greek:\n",
    "                results[file].append(found_greek)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(list(results.values()), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(list(results.values()), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the model \n",
    "We can assess the classifier's performance by showing how it behaves with different amounts of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Learning Curves (RandomForest)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)  # changed 100 to 50\n",
    "estimator = RandomForestClassifier(**mdl_params)\n",
    "ylim = (0.7, 1.01)\n",
    "n_jobs = 7\n",
    "train_sizes = np.linspace(.1, 1.0, 5)\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "plt.title(title)\n",
    "if ylim is not None:\n",
    "    plt.ylim(*ylim)\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator, all_X, all_y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.grid()\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout(pad=0.5, w_pad=20, h_pad=0.5)\n",
    "plt.show()\n",
    "# plt.savefig('loanwords.prob.solutions.learningcurve.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find which texts in the Latin Library have the most transliterated Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = {}\n",
    "latin_reader = get_corpus_reader(corpus_name='latin_text_latin_library', language='latin')\n",
    "greek_in_corpus_selection = set()\n",
    "found_greek = []\n",
    "\n",
    "for full_path in tqdm(latin_reader.fileids(), total=len(latin_reader.fileids()), unit='files'):\n",
    "    filename = full_path[full_path.rfind('/') + 1:]\n",
    "    unseen_X = process_latin_text_pipeline.fit_transform(list(latin_reader.sents([full_path])))\n",
    "    distinct_unseen = distinct_words(unseen_X)\n",
    "    unseen_words = list(distinct_unseen)\n",
    "    total_words = [word\n",
    "                   for sentence in unseen_X\n",
    "                   for word in sentence]\n",
    "    arr = classifier.predict(sparse(np.array([word_to_features(word, max_len) for word in total_words])))\n",
    "    total_greek_words = np.count_nonzero(arr)\n",
    "    marks = arr.tolist()\n",
    "    if marks:\n",
    "        found_greek = [total_words[idx]\n",
    "                       for idx, point in enumerate(marks)\n",
    "                       if point == 1]\n",
    "        greek_in_corpus_selection |= set(found_greek)\n",
    "        corpus_files[filename] = (len(total_words), total_greek_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total Greek words found in Latin selection {len(greek_in_corpus_selection):,}')\n",
    "print(f'Number of Greek words not in training data: {len(greek_in_corpus_selection - only_greek_transliterated):,}')\n",
    "print(f'Random sample: {random.sample(greek_in_corpus_selection, 20)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus_files ))\n",
    "rankings = [ (key, val[0], val[1], val[1]/val[0]) for key, val in corpus_files.items()]\n",
    "rankings.sort(key=lambda x: x[3])\n",
    "for rank in rankings:\n",
    "    print (rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all for now folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
