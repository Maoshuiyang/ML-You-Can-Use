{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Training Data\n",
    "Once you have a classifier, it's usually important to gather more training data so that you can rebuild the classifier for better results. Unfortunately, it's usually undesirable to use the classifier to harvest addition training data. Let's compare using the classifier against word probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import site\n",
    "import pickle\n",
    "import os\n",
    "import logging \n",
    "import site\n",
    "\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import sklearn\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.tokenize.word import WordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add our candidate training data\n",
    "we've selected a lemmatization dictionary, the keys are distinct forms which map to many inflected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "site.addsitedir(os.path.expanduser('~/cltk_data/latin/lemma/latin_pos_lemmata_cltk'))\n",
    "from latin_lemmata_cltk import LEMMATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = logging.getLogger('make_model')\n",
    "LOG.addHandler(logging.NullHandler())\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_features(word, max_word_length=20):\n",
    "    \"\"\"\n",
    "    Convert a single word into an array of numbers based on character ordinals, with padding\n",
    "    :param word: a single word\n",
    "    :param max_word_length: the maximum word length for the feature array\n",
    "    :return: a list of integers padded to the max word length\n",
    "\n",
    "    >>> word_to_features('far', 20)\n",
    "    [116, 114, 97, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32]\n",
    "    \"\"\"\n",
    "    if len(word) > max_word_length:\n",
    "        LOG.warning('Excessive word length {} for {}, truncating to {}'.format(len(word), word,\n",
    "                                                                               max_word_length))\n",
    "        word = word[:max_word_length]\n",
    "    word = list(word)\n",
    "    word.reverse() #: encourage aligning on word endings if possible\n",
    "    return [ord(c) for c in \"\".join(word).ljust(max_word_length, ' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270,227 words\n"
     ]
    }
   ],
   "source": [
    "candidate_words = list(LEMMATA.keys())\n",
    "print(f'{len(candidate_words):,} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some minor text processing must occur, such as JV Transformation and taking the first tokenized form, which will drop enclitic endings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jv_transform = JVReplacer()\n",
    "tokenizer = WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cerycia',\n",
       " 'decimo',\n",
       " 'Mutycenses',\n",
       " 'enauiganda',\n",
       " 'quadriiugo',\n",
       " 'colocynthide',\n",
       " 'aspicientibus',\n",
       " 'Dulichius',\n",
       " 'euangelizauerit',\n",
       " 'patrocinatur']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cans = [tokenizer.tokenize(jv_transform.replace(word))[0] for word in candidate_words ]\n",
    "cans[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate words: 224,576\n"
     ]
    }
   ],
   "source": [
    "cans= list(set(cans))\n",
    "print(f'Candidate words: {len(cans):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_file = 'is_transliterated_greek.mdl.{}.joblib'.format(sklearn.__version__)\n",
    "classifier = load(os.path.join('./',model_output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}.prov.json'.format(model_output_file), 'rt') as reader:\n",
    "    prov = json.load(reader)\n",
    "    max_len =  prov['max_word_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's also try to separate the lemmatization keys by using the probability distributions of the two languages in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_transliterated_word_probs = {}\n",
    "with open('freq_dist.greek.transliterated.pkl', 'rb') as reader:\n",
    "    greek_transliterated_word_probs = pickle.load(reader)\n",
    "    \n",
    "latin_word_probs = {}\n",
    "with open(os.path.join('../building_language_model', 'freq_dist.latin.pkl'), 'rb') as reader:\n",
    "    latin_word_probs = pickle.load(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175969\n"
     ]
    }
   ],
   "source": [
    "greater_prob_latin = [word for word in cans \n",
    "                     if latin_word_probs.get(word, 0.0000001) >= greek_transliterated_word_probs.get(word,0.0000001 )]\n",
    "print(len(greater_prob_latin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('latin.lemma.forms.txt', 'wt') as writer:\n",
    "    for word in greater_prob_latin:\n",
    "        writer.write(word)\n",
    "        writer.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:make_model:Excessive word length 28 for Thensaurochrysonicochrysides, truncating to 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "192113"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probable_latin=[]\n",
    "maybe_greek=[]\n",
    " \n",
    "for word in cans:\n",
    "    result = classifier.predict(np.array([word_to_features(word, max_len) ]))\n",
    "    if result:\n",
    "        probable_latin.append(word)\n",
    "    else:\n",
    "        maybe_greek.append(word)\n",
    "print(len(probable_latin))\n",
    "len(maybe_greek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's all for now folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
