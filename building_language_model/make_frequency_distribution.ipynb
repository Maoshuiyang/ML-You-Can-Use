{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Frequency Distribution\n",
    "### Often, when working with a corpus or a body of words that belong to a corpus, it's helpful to use the metric of a freqency distribution. Usually a frequency distribution is a count of each distinct word form, and then the total occurences are normalized so that all frequency values fall between 0.0 and 0.99999.\n",
    "#### why 0.99999? see: https://en.wikipedia.org/wiki/Cromwell%27s_rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "from cltk.prosody.latin.string_utils import remove_punctuation_dict\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from building_language_model.aeoe_replacer import AEOEReplacer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_reader = get_corpus_reader(corpus_name='latin_text_latin_library', language='latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16455728it [18:03, 15186.65it/s]\n"
     ]
    }
   ],
   "source": [
    "word_counter = Counter()\n",
    "jv_replacer = JVReplacer()\n",
    "aeoe_replacer = AEOEReplacer()\n",
    "\n",
    "for word in tqdm(latin_reader.words()):\n",
    "    if word.isalpha():\n",
    "        word = aeoe_replacer.replace(jv_replacer.replace(word))\n",
    "        word_counter.update({word: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('et', 426296),\n",
       " ('in', 264109),\n",
       " ('est', 170724),\n",
       " ('non', 155822),\n",
       " ('ad', 127206),\n",
       " ('ut', 115717),\n",
       " ('cum', 100820),\n",
       " ('quod', 95403),\n",
       " ('qui', 86333),\n",
       " ('si', 79412)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426296"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = sum(word_counter.values())\n",
    "word_counter['et']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.032449394302737196"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter['et']/float(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kai is the Greek word for 'and' transliterated into Latin. It is one of the most common words in Greek, and thus it is the one Greek word most likely to appear as loanword, as such we could use it as a threshold for detecting whether or not a random word is candidate for being a transliterated Greek loanword; we'll try this in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter['kai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6232832792479645e-05"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter['kai'] / float(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The raw percentage number isn't very readable, so we'll normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999900000000002]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(word_counter.keys())\n",
    "counts = [ tmp/float(total_words) for tmp in word_counter.values()]\n",
    "counts = np.array(counts)\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 0.99999))\n",
    "# why 0.999999? see: https://en.wikipedia.org/wiki/Cromwell%27s_rule\n",
    "scaled_data = min_max_scaler.fit_transform(counts.reshape(-1, 1))\n",
    "\n",
    "word_probabilities = Counter(dict(zip (words, scaled_data.tolist())))\n",
    "word_probabilities['et']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0011142407253193212]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_probabilities['kai']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that normalized number looks more managable. Let's save the counter for resuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('freq_dist.latin.pkl', 'wb') as writer:\n",
    "    pickle.dump(word_probabilities, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prove that we can load and use what we just saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_frequency_dist = None\n",
    "with open('freq_dist.latin.pkl', 'rb') as reader:\n",
    "    latin_frequency_dist = pickle.load(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.015599370154470498]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latin_frequency_dist['rex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
